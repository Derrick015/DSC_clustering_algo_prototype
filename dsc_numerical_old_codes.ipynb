{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"K9YOlpjXJmg1"},"outputs":[],"source":["# performance bending for score\n","# nuc to meet min and max range\n","# Explanation of distance matrix: https://chat.openai.com/share/35f27f11-58d3-4007-8550-2e74fa0181f9"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvBtHXCcJmg3","outputId":"5d51c822-4c1e-42d9-9ed0-36c751de444a"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","import pandas as pd\n","import numpy as np\n","df_country = pd.read_csv('Country-data.csv')\n","df_country = df_country.sample(frac=1, axis=0, random_state=5454) #BP76\n","df_country.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IRpbiTlGJmg3"},"outputs":[],"source":["# Outlier removal\n","coun_data = df_country[\"country\"].copy()\n","\n","df_country  = df_country.drop('country', axis=1)\n","df_country0 = df_country.copy(deep=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNNzb-wXJmg3"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","# Initialize the StandardScaler\n","scaler = MinMaxScaler()\n","\n","# Fit the scaler to the data and transform it\n","df_count_scaled = scaler.fit_transform(df_country0)\n","\n","# Convert the scaled data back to a DataFrame\n","# This step is optional but often useful for readability and further processing\n","df_country= pd.DataFrame(df_count_scaled, columns=df_country.columns)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yh5uukTbJmg3"},"outputs":[],"source":["# from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","# import pandas as pd\n","# import numpy as np\n","# df_wine = pd.read_csv('wine-clustering.csv')\n","\n","# # Initialize the StandardScaler\n","# scaler = StandardScaler()\n","\n","# # Fit the scaler to the data and transform it\n","# df_wine_scaled = scaler.fit_transform(df_wine)\n","\n","# # Convert the scaled data back to a DataFrame\n","# # This step is optional but often useful for readability and further processing\n","# df_wine = pd.DataFrame(df_wine_scaled, columns=df_wine.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgUpMKwaJmg3"},"outputs":[],"source":["# Writing to HDF5\n","# df_emb_free.to_hdf('df_resturant_emb.h5', key='df_emb_free', mode='w')\n","# df_r= pd.read_hdf(\"df_resturant_emb.h5\", key='df_emb_free')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6JbRXyfJmg3"},"outputs":[],"source":["# # Adjust the generation of clusters to ensure all values are positive\n","# import pandas as pd\n","# import numpy as np\n","# from sklearn.datasets import make_blobs\n","# # Generate the main clusters with positive centers\n","# cluster_1_pos, _ = make_blobs(n_samples=100, centers=[(10, 10)], cluster_std=1.5)\n","# cluster_2_pos, _ = make_blobs(n_samples=100, centers=[(20, 25), (30, 20)], cluster_std=[1.2, 0.8])\n","# cluster_3_pos, _ = make_blobs(n_samples=100, centers=[(25, 5)], cluster_std=0.5)\n","\n","# # Generate smaller clusters within the larger clusters to create nested complexity\n","# nested_cluster_1_pos, _ = make_blobs(n_samples=50, centers=[(12, 12)], cluster_std=0.3)\n","# nested_cluster_2_pos, _ = make_blobs(n_samples=50, centers=[(28, 18)], cluster_std=0.2)\n","\n","# # Combine all clusters\n","# complex_data_pos = np.vstack([cluster_1_pos, cluster_2_pos, cluster_3_pos, nested_cluster_1_pos, nested_cluster_2_pos])\n","\n","# # Introduce some noise, ensuring it's also positive\n","# noise_pos = np.random.uniform(low=0, high=35, size=(50, 2))\n","# complex_data_with_noise_pos = np.vstack([complex_data_pos, noise_pos])\n","\n","# # Create the complex DataFrame with only positive values\n","# complex_df_pos = pd.DataFrame(complex_data_with_noise_pos, columns=['Feature_1', 'Feature_2'])\n","# complex_df_pos = complex_df_pos.sample(frac=1).reset_index(drop=True)\n","# complex_df_pos.head(), complex_df_pos.shape\n","# df_iris = complex_df_pos.copy(deep=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10297,"status":"ok","timestamp":1714575688457,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"Z2Wc1BSALEuA","outputId":"2e003e65-ae01-4e57-cd90-0d55dc0b4d7f"},"outputs":[],"source":["# to use this script or use it elsewhere\n","from google.colab import drive\n","from google.colab import files\n","drive.mount('/content/drive')\n","\n","# %run '/content/drive/My Drive/dsc_dir/IRIS.csv'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6236,"status":"ok","timestamp":1714575700829,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"wJfscrIYJmg3"},"outputs":[],"source":["import pandas as pd\n","\n","# try diagnosing from say 6 to 4 or something my indexing seems to work as it should\n","# so i think the issue is when the centroid brings back a group not every row may truly belong to that group\n","# Could there be some kind of test to see if ever row agrees to the new cluster ? or they will switch teams.\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","\n","\n","df_ir = pd.read_csv('/content/drive/My Drive/dsc_dir/IRIS.csv')\n","df_ir1 = df_ir.drop('species', axis=1)\n","# # Initialize the scaler\n","scaler = MinMaxScaler()\n","\n","# Fit the scaler to the data and transform it\n","scaled_data = scaler.fit_transform(df_ir1)\n","\n","# Convert the scaled data back into a pandas DataFrame\n","df_iris = pd.DataFrame(scaled_data, columns=df_ir1.columns)\n","# df_iris = df_iris.sample(frac=1).reset_index(drop=True)\n","# df_iris_sample = df_iris.sample(n=40).reset_index(drop=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UZS23DxDJmg3","outputId":"a1e2939d-3c7f-4eb7-a40d-90ae20a47155"},"outputs":[],"source":["from sklearn.datasets import make_blobs\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def generate_complex_nested_clusters(n_outer_clusters=3, n_inner_clusters=4, n_samples_per_inner_cluster=50,\n","                                     outer_cluster_std=10.0, inner_cluster_std=1.0, center_box=(0, 100), random_state=None):\n","    # Generate outer cluster centers\n","    outer_centers, _ = make_blobs(n_samples=n_outer_clusters, cluster_std=outer_cluster_std,\n","                                  center_box=center_box, random_state=random_state)\n","\n","    X, y = [], []\n","    label = 0\n","    for outer_center in outer_centers:\n","        # Generate inner cluster centers around each outer center\n","        inner_centers, _ = make_blobs(n_samples=n_inner_clusters, centers=[outer_center], cluster_std=outer_cluster_std/2,\n","                                      center_box=center_box, random_state=random_state)\n","\n","        for inner_center in inner_centers:\n","            # Generate samples for each inner cluster\n","            X_inner, y_inner = make_blobs(n_samples=n_samples_per_inner_cluster, centers=[inner_center], cluster_std=inner_cluster_std,\n","                                          center_box=center_box, random_state=random_state)\n","            X.append(X_inner)\n","            y.append(np.full(n_samples_per_inner_cluster, label))\n","            label += 1\n","\n","    X = np.vstack(X)\n","    y = np.concatenate(y)\n","\n","    return X, y\n","\n","# Generate a more complex nested clusters dataset\n","X_complex, y_complex = generate_complex_nested_clusters(n_outer_clusters=3, n_inner_clusters=5, n_samples_per_inner_cluster=100,\n","                                                        outer_cluster_std=15.0, inner_cluster_std=2.5, random_state=42)\n","\n","# Visualize the complex nested dataset\n","plt.figure(figsize=(10, 8))\n","plt.scatter(X_complex[:, 0], X_complex[:, 1], c=y_complex, cmap='viridis', marker='.')\n","plt.title('Complex Nested Clusters')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h_ImkBK4Jmg4"},"outputs":[],"source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","# Convert the complex nested dataset into a DataFrame\n","df_complex_nested = pd.DataFrame(X_complex, columns=['Feature_1', 'Feature_2'])\n","df_complex_nested['Cluster'] = y_complex\n","\n","df_complex_nested.head()\n","df_complex_nested_sample = df_complex_nested.sample(n=1500).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8siWBZfjJmg4"},"outputs":[],"source":["df_complex_nested_sample1 = df_complex_nested.drop(['Cluster'], axis=1)\n","scaler = StandardScaler()\n","\n","# Fit the scaler to the data and transform it\n","scaled_data = scaler.fit_transform(df_complex_nested_sample1)\n","\n","# Convert the scaled data back into a pandas DataFrame\n","df_complex_nested_sample1 = pd.DataFrame(scaled_data, columns=df_complex_nested_sample1.columns)\n","df_complex_nested_sample1 = df_complex_nested_sample1.sample(frac=1).reset_index(drop=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-_ynqUrJmg4"},"outputs":[],"source":["# # UMAP\n","# import pandas as pd\n","# import numpy as np\n","# from umap import UMAP\n","# import matplotlib.pyplot as plt\n","\n","# # Assuming df_iris is your DataFrame and it's already loaded\n","# # Let's say the last column is the target variable and the rest are features\n","\n","# # Initialize UMAP. The n_neighbors and min_dist parameters can be adjusted based on your dataset.\n","# umap_model = UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n","\n","# # Fit the model and transform your data\n","# X_reduced = umap_model.fit_transform(df_iris)\n","\n","\n","\n","# # Convert UMAP results into a DataFrame\n","# X_reduced_df = pd.DataFrame(X_reduced, columns=['UMAP_1', 'UMAP_2'])\n","\n","# # Display the first few rows of the DataFrame\n","# print(X_reduced_df.head())\n","#df_iris = X_reduced_df.copy(deep=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ue90VNkUJmg4"},"outputs":[],"source":["# understand what is happening\n","# Outlier handling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVir20meJmg4"},"outputs":[],"source":["# For outlier removal remove only those farthest away not those which are close. So only upper bound"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDz3JSOuJmg4"},"outputs":[],"source":["# DSC is important for outlier removal because\n","# two or more pure similarity points could be close together, but those two points themselves\n","# yeeaahh..\n","# assuming there are other points that are also outliers but close to the other two outliers they would be close together and hence\n","# the outlier removal step wouldnt hep much.. perhaps scrape it and use traditional outlier removals\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1TVcdI4kJmg4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXcsiwcHJmg4"},"outputs":[],"source":["# Initialize new columns in the DataFrame for future assignment\n","# df_complex_nested_sample2 = df_complex_nested.copy(deep=True)\n","# df_complex_nested_sample2['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_complex_nested_sample2))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kl_bQgBsJmg4"},"outputs":[],"source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","import pandas as pd\n","df_credit_card = pd.read_csv('creditcard.csv')\n","df_credit_card = df_credit_card.sample(frac=1).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0D_S_V7rJmg4"},"outputs":[],"source":["df_credit_card_1 = df_credit_card.drop([\"Time\",\"Amount\",\"Class\"], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1Y0gidvJmg4"},"outputs":[],"source":["df_credit_card_2 = df_credit_card_1.sample(n=10000, random_state=22).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJI9wgpoJmg5"},"outputs":[],"source":["scaler = StandardScaler()\n","\n","# Fit the scaler to the data and transform it\n","scaled_data = scaler.fit_transform(df_credit_card_2)\n","\n","# Convert the scaled data back into a pandas DataFrame\n","df_credit_card_3 = pd.DataFrame(scaled_data, columns=df_credit_card_2.columns)\n","df_credit_card_3 = df_credit_card_3.sample(frac=1).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhEjImp9Jmg5","jupyter":{"source_hidden":true}},"outputs":[],"source":["# # Graph but Union find is faster\n","# import pandas as pd\n","# import networkx as nx\n","\n","\n","# # Create an undirected graph\n","# G = nx.Graph()\n","\n","# # Add edges based on the 'closest_index' relationship. The DataFrame index is used as the row index.\n","# for row_index, closest_index in df['Closest_Row'].items():\n","#     # Adding 1 to both row_index and closest_index to match the usual 1-based indexing of examples\n","#     G.add_edge(row_index + 1, closest_index + 1)\n","\n","# # Find connected components\n","# connected_components = list(nx.connected_components(G))\n","\n","# # Assign family identifiers\n","# family_assignments = {}\n","# for i, component in enumerate(connected_components):\n","#     for node in component:\n","#         family_assignments[node] = i\n","\n","# # Convert family_assignments to a DataFrame for easy visualization\n","# family_df = pd.DataFrame(list(family_assignments.items()), columns=['Row Index', 'family_id'])\n","\n","# family_df.family_id.nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbXjKM3PJmg5","jupyter":{"source_hidden":true}},"outputs":[],"source":["# # OUtlier removal this way was not quite helpful with performance\n","# df_original=df_country0.copy(deep=True)\n","\n","# df=df_original.copy()\n","# # Compute the pairwise distances\n","# dist_matrix = pairwise_distances(df, metric='euclidean')\n","\n","# # Replace diagonal with np.inf to ignore self-distance\n","# np.fill_diagonal(dist_matrix, np.inf)\n","\n","# # Find the index of the closest row and the distance for each row\n","# closest_indices = dist_matrix.argmin(axis=1)\n","# closest_distances = dist_matrix.min(axis=1)\n","\n","# # Add the results to the original DataFrame\n","# df['Closest_Row'] = closest_indices\n","# df['Distance'] = closest_distances\n","\n","# # 1. Remove outliers after\n","# # We will modify the function to also return the outliers\n","\n","# def remove_upper_outliers_and_save(df, column_name):\n","#     quartile_1, quartile_3 = np.percentile(df[column_name], [25, 75])\n","#     iqr = quartile_3 - quartile_1\n","#     upper_bound = quartile_3 + (iqr * 1.5)\n","\n","#     # Filter the DataFrame to remove data points above the upper bound\n","#     filtered_df = df[df[column_name] <= upper_bound]\n","#     # Save the outliers in a separate DataFrame\n","#     outliers_df = df[df[column_name] > upper_bound]\n","\n","#     return filtered_df, outliers_df\n","\n","# # Apply the modified function to remove upper outliers from 'Distance' column and save the removed outliers\n","# filtered_df, outliers_df = remove_upper_outliers_and_save(df, 'Distance')\n","\n","# # Now we have two DataFrames: `filtered_df` without outliers and `outliers_df` with only the removed outliers\n","# # (filtered_df, outliers_df)\n","\n","# # 1B..recalcualted distances again since i removed some indexes that were outliers\n","# import pandas as pd\n","# from sklearn.metrics import pairwise_distances\n","\n","# # Assuming 'df' is your DataFrame\n","# # For example:\n","# # data = {\n","# #     'Feature1': [1, 2, 3, 4],\n","# #     'Feature2': [4, 3, 2, 1],\n","# #     'Feature3': [2, 3, 4, 5]\n","# # }\n","# # df = pd.DataFrame(data)\n","\n","# filtered_df.reset_index(inplace=True, drop=True)\n","# df=filtered_df.copy(deep=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DvV-iVBfJmg5","jupyter":{"source_hidden":true}},"outputs":[],"source":["#df_original = df.copy(deep=True)\n","#df_original.drop(['Closest_Row','Distance'], axis = 1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EO_wRs8IJmg5","jupyter":{"source_hidden":true}},"outputs":[],"source":["# from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","# # Initialize the StandardScaler\n","# scaler = StandardScaler()\n","\n","# # Fit the scaler to the data and transform it\n","# df_count_scaled = scaler.fit_transform(df_original)\n","\n","# # Convert the scaled data back to a DataFrame\n","# # This step is optional but often useful for readability and further processing\n","# df_original= pd.DataFrame(df_count_scaled, columns=df_original.columns)\n"]},{"cell_type":"markdown","metadata":{"id":"jYdJT6IhJmg5"},"source":["# MAIN MODEL"]},{"cell_type":"markdown","metadata":{"id":"PTZY5Dr3Jmg6"},"source":["## Requirements"]},{"cell_type":"markdown","metadata":{"id":"VsS_ClIGJmg6"},"source":["1. USE ROBUST SCALING LIKE MIN MAX AND ROBOST SCALER FOR DATA\n","\n","If the scaling method you used is sensitive to outliers (like Z-score normalization), you might benefit from rescaling after outlier removal. However, if you used a more robust scaling method (like Min-Max scaling or Robust Scaler in sklearn that scales data according to percentiles and is thus less sensitive to outliers), rescaling might not be as necessary.\n"]},{"cell_type":"markdown","metadata":{"id":"VqZRt9tSJmg6"},"source":["## Outlier removal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9JMglA1DJmg6","jupyter":{"source_hidden":true}},"outputs":[],"source":["%%time\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import pairwise_distances\n","# df_original = df_iris1.copy(deep=True)\n","#df_original = df_credit_card_3.copy(deep=True)\n","all_outliers = pd.DataFrame()\n","df = df_iris.copy(deep=True)\n","\n","\n","# Compute the pairwise distances\n","dist_matrix = pairwise_distances(df, metric='euclidean')\n","\n","# Replace diagonal with np.inf to ignore self-distance\n","np.fill_diagonal(dist_matrix, np.inf)\n","\n","# Find the index of the closest row and the distance for each row\n","closest_indices = dist_matrix.argmin(axis=1)\n","closest_distances = dist_matrix.min(axis=1)\n","\n","# Add the results to the original DataFrame\n","df['Closest_Row'] = closest_indices\n","df['Distance'] = closest_distances\n","\n","# 1. Remove outliers after\n","# We will modify the function to also return the outliers\n","\n","def remove_upper_outliers_and_save(df, column_name):\n","    quartile_1, quartile_3 = np.percentile(df[column_name], [25, 75])\n","    iqr = quartile_3 - quartile_1\n","    upper_bound = quartile_3 + (iqr * 1.5) # 1.5\n","\n","    # Filter the DataFrame to remove data points above the upper bound\n","    filtered_df = df[df[column_name] <= upper_bound]\n","    # Save the outliers in a separate DataFrame\n","    outliers_df = df[df[column_name] > upper_bound]\n","\n","    return filtered_df, outliers_df\n","\n","# Apply the modified function to remove upper outliers from 'Distance' column and save the removed outliers\n","filtered_df, outliers_df = remove_upper_outliers_and_save(df, 'Distance')\n","\n","# Now we have two DataFrames: `filtered_df` without outliers and `outliers_df` with only the removed outliers\n","# (filtered_df, outliers_df)\n","\n","outliers_df0 = outliers_df.copy(deep=True)\n","del(outliers_df)\n","#filtered_df.reset_index(inplace=True, drop=True)\n","\n","df_original = filtered_df.copy(deep=True)\n","df_original.drop(['Closest_Row','Distance'], axis = 1, inplace=True)\n","outliers_df0.drop(['Closest_Row','Distance'], axis = 1, inplace=True)\n","outliers_df0['fam_id'] = -1\n","all_outliers = pd.concat([all_outliers, outliers_df0], ignore_index=True)\n","del(outliers_df0)\n","df_original['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_original))]\n","df_original.reset_index(drop=True, inplace=True)\n","#\n","#\n","#\n","# Using factorize to encode 'fam_id'\n","# df_original['fam_id'] = pd.factorize(df_original['fam_id'])[0]\n","\n","# # Convert the encoded numbers to strings\n","# df_original['fam_id'] = df_original['fam_id'].astype(str)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6fE_c_63Jmg6"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Wl-NoPtcJmg6"},"source":["# Manual test"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6324,"status":"ok","timestamp":1714575805334,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"LZ_cXdeSJmg6"},"outputs":[],"source":["# import pandas as pd\n","# import numpy as np\n","# df11 = pd.read_hdf('/content/drive/My Drive/dsc_dir/Godiva_df_emb.h5', key='df11')\n","# embeddings_array = np.stack(df11[\"gpt_3_large_emb\"].values)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\derri\\anaconda3\\envs\\compu76\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import umap\n","\n","# Load your DataFrame\n","df11 = pd.read_hdf('Godiva_df_emb_500.h5', key='df11')\n","\n","# Assuming df11['gpt_3_large_emb'] contains lists or arrays of embeddings\n","embeddings_array = np.stack(df11[\"gpt_3_large_emb\"].values)\n","\n","# Initialize UMAP\n","umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=3, metric='cosine',random_state=76)\n","\n","# Fit and transform the data using UMAP\n","umap_result = umap_reducer.fit_transform(embeddings_array)\n","\n","# Convert UMAP results to a DataFrame\n","umap_df = pd.DataFrame(umap_result, columns=[f'UMAP_{i+1}' for i in range(umap_result.shape[1])])\n","\n","# Concatenate the original DataFrame and the UMAP DataFrame\n","df11 = pd.concat([df11.reset_index(drop=True), umap_df], axis=1)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":192,"status":"ok","timestamp":1714575816494,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"jUEfJb5bJmg6"},"outputs":[],"source":["# Remove columns with totally missing values\n","\n","# Import pandas if not already imported\n","import pandas as pd\n","\n","# Load your DataFrame here if it's not already loaded\n","# df11 = pd.read_csv('your_file.csv') or any other source\n","\n","# Remove columns where all values are missing\n","df12 = df11.loc[:, df11.isna().mean() < 0.5]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224,"status":"ok","timestamp":1714575817765,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"OA4dmq0QJmg6","outputId":"320d2d98-ba30-4ff2-ac10-5d79ec3f80fc"},"outputs":[],"source":["df12.isna().mean()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":186,"status":"ok","timestamp":1714575821036,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"TOAeRW9MJmg6"},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming 'df' is your DataFrame\n","# Drop rows where 'Net Price' is missing\n","df_cleaned = df12.dropna(subset=['Net Price'])\n","df_cleaned.reset_index(drop=True,inplace=True)\n","# If you want to modify the original DataFrame in place:\n","# df.dropna(subset=['Net Price'], inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":296,"status":"ok","timestamp":1714575823230,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"8XTwx4dhOHUU","outputId":"ae6abd61-c528-45ac-ac72-ffa0c073fe6d"},"outputs":[],"source":["df_cleaned.isna().mean()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":187,"status":"ok","timestamp":1714575826333,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"I8BUDJfQJmg6"},"outputs":[],"source":["import pandas as pd\n","df13 = df_cleaned[['UMAP_1','UMAP_2','UMAP_3','Weight','Net Price']]\n","\n","# try diagnosing from say 6 to 4 or something my indexing seems to work as it should\n","# so i think the issue is when the centroid brings back a group not every row may truly belong to that group\n","# Could there be some kind of test to see if ever row agrees to the new cluster ? or they will switch teams.\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","\n","scaler = RobustScaler()\n","\n","# Fit the scaler to the data and transform it\n","scaled_data = scaler.fit_transform(df13)\n","\n","# Convert the scaled data back into a pandas DataFrame\n","df_14 = pd.DataFrame(scaled_data, columns=df13.columns)\n","\n","# df_iris = df_iris.sample(frac=1).reset_index(drop=True)\n","# df_iris_sample = df_iris.sample(n=40).reset_index(drop=True)\n","\n","\n","# from sklearn.preprocessing import StandardScaler\n","\n","# # Initialize the StandardScaler\n","# scaler = StandardScaler()\n","\n","# # Fit and transform the embeddings\n","# scaled_embeddings = scaler.fit_transform(embeddings_array)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":722,"status":"ok","timestamp":1714575832737,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"44koRYGqJmg6","outputId":"45ab29c2-8bf7-4181-8d0b-b3949a404335"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>UMAP_1</th>\n","      <th>UMAP_2</th>\n","      <th>UMAP_3</th>\n","      <th>Weight</th>\n","      <th>Net Price</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.673740</td>\n","      <td>0.168381</td>\n","      <td>0.864829</td>\n","      <td>-0.047938</td>\n","      <td>0.037631</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.825603</td>\n","      <td>-0.579050</td>\n","      <td>-0.003787</td>\n","      <td>-0.263099</td>\n","      <td>-0.262995</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.517034</td>\n","      <td>-1.040929</td>\n","      <td>0.436727</td>\n","      <td>0.690078</td>\n","      <td>0.668734</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.205307</td>\n","      <td>-0.496217</td>\n","      <td>-0.910022</td>\n","      <td>-0.282051</td>\n","      <td>-0.298944</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.495664</td>\n","      <td>-0.985897</td>\n","      <td>0.360992</td>\n","      <td>-0.235229</td>\n","      <td>0.004625</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>0.816534</td>\n","      <td>-0.596586</td>\n","      <td>-0.100315</td>\n","      <td>-0.285396</td>\n","      <td>-0.299995</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>0.054816</td>\n","      <td>1.130081</td>\n","      <td>0.098293</td>\n","      <td>0.901895</td>\n","      <td>0.228517</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>0.685447</td>\n","      <td>0.223073</td>\n","      <td>0.876690</td>\n","      <td>-0.284281</td>\n","      <td>-0.213381</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>0.669445</td>\n","      <td>0.193699</td>\n","      <td>0.879465</td>\n","      <td>-0.082497</td>\n","      <td>0.006307</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>-0.170053</td>\n","      <td>0.210768</td>\n","      <td>0.687583</td>\n","      <td>8.516165</td>\n","      <td>4.667473</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows Ã— 5 columns</p>\n","</div>"],"text/plain":["       UMAP_1    UMAP_2    UMAP_3    Weight  Net Price\n","0    0.673740  0.168381  0.864829 -0.047938   0.037631\n","1    0.825603 -0.579050 -0.003787 -0.263099  -0.262995\n","2   -0.517034 -1.040929  0.436727  0.690078   0.668734\n","3   -0.205307 -0.496217 -0.910022 -0.282051  -0.298944\n","4   -0.495664 -0.985897  0.360992 -0.235229   0.004625\n","..        ...       ...       ...       ...        ...\n","495  0.816534 -0.596586 -0.100315 -0.285396  -0.299995\n","496  0.054816  1.130081  0.098293  0.901895   0.228517\n","497  0.685447  0.223073  0.876690 -0.284281  -0.213381\n","498  0.669445  0.193699  0.879465 -0.082497   0.006307\n","499 -0.170053  0.210768  0.687583  8.516165   4.667473\n","\n","[500 rows x 5 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df_14"]},{"cell_type":"markdown","metadata":{"id":"Lmdmx-nUJmg7"},"source":["# ARRAY WITH LOOP"]},{"cell_type":"markdown","metadata":{"id":"gI5ebOxXJmg7"},"source":["## Outlier removal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Upwl7ZxBJmg7"},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import pairwise_distances\n","\n","# Assuming embeddings is a numpy array already defined\n","df_original = df_14.values.copy()\n","\n","# Compute the pairwise distances\n","dist_matrix = pairwise_distances(df_original, metric='cosine')\n","\n","# Replace diagonal with np.inf to ignore self-distance\n","np.fill_diagonal(dist_matrix, np.inf)\n","\n","# Find the index of the closest row and the distance for each row\n","closest_indices = np.argmin(dist_matrix, axis=1)\n","closest_distances = np.min(dist_matrix, axis=1)\n","\n","def remove_upper_outliers_and_save(data, distances):\n","    quartile_1, quartile_3 = np.percentile(distances, [25, 75])\n","    iqr = quartile_3 - quartile_1\n","    upper_bound = quartile_3 + (iqr * 1.5)\n","\n","    not_outliers_mask = distances <= upper_bound\n","    outliers_mask = distances > upper_bound\n","\n","    filtered_data = data[not_outliers_mask]\n","    outliers_data = data[outliers_mask]\n","\n","    return filtered_data, outliers_data\n","\n","filtered_data, outliers_data = remove_upper_outliers_and_save(df_original, closest_distances)\n","\n","# If you simply want to track the indices, you could just keep them as separate arrays:\n","filtered_ids = np.array(['unq_id_' + str(i) for i in range(filtered_data.shape[0])])\n","outlier_ids = np.array(['unq_id_' + str(i) for i in range(filtered_data.shape[0], filtered_data.shape[0] + outliers_data.shape[0])])\n","\n","def remove_upper_outliers_and_save(distances):\n","    quartile_1, quartile_3 = np.percentile(distances, [25, 75])\n","    iqr = quartile_3 - quartile_1\n","    upper_bound = quartile_3 + 1.5 * iqr\n","    not_outliers_mask = distances <= upper_bound\n","    return not_outliers_mask\n","\n","not_outliers_mask = remove_upper_outliers_and_save(closest_distances)\n","filtered_df11 = df11.iloc[not_outliers_mask].copy(deep=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3k6vxvQ2Jmg7","outputId":"75b27fe4-4f18-473b-ebb5-2db1ecf3764a"},"outputs":[],"source":["%%time\n","\n","import networkx as nx\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import pairwise_distances\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","import numpy as np\n","\n","# Assuming df_original is already defined\n","\n","df_original = filtered_data.copy()\n","# df_original['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_original))]\n","\n","fam_ids = np.array(['unq_id_' + str(i) for i in range(len(df_original))])\n","fam_ids_array = np.array(fam_ids).reshape(-1, 1)\n","new_array = np.concatenate((df_original, fam_ids_array), axis=1)\n","\n","# 2. MAIN NETWORK CLUSTERING\n","\n","## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","# Assuming df is your DataFrame and it includes 'fam_id' along with the features.\n","\n","# Compute the pairwise distances using only the feature columns.\n","dist_matrix0 = pairwise_distances(df_original, metric='cosine')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"erEbAAzoJmg7"},"outputs":[],"source":["breaker = 32"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yoS-QXvJmg7","outputId":"78fae222-153c-496b-bee4-a498cf5725fe"},"outputs":[],"source":["%%time\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","# ee\n","iteration_list = []\n","sil_perf_list = []\n","calinski_perf_list = []\n","unique_fam_list = []\n","\n","\n","# NUCER\n","\n","for ir in range(50):\n","    dist_matrix = np.copy(dist_matrix0)\n","    fam_ids = new_array[:, -1]\n","    same_fam_id = new_array[:, -1][:, None] == new_array[:, -1]\n","    dist_matrix[same_fam_id] = np.inf\n","\n","    closest_indices = np.argmin(dist_matrix, axis=1)\n","    closest_distances = np.min(dist_matrix, axis=1)\n","\n","    # Handling migrations\n","    closest_families = fam_ids[closest_indices]\n","    dtype = [('fam_id', 'U10'), ('closest_distance', 'f8'), ('closest_family', 'U10')]\n","    structured_array = np.array(list(zip(fam_ids, closest_distances, closest_families)), dtype=dtype)\n","    sorted_structured_array = np.sort(structured_array, order=['fam_id', 'closest_distance'])\n","    _, unique_indices = np.unique(sorted_structured_array['fam_id'], return_index=True)\n","    result_structured_array = sorted_structured_array[unique_indices]\n","    migration_map = {fam_id: closest_family for fam_id, closest_family in result_structured_array[['fam_id', 'closest_family']]}\n","    migrate_to_family = np.array([migration_map[fam] for fam in fam_ids])\n","\n","\n","    # Assuming fam_ids, migrate_to_families, and closest_distances are defined as NumPy arrays\n","\n","    # Create a structured array for sorting and unique operation\n","    dtype = [('migrate_to_family', 'U10'), ('closest_distance', float), ('fam_id', 'U10')]\n","    structured_array = np.array(list(zip(migrate_to_family , closest_distances, fam_ids)), dtype=dtype)\n","\n","    # Sort by 'migrate_to_family' and 'closest_distance'\n","    sorted_array = np.sort(structured_array, order=['migrate_to_family', 'closest_distance'])\n","\n","    # Remove duplicates\n","    _, unique_indices = np.unique(sorted_array['migrate_to_family'], return_index=True)\n","    unique_families = sorted_array[unique_indices]\n","\n","    # Mapping for migration decisions\n","    migration_decision_map = {entry['migrate_to_family']: entry['fam_id'] for entry in unique_families}\n","\n","    # Calculate the minimum distance per migration family\n","    unique_families_distances = {entry['migrate_to_family']: entry['closest_distance'] for entry in unique_families}\n","\n","    # Assign migration permissions\n","    can_migrate = np.array([closest_distances[i] == unique_families_distances[migrate_to_family[i]] for i in range(len(fam_ids))])\n","\n","    import networkx as nx\n","\n","    uf = nx.utils.UnionFind()\n","    for i, entry in enumerate(structured_array):\n","        uf.union(entry['fam_id'], entry['migrate_to_family'])\n","\n","    # Map the results back\n","    fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","    mapped_fam_ids = np.array([fam_id_mapping[fam_id] for fam_id in fam_ids])\n","\n","    from sklearn.metrics import pairwise_distances\n","\n","    # Calculate centroids\n","    unique_fam_ids = np.unique(mapped_fam_ids)\n","    centroids = np.array([df_original[mapped_fam_ids == fam_id].mean(axis=0) for fam_id in unique_fam_ids])\n","\n","    # Calculate distances from each point to each centroid and find the closest\n","    distances = pairwise_distances(df_original, centroids, metric='cosine')\n","    closest_centroids = np.argmin(distances, axis=1)\n","\n","    # Assign new fam_ids based on the closest centroid\n","    new_fam_ids = np.array([unique_fam_ids[idx] for idx in closest_centroids])\n","    fam_ids_array1 = np.array(new_fam_ids).reshape(-1, 1)\n","    new_array = np.concatenate((df_original, fam_ids_array1), axis=1)\n","\n","\n","    features = new_array[:, :-1].astype(float)\n","    # features = new_array[:, :-1]  # All rows, all columns except the last\n","\n","    # Extract 'fam_id' for labels\n","    labels = new_array[:, -1]  # Last column for labels\n","\n","\n","\n","\n","    sil_perf = silhouette_score(features, labels)\n","    calinski_harabasz = calinski_harabasz_score(features, labels)\n","    sil_perf_list.append(sil_perf)\n","    calinski_perf_list.append(calinski_harabasz)\n","    unique_fam_list.append(len(set(new_fam_ids)))\n","    iteration_list.append(ir)\n","\n","\n","    if len(set(new_fam_ids)) == breaker:\n","        break\n","\n","data = {\n","    'iteration_list': iteration_list,\n","    'sil_perf_list': sil_perf_list,\n","    'calinski_perf_list': calinski_perf_list,\n","    'unique_fam_list': unique_fam_list\n","}\n","\n","# Create DataFrame\n","df_eval = pd.DataFrame(data)\n","\n","df_eval\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DNjQkQMuJmg7","outputId":"8900fcd6-f417-4e1c-9b47-87a2f5a2b94d"},"outputs":[],"source":["len(set(new_fam_ids))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kge5yEbPJmg7"},"outputs":[],"source":["\n","filtered_df11['label'] = new_fam_ids\n","outliers_mask = ~not_outliers_mask  # Get the outliers mask\n","outliers_df11 = df11.iloc[outliers_mask].copy(deep=True)  # Extract outliers\n","outliers_df11['label'] = -1\n","df_comb = pd.concat([filtered_df11, outliers_df11], axis=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"soHabVojJmg7"},"outputs":[],"source":["df21 = df_comb.drop(['gpt_3_large_emb'],axis=1)\n","df21.to_excel('Godiva_done_32_pca.xlsx')"]},{"cell_type":"markdown","metadata":{"id":"bTGnLOLSJmg7"},"source":["\n","# ARRAY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ab3RYY3qJmg7"},"outputs":[],"source":["%%time\n","\n","import networkx as nx\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import pairwise_distances\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","import numpy as np\n","\n","# Assuming df_original is already defined\n","\n","df_original = df_iris.values.copy()\n","# df_original['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_original))]\n","\n","fam_ids = np.array(['unq_id_' + str(i) for i in range(len(df_original))])\n","fam_ids_array = np.array(fam_ids).reshape(-1, 1)\n","new_array = np.concatenate((df_original, fam_ids_array), axis=1)\n","\n","# 2. MAIN NETWORK CLUSTERING\n","\n","## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","# Assuming df is your DataFrame and it includes 'fam_id' along with the features.\n","\n","# Compute the pairwise distances using only the feature columns.\n","dist_matrix0 = pairwise_distances(df_original, metric='euclidean')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSqCP6zsJmg7"},"outputs":[],"source":["# Finding the closest valid entries\n","\n","dist_matrix = np.copy(dist_matrix0)\n","fam_ids = new_array[:, -1]\n","same_fam_id = new_array[:, -1][:, None] == new_array[:, -1]\n","dist_matrix[same_fam_id] = np.inf\n","\n","closest_indices = np.argmin(dist_matrix, axis=1)\n","closest_distances = np.min(dist_matrix, axis=1)\n","\n","# Handling migrations\n","closest_families = fam_ids[closest_indices]\n","dtype = [('fam_id', 'U10'), ('closest_distance', 'f8'), ('closest_family', 'U10')]\n","structured_array = np.array(list(zip(fam_ids, closest_distances, closest_families)), dtype=dtype)\n","sorted_structured_array = np.sort(structured_array, order=['fam_id', 'closest_distance'])\n","_, unique_indices = np.unique(sorted_structured_array['fam_id'], return_index=True)\n","result_structured_array = sorted_structured_array[unique_indices]\n","migration_map = {fam_id: closest_family for fam_id, closest_family in result_structured_array[['fam_id', 'closest_family']]}\n","migrate_to_family = np.array([migration_map[fam] for fam in fam_ids])\n","\n","\n","# Assuming fam_ids, migrate_to_families, and closest_distances are defined as NumPy arrays\n","\n","# Create a structured array for sorting and unique operation\n","dtype = [('migrate_to_family', 'U10'), ('closest_distance', float), ('fam_id', 'U10')]\n","structured_array = np.array(list(zip(migrate_to_family , closest_distances, fam_ids)), dtype=dtype)\n","\n","# Sort by 'migrate_to_family' and 'closest_distance'\n","sorted_array = np.sort(structured_array, order=['migrate_to_family', 'closest_distance'])\n","\n","# Remove duplicates\n","_, unique_indices = np.unique(sorted_array['migrate_to_family'], return_index=True)\n","unique_families = sorted_array[unique_indices]\n","\n","# Mapping for migration decisions\n","migration_decision_map = {entry['migrate_to_family']: entry['fam_id'] for entry in unique_families}\n","\n","# Calculate the minimum distance per migration family\n","unique_families_distances = {entry['migrate_to_family']: entry['closest_distance'] for entry in unique_families}\n","\n","# Assign migration permissions\n","can_migrate = np.array([closest_distances[i] == unique_families_distances[migrate_to_family[i]] for i in range(len(fam_ids))])\n","\n","import networkx as nx\n","\n","uf = nx.utils.UnionFind()\n","for i, entry in enumerate(structured_array):\n","    uf.union(entry['fam_id'], entry['migrate_to_family'])\n","\n","# Map the results back\n","fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","mapped_fam_ids = np.array([fam_id_mapping[fam_id] for fam_id in fam_ids])\n","\n","from sklearn.metrics import pairwise_distances\n","\n","# Calculate centroids\n","unique_fam_ids = np.unique(mapped_fam_ids)\n","centroids = np.array([df_original[mapped_fam_ids == fam_id].mean(axis=0) for fam_id in unique_fam_ids])\n","\n","# Calculate distances from each point to each centroid and find the closest\n","distances = pairwise_distances(df_original, centroids, metric='euclidean')\n","closest_centroids = np.argmin(distances, axis=1)\n","\n","# Assign new fam_ids based on the closest centroid\n","new_fam_ids = np.array([unique_fam_ids[idx] for idx in closest_centroids])\n","fam_ids_array1 = np.array(new_fam_ids).reshape(-1, 1)\n","new_array = np.concatenate((df_original, fam_ids_array1), axis=1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwHM0hPIJmg8"},"outputs":[],"source":["len(set(new_fam_ids))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3TxPcWMJmg8"},"outputs":[],"source":["%%time\n","\n","    # NUCER\n","    # 2. MAIN NETWORK CLUSTERING\n","\n","    ## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","    # dist_matrix = dist_matrix0.copy()\n","    dist_matrix = np.copy(dist_matrix0)\n","    # Reset the index of the DataFrame to keep track of original indices\n","\n","    # OPTIMIZATION 1\n","    #############\n","    # WAS\n","    # # Get a boolean matrix where True represents rows with the same 'fam_id'.\n","    # same_fam_id = df_original['fam_id'].values[:, None] == df_original['fam_id'].values\n","\n","    # # Set the distances for rows with the same 'fam_id' to infinity.\n","    # # np.inf is used here to ensure these distances are never considered as the minimum.\n","    # dist_matrix[same_fam_id] = np.inf\n","\n","    # NOW..SIGNIFICANTLY FASTER\n","    # df_with_index = df_original.reset_index()\n","\n","    # Perform the merge to find all pairs with the same 'fam_id'\n","    # same_fam_id = df_with_index.merge(\n","    #     df_with_index[['index', 'fam_id']], on='fam_id', suffixes=('_left', '_right'))\n","\n","    # # Minimal column merge to reduce memory footprint\n","    # same_fam_id = df_with_index[['index', 'fam_id']].merge(\n","    #     df_with_index[['index', 'fam_id']], on='fam_id', suffixes=('_left', '_right'))\n","    # Get a boolean matrix where True represents rows with the same 'fam_id'.\n","    same_fam_id = df_original['fam_id'].values[:, None] == df_original['fam_id'].values\n","\n","    # Set the distances for rows with the same 'fam_id' to infinity.\n","    # np.inf is used here to ensure these distances are never considered as the minimum.\n","    dist_matrix[same_fam_id] = np.inf\n","\n","\n","    ##########\n","\n","    # num_rows = len(df_original)\n","    # # Initialize a boolean matrix of size (num_rows, num_rows)\n","    # same_fam_id_matrix = np.zeros((num_rows, num_rows), dtype=bool)\n","\n","    # # Fill the boolean matrix with True where indices have the same 'fam_id'\n","    # same_fam_id_matrix[same_fam_id['index_left'].values, same_fam_id['index_right'].values] = True\n","\n","    # dist_matrix[same_fam_id_matrix]= np.inf\n","    ##########\n","\n","    # Find the index of the closest row and the distance for each row, ignoring those with the same 'fam_id'.\n","    closest_indices = dist_matrix.argmin(axis=1)\n","    closest_distances = dist_matrix.min(axis=1)\n","\n","    # Add the results back to the original DataFrame.\n","    df_original['closest_row'] = closest_indices\n","    df_original['closest_distance'] = closest_distances\n","\n","\n","    # OPTIMIZATION 2\n","    #############\n","    # WAS\n","\n","    # Assuming df_original is your original DataFrame and it includes the 'fam_id' column.\n","\n","    # 2B set up families to migrate to and solve conflicts if mulitple row in the same family want to migrate to the different families\n","    # Add a column for the family that the closest row belongs to for easier reference\n","    # df_original['closest_family'] = df_original['closest_row'].apply(lambda x: df_original.at[x, 'fam_id'] if pd.notnull(x) else None)\n","\n","    # # Determine the closest family for each original family based on the smallest distance\n","    # closest_family_for_group = df_original.groupby('fam_id').apply(lambda x: x.loc[x['closest_distance'].idxmin(), 'closest_family'])\n","\n","    # # Map the target family for migration back to the original DataFrame\n","    # df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","    # NOW\n","\n","    df_original['closest_family'] = df_original.loc[df_original['closest_row'], 'fam_id'].values\n","\n","    # Sort the DataFrame first by 'fam_id', then by 'closest_distance'\n","    sorted_df = df_original.sort_values(by=['fam_id', 'closest_distance'])\n","\n","    # Drop duplicates, keeping the first entry for each 'fam_id'\n","    # which will be the one with the smallest 'closest_distance' due to the sort order\n","    result_df = sorted_df.drop_duplicates(subset='fam_id', keep='first')\n","\n","    # Select only the 'fam_id' and 'closest_family' columns if necessary\n","    closest_family_for_group = result_df.set_index('fam_id')['closest_family']\n","\n","    # Map the target family for migration back to the original DataFrame\n","    df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","    ##################\n","\n","    # OPTIMIZATION 3\n","    #############\n","\n","    # WAS\n","    # Choosing only the closest family on the migrate_to_family side\n","    # Step 1: Determine the eligible family for each target family\n","\n","    # Group by 'migrate_to_family' and find the family with the closest distance for each target\n","    # eligible_families = df_original.groupby('migrate_to_family').apply(\n","    #     lambda x: x.loc[x['closest_distance'].idxmin(), 'fam_id']\n","    # ).reset_index().rename(columns={0: 'eligible_fam_id'})\n","\n","    # NOW\n","    # Sort the DataFrame by 'migrate_to_family' and 'closest_distance'\n","    sorted_df = df_original.sort_values(by=['migrate_to_family', 'closest_distance'])\n","\n","    # Drop duplicates, keeping the first entry for each 'migrate_to_family'\n","    # which will be the one with the smallest 'closest_distance' due to the sort order\n","    unique_families = sorted_df.drop_duplicates(subset='migrate_to_family', keep='first')\n","\n","    # Rename and clean up the result if necessary\n","    eligible_families = unique_families.rename(columns={'fam_id': 'eligible_fam_id'})[['migrate_to_family', 'eligible_fam_id']]\n","\n","    ##############\n","\n","    # Step 2: Assign migration permission based on eligibility\n","    # Merge the eligible families DataFrame back to the original DataFrame to assign migration permission\n","    df_original = df_original.merge(\n","        eligible_families,\n","        how='left',\n","        left_on=['migrate_to_family', 'fam_id'],\n","        right_on=['migrate_to_family', 'eligible_fam_id']\n","    )\n","\n","\n","    # Step 1 & 2: Group by 'migrate_to_family' and find all families with the smallest distance for each target\n","    # This involves creating a temporary DataFrame to calculate minimum distances for each migration target group\n","    min_distances = df_original.groupby('migrate_to_family')['closest_distance'].transform('min')\n","\n","    # Step 3: Assign migration permission based on matching the minimum distance\n","    # Families that match the minimum distance to their target are allowed to migrate\n","    df_original['can_migrate'] = df_original['closest_distance'] == min_distances\n","\n","    # Step 1: Find all fam_id values where at least one row has can_migrate = True\n","    fam_ids_to_update = df_original[df_original['can_migrate'] == True]['fam_id'].unique()\n","\n","    # Step 2: Update can_migrate for rows where fam_id matches the identified fam_ids\n","    df_original.loc[df_original['fam_id'].isin(fam_ids_to_update), 'can_migrate'] = True\n","\n","    # Logic to update 'migrate_to_family' where 'can_migrate' is False\n","    df_original.loc[df_original['can_migrate'] == False, 'migrate_to_family'] = df_original['fam_id']\n","\n","    # Drop the 'eligible_fam_id' column as it's no longer needed\n","    df_original.drop('eligible_fam_id', axis=1, inplace=True)\n","\n","    # Initialize NetworkX UnionFind\n","\n","\n","\n","    ## OPTIMIZATION 4\n","    ##################################\n","\n","    # WAS\n","    # # Using DataFrame.apply() to perform Union operations on all rows at once\n","    # uf = nx.utils.UnionFind()\n","    # df_original.apply(lambda row: uf.union(row['fam_id'], row['migrate_to_family']), axis=1)\n","\n","    # # Mapping the results back to the DataFrame can be optimized by\n","    # # creating a mapping dictionary from the UnionFind structure and then using map()\n","    # fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","\n","    # df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping)\n","\n","    # Drop unnecessary columns in a more concise way\n","    # columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","    # df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","\n","    # NOW\n","    # Iterate over DataFrame rows using itertuples() for better performance\n","    uf = nx.utils.UnionFind()\n","    for row in df_original.itertuples():\n","        uf.union(row.fam_id, row.migrate_to_family)\n","\n","    # Creating a mapping dictionary from the UnionFind structure\n","    fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","\n","    # Ensure that all original IDs are covered in the mapping\n","    df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping).fillna(df_original['fam_id'])\n","\n","    # Drop unnecessary columns in a more concise way\n","    columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","    df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","\n","        # # Solution to all my issues.. recenter\n","    def calculate_centroids(df):\n","            \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","            centroids = df.groupby('fam_id').mean()\n","            return centroids\n","\n","    def reassign_families(df, centroids):\n","        \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","        # Calculate distances between each row and each centroid\n","        distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","        # Find the closest centroid for each row\n","        closest_centroids = np.argmin(distances, axis=1)\n","\n","        # Map centroid indices back to family labels\n","        index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","        df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","        return df\n","\n","    # # Calculate initial centroids\n","    centroids = calculate_centroids(df_original)\n","\n","    # Reassign families based on closest centroid\n","    df_original = reassign_families(df_original, centroids)\n","\n","    ####################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FanaSbvUJmg8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7VrkiRF4Jmg8"},"outputs":[],"source":["# Manual"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f0S6yoxnJmg8"},"outputs":[],"source":["%%time\n","\n","df_original = df_iris.copy(deep=True)\n","df_original['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_original))]\n","\n","# 2. MAIN NETWORK CLUSTERING\n","\n","## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","import networkx as nx\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import pairwise_distances\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","# Assuming df is your DataFrame and it includes 'fam_id' along with the features.\n","\n","# Compute the pairwise distances using only the feature columns.\n","features = df_original.drop('fam_id', axis=1)\n","dist_matrix0 = pairwise_distances(features, metric='euclidean')\n","\n","\n","# # Solution to all my issues.. recenter\n","def calculate_centroids(df):\n","        \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","        centroids = df.groupby('fam_id').mean()\n","        return centroids\n","\n","def reassign_families(df, centroids):\n","    \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","    # Calculate distances between each row and each centroid\n","    distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","    # Find the closest centroid for each row\n","    closest_centroids = np.argmin(distances, axis=1)\n","\n","    # Map centroid indices back to family labels\n","    index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","    df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","    return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CUCtMYbmJmg8"},"outputs":[],"source":["%%time\n","\n","\n","\n","    # NUCER\n","    # 2. MAIN NETWORK CLUSTERING\n","\n","    ## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","    # dist_matrix = dist_matrix0.copy()\n","    dist_matrix = np.copy(dist_matrix0)\n","    # Reset the index of the DataFrame to keep track of original indices\n","\n","    # OPTIMIZATION 1\n","    #############\n","    # WAS\n","    # # Get a boolean matrix where True represents rows with the same 'fam_id'.\n","    # same_fam_id = df_original['fam_id'].values[:, None] == df_original['fam_id'].values\n","\n","    # # Set the distances for rows with the same 'fam_id' to infinity.\n","    # # np.inf is used here to ensure these distances are never considered as the minimum.\n","    # dist_matrix[same_fam_id] = np.inf\n","\n","    # NOW..SIGNIFICANTLY FASTER\n","    # df_with_index = df_original.reset_index()\n","\n","    # Perform the merge to find all pairs with the same 'fam_id'\n","    # same_fam_id = df_with_index.merge(\n","    #     df_with_index[['index', 'fam_id']], on='fam_id', suffixes=('_left', '_right'))\n","\n","    # # Minimal column merge to reduce memory footprint\n","    # same_fam_id = df_with_index[['index', 'fam_id']].merge(\n","    #     df_with_index[['index', 'fam_id']], on='fam_id', suffixes=('_left', '_right'))\n","# Get a boolean matrix where True represents rows with the same 'fam_id'.\n","    same_fam_id = df_original['fam_id'].values[:, None] == df_original['fam_id'].values\n","\n","    # Set the distances for rows with the same 'fam_id' to infinity.\n","    # np.inf is used here to ensure these distances are never considered as the minimum.\n","    dist_matrix[same_fam_id] = np.inf\n","\n","\n","    ##########\n","\n","    # num_rows = len(df_original)\n","    # # Initialize a boolean matrix of size (num_rows, num_rows)\n","    # same_fam_id_matrix = np.zeros((num_rows, num_rows), dtype=bool)\n","\n","    # # Fill the boolean matrix with True where indices have the same 'fam_id'\n","    # same_fam_id_matrix[same_fam_id['index_left'].values, same_fam_id['index_right'].values] = True\n","\n","    # dist_matrix[same_fam_id_matrix]= np.inf\n","    ##########\n","\n","    # Find the index of the closest row and the distance for each row, ignoring those with the same 'fam_id'.\n","    closest_indices = dist_matrix.argmin(axis=1)\n","    closest_distances = dist_matrix.min(axis=1)\n","\n","    # Add the results back to the original DataFrame.\n","    df_original['closest_row'] = closest_indices\n","    df_original['closest_distance'] = closest_distances\n","\n","\n","    # OPTIMIZATION 2\n","    #############\n","    # WAS\n","\n","    # Assuming df_original is your original DataFrame and it includes the 'fam_id' column.\n","\n","    # 2B set up families to migrate to and solve conflicts if mulitple row in the same family want to migrate to the different families\n","    # Add a column for the family that the closest row belongs to for easier reference\n","    # df_original['closest_family'] = df_original['closest_row'].apply(lambda x: df_original.at[x, 'fam_id'] if pd.notnull(x) else None)\n","\n","    # # Determine the closest family for each original family based on the smallest distance\n","    # closest_family_for_group = df_original.groupby('fam_id').apply(lambda x: x.loc[x['closest_distance'].idxmin(), 'closest_family'])\n","\n","    # # Map the target family for migration back to the original DataFrame\n","    # df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","    # NOW\n","\n","    df_original['closest_family'] = df_original.loc[df_original['closest_row'], 'fam_id'].values\n","\n","    # Sort the DataFrame first by 'fam_id', then by 'closest_distance'\n","    sorted_df = df_original.sort_values(by=['fam_id', 'closest_distance'])\n","\n","    # Drop duplicates, keeping the first entry for each 'fam_id'\n","    # which will be the one with the smallest 'closest_distance' due to the sort order\n","    result_df = sorted_df.drop_duplicates(subset='fam_id', keep='first')\n","\n","    # Select only the 'fam_id' and 'closest_family' columns if necessary\n","    closest_family_for_group = result_df.set_index('fam_id')['closest_family']\n","\n","    # Map the target family for migration back to the original DataFrame\n","    df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","    ##################\n","\n","    # OPTIMIZATION 3\n","    #############\n","\n","    # WAS\n","    # Choosing only the closest family on the migrate_to_family side\n","    # Step 1: Determine the eligible family for each target family\n","\n","    # Group by 'migrate_to_family' and find the family with the closest distance for each target\n","    # eligible_families = df_original.groupby('migrate_to_family').apply(\n","    #     lambda x: x.loc[x['closest_distance'].idxmin(), 'fam_id']\n","    # ).reset_index().rename(columns={0: 'eligible_fam_id'})\n","\n","    # NOW\n","    # Sort the DataFrame by 'migrate_to_family' and 'closest_distance'\n","    sorted_df = df_original.sort_values(by=['migrate_to_family', 'closest_distance'])\n","\n","    # Drop duplicates, keeping the first entry for each 'migrate_to_family'\n","    # which will be the one with the smallest 'closest_distance' due to the sort order\n","    unique_families = sorted_df.drop_duplicates(subset='migrate_to_family', keep='first')\n","\n","    # Rename and clean up the result if necessary\n","    eligible_families = unique_families.rename(columns={'fam_id': 'eligible_fam_id'})[['migrate_to_family', 'eligible_fam_id']]\n","\n","    ##############\n","\n","    # Step 2: Assign migration permission based on eligibility\n","    # Merge the eligible families DataFrame back to the original DataFrame to assign migration permission\n","    df_original = df_original.merge(\n","        eligible_families,\n","        how='left',\n","        left_on=['migrate_to_family', 'fam_id'],\n","        right_on=['migrate_to_family', 'eligible_fam_id']\n","    )\n","\n","\n","    # Step 1 & 2: Group by 'migrate_to_family' and find all families with the smallest distance for each target\n","    # This involves creating a temporary DataFrame to calculate minimum distances for each migration target group\n","    min_distances = df_original.groupby('migrate_to_family')['closest_distance'].transform('min')\n","\n","    # Step 3: Assign migration permission based on matching the minimum distance\n","    # Families that match the minimum distance to their target are allowed to migrate\n","    df_original['can_migrate'] = df_original['closest_distance'] == min_distances\n","\n","    # Step 1: Find all fam_id values where at least one row has can_migrate = True\n","    fam_ids_to_update = df_original[df_original['can_migrate'] == True]['fam_id'].unique()\n","\n","    # Step 2: Update can_migrate for rows where fam_id matches the identified fam_ids\n","    df_original.loc[df_original['fam_id'].isin(fam_ids_to_update), 'can_migrate'] = True\n","\n","    # Logic to update 'migrate_to_family' where 'can_migrate' is False\n","    df_original.loc[df_original['can_migrate'] == False, 'migrate_to_family'] = df_original['fam_id']\n","\n","    # Drop the 'eligible_fam_id' column as it's no longer needed\n","    df_original.drop('eligible_fam_id', axis=1, inplace=True)\n","\n","    # Initialize NetworkX UnionFind\n","\n","\n","\n","    ## OPTIMIZATION 4\n","    ##################################\n","\n","    # WAS\n","    # # Using DataFrame.apply() to perform Union operations on all rows at once\n","    # uf = nx.utils.UnionFind()\n","    # df_original.apply(lambda row: uf.union(row['fam_id'], row['migrate_to_family']), axis=1)\n","\n","    # # Mapping the results back to the DataFrame can be optimized by\n","    # # creating a mapping dictionary from the UnionFind structure and then using map()\n","    # fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","\n","    # df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping)\n","\n","    # Drop unnecessary columns in a more concise way\n","    # columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","    # df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","\n","    # NOW\n","    # Iterate over DataFrame rows using itertuples() for better performance\n","    uf = nx.utils.UnionFind()\n","    for row in df_original.itertuples():\n","        uf.union(row.fam_id, row.migrate_to_family)\n","\n","    # Creating a mapping dictionary from the UnionFind structure\n","    fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","\n","    # Ensure that all original IDs are covered in the mapping\n","    df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping).fillna(df_original['fam_id'])\n","\n","    # Drop unnecessary columns in a more concise way\n","    columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","    df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","\n","    # # Calculate initial centroids\n","    centroids = calculate_centroids(df_original)\n","\n","    # Reassign families based on closest centroid\n","    df_original = reassign_families(df_original, centroids)\n","\n","    ####################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQSuiqUwJmg8"},"outputs":[],"source":["df_original.fam_id.nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQ84JRz8Jmg8","jupyter":{"source_hidden":true}},"outputs":[],"source":["%%time\n","\n","    # NUCER\n","    # 2. MAIN NETWORK CLUSTERING\n","\n","    ## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","    # dist_matrix = dist_matrix0.copy()\n","    dist_matrix = np.copy(dist_matrix0)\n","    # Reset the index of the DataFrame to keep track of original indices\n","\n","    # OPTIMIZATION 1\n","    #############\n","    # WAS\n","    # # Get a boolean matrix where True represents rows with the same 'fam_id'.\n","    # same_fam_id = df_original['fam_id'].values[:, None] == df_original['fam_id'].values\n","\n","    # # Set the distances for rows with the same 'fam_id' to infinity.\n","    # # np.inf is used here to ensure these distances are never considered as the minimum.\n","    # dist_matrix[same_fam_id] = np.inf\n","\n","    # NOW..SIGNIFICANTLY FASTER\n","    df_with_index = df_original.reset_index()\n","\n","    # Perform the merge to find all pairs with the same 'fam_id'\n","    same_fam_id = df_with_index.merge(\n","        df_with_index[['index', 'fam_id']], on='fam_id', suffixes=('_left', '_right'))\n","\n","    num_rows = len(df_original)\n","    # Initialize a boolean matrix of size (num_rows, num_rows)\n","    same_fam_id_matrix = np.zeros((num_rows, num_rows), dtype=bool)\n","\n","    # Fill the boolean matrix with True where indices have the same 'fam_id'\n","    same_fam_id_matrix[same_fam_id['index_left'].values, same_fam_id['index_right'].values] = True\n","\n","    dist_matrix[same_fam_id_matrix]= np.inf\n","    ##########\n","\n","    # Find the index of the closest row and the distance for each row, ignoring those with the same 'fam_id'.\n","    closest_indices = dist_matrix.argmin(axis=1)\n","    closest_distances = dist_matrix.min(axis=1)\n","\n","    # Add the results back to the original DataFrame.\n","    df_original['closest_row'] = closest_indices\n","    df_original['closest_distance'] = closest_distances\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2B2UJ8XMJmg8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZkSPsgyJmg9"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vFR2bc_rJmg9"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GYztWagKJmg9"},"outputs":[],"source":["df_original.fam_id.nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-3kUX1RJmg9"},"outputs":[],"source":["# # Medoid\n","\n","# # Reset the index to ensure alignment\n","# df_with_index = df_original.reset_index()\n","# # Perform the merge on 'fam_id' to identify the same family pairings\n","# same_fam_id = df_with_index.merge(df_with_index[['index', 'fam_id']],\n","#                     on='fam_id', suffixes=('_left', '_right'))\n","\n","# # Initialize a  full infinity matrix\n","# intra_fam_dist_matrix = np.full((len(df_original), len(df_original)), np.inf)\n","\n","# # Fill only the distances for the same family members\n","# intra_fam_dist_matrix[same_fam_id['index_left'].values, same_fam_id['index_right'].values] = dist_matrix0[same_fam_id['index_left'].values, same_fam_id['index_right'].values]\n","\n","# # Sum distances in the intra-family distance matrix\n","# sum_distances = np.sum(intra_fam_dist_matrix, axis=1)\n","\n","# # Add a column to df_original to identify the medoid's index within each family\n","# df_with_index['sum_distances'] = sum_distances\n","\n","# # Identify the index with the minimum sum of distances within each family group\n","# medoids = df_with_index.loc[df_with_index.groupby('fam_id')['sum_distances'].idxmin()]\n","\n","# medoid_family_mapping = df_original.loc[medoids.index, ['fam_id']]\n","# medoid_family_mapping['medoid_index'] = medoids.index\n","# # Select only the columns of the distance matrix corresponding to the medoids\n","# medoid_distances = dist_matrix0[:, medoid_family_mapping['medoid_index']]\n","# closest_medoid_indices = np.argmin(medoid_distances, axis=1)\n","# # Create a mapping from index in the medoid_distances array to family ID\n","# index_to_family_id = medoid_family_mapping['fam_id'].iloc[closest_medoid_indices].values\n","# df_original['fam_id'] = index_to_family_id"]},{"cell_type":"markdown","metadata":{"id":"HlDe3CLdJmg9"},"source":["# Import test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrOGOfUfJmg9"},"outputs":[],"source":["# With outlier removal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":197,"status":"ok","timestamp":1714574629836,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"bKQFjvMtJmg9","outputId":"939c3545-b8d0-41b7-ad8e-f10e530491b8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":392,"status":"ok","timestamp":1714574632890,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"TOHTlCLrJmg9","outputId":"1a692e22-6fc6-4435-e7e7-4f43ba94bcb8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nYsJ7ExpJmg9"},"outputs":[],"source":["# idea.. when change is 1, which ever has the highest calinski wins\n","breaker = 3"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 188 ms\n","Wall time: 382 ms\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>iteration_list</th>\n","      <th>sil_perf_list</th>\n","      <th>calinski_perf_list</th>\n","      <th>unique_fam_list</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.307999</td>\n","      <td>394.834372</td>\n","      <td>303</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.361714</td>\n","      <td>287.235809</td>\n","      <td>175</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.405565</td>\n","      <td>256.649844</td>\n","      <td>103</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.428583</td>\n","      <td>297.048743</td>\n","      <td>62</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.422243</td>\n","      <td>281.018386</td>\n","      <td>35</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>0.435867</td>\n","      <td>323.781674</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>0.387908</td>\n","      <td>460.719629</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>0.281347</td>\n","      <td>326.963439</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>0.355716</td>\n","      <td>275.212680</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>0.277275</td>\n","      <td>336.132545</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>0.273687</td>\n","      <td>474.058596</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>0.794613</td>\n","      <td>850.404654</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    iteration_list  sil_perf_list  calinski_perf_list  unique_fam_list\n","0                0       0.307999          394.834372              303\n","1                1       0.361714          287.235809              175\n","2                2       0.405565          256.649844              103\n","3                3       0.428583          297.048743               62\n","4                4       0.422243          281.018386               35\n","5                5       0.435867          323.781674               22\n","6                6       0.387908          460.719629               13\n","7                7       0.281347          326.963439                8\n","8                8       0.355716          275.212680                5\n","9                9       0.277275          336.132545                4\n","10              10       0.273687          474.058596                3\n","11              11       0.794613          850.404654                2"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","\n","breaker = 2\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import pairwise_distances\n","# df_original = df_iris1.copy(deep=True)\n","#df_original = df_credit_card_3.copy(deep=True)\n","all_outliers = pd.DataFrame()\n","\n","all_df_original = pd.DataFrame()\n","df_original = df_14.copy(deep=True)\n","\n","# # Compute the pairwise distances\n","# dist_matrix = pairwise_distances(df, metric='euclidean')\n","\n","# # Replace diagonal with np.inf to ignore self-distance\n","# np.fill_diagonal(dist_matrix, np.inf)\n","\n","# # Find the index of the closest row and the distance for each row\n","# closest_indices = dist_matrix.argmin(axis=1)\n","# closest_distances = dist_matrix.min(axis=1)\n","\n","# # Add the results to the original DataFrame\n","# df['Closest_Row'] = closest_indices\n","# df['Distance'] = closest_distances\n","\n","# # 1. Remove outliers after\n","# # We will modify the function to also return the outliers\n","\n","# def remove_upper_outliers_and_save(df, column_name):\n","#     quartile_1, quartile_3 = np.percentile(df[column_name], [25, 75])\n","#     iqr = quartile_3 - quartile_1\n","#     upper_bound = quartile_3 + (iqr * 1.5) # 1.5\n","\n","#     # Filter the DataFrame to remove data points above the upper bound\n","#     filtered_df = df[df[column_name] <= upper_bound]\n","#     # Save the outliers in a separate DataFrame\n","#     outliers_df = df[df[column_name] > upper_bound]\n","\n","#     return filtered_df, outliers_df\n","\n","# # Apply the modified function to remove upper outliers from 'Distance' column and save the removed outliers\n","# filtered_df, outliers_df = remove_upper_outliers_and_save(df, 'Distance')\n","\n","# # Now we have two DataFrames: `filtered_df` without outliers and `outliers_df` with only the removed outliers\n","# # (filtered_df, outliers_df)\n","\n","# outliers_df0 = outliers_df.copy(deep=True)\n","# del(outliers_df)\n","# #filtered_df.reset_index(inplace=True, drop=True)\n","\n","# df_original = filtered_df.copy(deep=True)\n","# df_original.drop(['Closest_Row','Distance'], axis = 1, inplace=True)\n","# outliers_df0.drop(['Closest_Row','Distance'], axis = 1, inplace=True)\n","# outliers_df0['fam_id'] = -1\n","# all_outliers = pd.concat([all_outliers, outliers_df0], ignore_index=True)\n","# # del(outliers_df0)\n","\n","# df_clean1 = df_cleaned.loc[df_original.index].copy(deep=True)\n","# df_clean1.reset_index(inplace=True, drop=True)\n","\n","# # df_original['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_original))]\n","# df_original.reset_index(drop=True, inplace=True)\n","#\n","#\n","#\n","# Using factorize to encode 'fam_id'\n","# df_original['fam_id'] = pd.factorize(df_original['fam_id'])[0]\n","\n","# # Convert the encoded numbers to strings\n","# df_original['fam_id'] = df_original['fam_id'].astype(str)\n","\n","\n","\n","\n","#\n","#\n","#\n","#\n","#44\n","# df_original = df_iris.copy(deep=True)\n","df_original['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_original))]\n","\n","# 2. MAIN NETWORK CLUSTERING\n","\n","## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","import networkx as nx\n","import numpy as np\n","import pandas as pd\n","from scipy.spatial.distance import cdist\n","\n","from sklearn.metrics import pairwise_distances\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","# Assuming df is your DataFrame and it includes 'fam_id' along with the features.\n","\n","# Compute the pairwise distances using only the feature columns.\n","features = df_original.drop('fam_id', axis=1)\n","dist_matrix0 = pairwise_distances(features, metric='euclidean')\n","\n","\n","# # Solution to all my issues.. recenter\n","def calculate_centroids(df):\n","        \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","        centroids = df.groupby('fam_id').mean()\n","        return centroids\n","\n","def reassign_families(df, centroids):\n","    \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","    # Calculate distances between each row and each centroid\n","    distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","    # Find the closest centroid for each row\n","    closest_centroids = np.argmin(distances, axis=1)\n","\n","    # Map centroid indices back to family labels\n","    index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","    df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","    return df\n","\n","iteration_list = []\n","sil_perf_list = []\n","calinski_perf_list = []\n","unique_fam_list = []\n","\n","\n","# NUCER\n","\n","for ir in range(50):\n","\n","    # 2. MAIN NETWORK CLUSTERING\n","\n","    ## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","    # dist_matrix = dist_matrix0.copy()\n","    features = df_original.drop('fam_id', axis=1)\n","    dist_matrix0 = pairwise_distances(features, metric='euclidean')\n","\n","    dist_matrix = np.copy(dist_matrix0)\n","    # Reset the index of the DataFrame to keep track of original indices\n","\n","    # OPTIMIZATION 1\n","    #############\n","    # WAS\n","    # # Get a boolean matrix where True represents rows with the same 'fam_id'.\n","    # same_fam_id = df_original['fam_id'].values[:, None] == df_original['fam_id'].values\n","\n","    # # Set the distances for rows with the same 'fam_id' to infinity.\n","    # # np.inf is used here to ensure these distances are never considered as the minimum.\n","    # dist_matrix[same_fam_id] = np.inf\n","\n","    # NOW..SIGNIFICANTLY FASTER\n","    # df_with_index = df_original.reset_index()\n","\n","    # # Perform the merge to find all pairs with the same 'fam_id'\n","    # same_fam_id = df_with_index.merge(\n","    #     df_with_index[['index', 'fam_id']], on='fam_id', suffixes=('_left', '_right'))\n","\n","    # num_rows = len(df_original)\n","    # # Initialize a boolean matrix of size (num_rows, num_rows)\n","    # same_fam_id_matrix = np.zeros((num_rows, num_rows), dtype=bool)\n","\n","    # # Fill the boolean matrix with True where indices have the same 'fam_id'\n","    # same_fam_id_matrix[same_fam_id['index_left'].values, same_fam_id['index_right'].values] = True\n","\n","    # dist_matrix[same_fam_id_matrix]= np.inf\n","    ##########\n","\n","    # Get a boolean matrix where True represents rows with the same 'fam_id'.\n","    same_fam_id = df_original['fam_id'].values[:, None] == df_original['fam_id'].values\n","\n","    # Set the distances for rows with the same 'fam_id' to infinity.\n","    # np.inf is used here to ensure these distances are never considered as the minimum.\n","    dist_matrix[same_fam_id] = np.inf\n","\n","    # Find the index of the closest row and the distance for each row, ignoring those with the same 'fam_id'.\n","    closest_indices = dist_matrix.argmin(axis=1)\n","    closest_distances = dist_matrix.min(axis=1)\n","\n","    # Add the results back to the original DataFrame.\n","    df_original['closest_row'] = closest_indices\n","    df_original['closest_distance'] = closest_distances\n","\n","    # OPTIMIZATION 2\n","    #############\n","    # WAS\n","\n","    # Assuming df_original is your original DataFrame and it includes the 'fam_id' column.\n","\n","    # 2B set up families to migrate to and solve conflicts if mulitple row in the same family want to migrate to the different families\n","    # Add a column for the family that the closest row belongs to for easier reference\n","    # df_original['closest_family'] = df_original['closest_row'].apply(lambda x: df_original.at[x, 'fam_id'] if pd.notnull(x) else None)\n","\n","    # # Determine the closest family for each original family based on the smallest distance\n","    # closest_family_for_group = df_original.groupby('fam_id').apply(lambda x: x.loc[x['closest_distance'].idxmin(), 'closest_family'])\n","\n","    # # Map the target family for migration back to the original DataFrame\n","    # df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","    # NOW\n","\n","    df_original['closest_family'] = df_original.loc[df_original['closest_row'], 'fam_id'].values\n","\n","\n","    # Sort the DataFrame first by 'fam_id', then by 'closest_distance'\n","    sorted_df = df_original.sort_values(by=['fam_id', 'closest_distance'])\n","\n","    # Drop duplicates, keeping the first entry for each 'fam_id'\n","    # which will be the one with the smallest 'closest_distance' due to the sort order\n","    result_df = sorted_df.drop_duplicates(subset='fam_id', keep='first')\n","\n","    # Select only the 'fam_id' and 'closest_family' columns if necessary\n","    closest_family_for_group = result_df.set_index('fam_id')['closest_family']\n","\n","    # Map the target family for migration back to the original DataFrame\n","    df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","    ##################\n","\n","    # OPTIMIZATION 3\n","    #############\n","\n","    # WAS\n","    # Choosing only the closest family on the migrate_to_family side\n","    # Step 1: Determine the eligible family for each target family\n","\n","    # Group by 'migrate_to_family' and find the family with the closest distance for each target\n","    # eligible_families = df_original.groupby('migrate_to_family').apply(\n","    #     lambda x: x.loc[x['closest_distance'].idxmin(), 'fam_id']\n","    # ).reset_index().rename(columns={0: 'eligible_fam_id'})\n","\n","    # NOW\n","    # Sort the DataFrame by 'migrate_to_family' and 'closest_distance'\n","    sorted_df = df_original.sort_values(by=['migrate_to_family', 'closest_distance'])\n","\n","    # Drop duplicates, keeping the first entry for each 'migrate_to_family'\n","    # which will be the one with the smallest 'closest_distance' due to the sort order\n","    unique_families = sorted_df.drop_duplicates(subset='migrate_to_family', keep='first')\n","\n","    # Rename and clean up the result if necessary\n","    eligible_families = unique_families.rename(columns={'fam_id': 'eligible_fam_id'})[['migrate_to_family', 'eligible_fam_id']]\n","\n","    ##############\n","\n","    # Step 2: Assign migration permission based on eligibility\n","    # Merge the eligible families DataFrame back to the original DataFrame to assign migration permission\n","    df_original = df_original.merge(\n","        eligible_families,\n","        how='left',\n","        left_on=['migrate_to_family', 'fam_id'],\n","        right_on=['migrate_to_family', 'eligible_fam_id']\n","    )\n","\n","\n","    # Step 1 & 2: Group by 'migrate_to_family' and find all families with the smallest distance for each target\n","    # This involves creating a temporary DataFrame to calculate minimum distances for each migration target group\n","    min_distances = df_original.groupby('migrate_to_family')['closest_distance'].transform('min')\n","\n","    # Step 3: Assign migration permission based on matching the minimum distance\n","    # Families that match the minimum distance to their target are allowed to migrate\n","    df_original['can_migrate'] = df_original['closest_distance'] == min_distances\n","\n","    # Step 1: Find all fam_id values where at least one row has can_migrate = True\n","    fam_ids_to_update = df_original[df_original['can_migrate'] == True]['fam_id'].unique()\n","\n","    # Step 2: Update can_migrate for rows where fam_id matches the identified fam_ids\n","    df_original.loc[df_original['fam_id'].isin(fam_ids_to_update), 'can_migrate'] = True\n","\n","    # Logic to update 'migrate_to_family' where 'can_migrate' is False\n","    df_original.loc[df_original['can_migrate'] == False, 'migrate_to_family'] = df_original['fam_id']\n","\n","    # Drop the 'eligible_fam_id' column as it's no longer needed\n","    df_original.drop('eligible_fam_id', axis=1, inplace=True)\n","\n","    # Initialize NetworkX UnionFind\n","\n","\n","\n","    ## OPTIMIZATION 4\n","    ##################################\n","\n","    # WAS\n","    # # Using DataFrame.apply() to perform Union operations on all rows at once\n","    # uf = nx.utils.UnionFind()\n","    # df_original.apply(lambda row: uf.union(row['fam_id'], row['migrate_to_family']), axis=1)\n","\n","    # # Mapping the results back to the DataFrame can be optimized by\n","    # # creating a mapping dictionary from the UnionFind structure and then using map()\n","    # fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","\n","    # df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping)\n","\n","    # Drop unnecessary columns in a more concise way\n","    # columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","    # df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","\n","    # NOW\n","    # Iterate over DataFrame rows using itertuples() for better performance\n","    uf = nx.utils.UnionFind()\n","    for row in df_original.itertuples():\n","        uf.union(row.fam_id, row.migrate_to_family)\n","\n","    # Creating a mapping dictionary from the UnionFind structure\n","    fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","\n","    # Ensure that all original IDs are covered in the mapping\n","    df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping).fillna(df_original['fam_id'])\n","\n","    # Drop unnecessary columns in a more concise way\n","    columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","    df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","    ####################################\n","\n","    # # Calculate initial centroids\n","    centroids = calculate_centroids(df_original)\n","\n","    # Reassign families based on closest centroid\n","    df_original = reassign_families(df_original, centroids)\n","    # df_original.reset_index(drop=True,inplace=True)\n","\n","    # # Outlier removal\n","\n","\n","    # #Assuming df_original is your DataFrame and it already contains 'fam_id'\n","    # #Select numeric columns for centroid calculation, excluding 'fam_id' or any other non-relevant columns\n","    # features = [col for col in df_original.select_dtypes(include=np.number).columns if col != 'fam_id']\n","\n","    # # Calculate the centroid for each family using a groupby operation, this remains efficient\n","    # centroids = df_original.groupby('fam_id')[features].transform('mean')\n","\n","    # # Calculate pairwise distances to centroid for each member without using apply()\n","    # # Using scipy's cdist function for efficient distance calculation\n","    # distances_to_centroid = cdist(df_original[features], centroids, metric='euclidean').diagonal()\n","    # df_original['distance_to_centroid'] = distances_to_centroid\n","\n","    # # Identify outliers using the IQR method for each family (only above upper bound), vectorized for efficiency\n","    # Q1 = df_original.groupby('fam_id')['distance_to_centroid'].transform(lambda x: x.quantile(0.25))\n","    # Q3 = df_original.groupby('fam_id')['distance_to_centroid'].transform(lambda x: x.quantile(0.75))\n","    # IQR = Q3 - Q1\n","    # upper_bound = Q3 + 1.5 * IQR\n","\n","    # # Assigning -1 to fam_id for outliers, more directly\n","    # df_original.loc[df_original['distance_to_centroid'] > upper_bound, 'fam_id'] = -1\n","\n","    # # Splitting outliers and non-outliers into separate DataFrames as needed\n","    # outliers_df = df_original[df_original['fam_id'] == -1].copy(deep=True)\n","    # df_original = df_original[df_original['fam_id'] != -1].copy(deep=True)\n","\n","    # del(all_df_original)\n","    # all_df_original = pd.DataFrame()\n","    # df_original.drop('distance_to_centroid', axis=1, inplace=True)\n","    # outliers_df.drop('distance_to_centroid', axis=1, inplace=True)\n","\n","    # # filtered_df1 = df_clean1.loc[df_original.index].copy()\n","    # # filtered_df1 = filtered_df1.join(df_original['fam_id'], how='left')\n","    # if ir == 0:\n","    #   filtered_df1 = df_clean1.join(df_original['fam_id'], how='right')\n","    #   filtered_outlier_df = df_clean1.loc[outliers_df.index]\n","    #   filtered_df1.reset_index(inplace=True, drop=True)\n","    # else:\n","    #   filtered_df2 = filtered_df1.join(df_original['fam_id'], how='right')\n","    #   filtered_outlier_df = filtered_df1.loc[outliers_df.index]\n","    #   filtered_outlier_df['fam_id'] = -1\n","    #   filtered_df2.reset_index(inplace=True, drop=True)\n","    #   filtered_df1 = filtered_df2.copy(deep=True)\n","\n","\n","    # all_df_original = pd.concat([all_df_original, filtered_df1], ignore_index=True)\n","    # all_outliers = pd.concat([all_outliers, filtered_outlier_df], ignore_index=True)\n","    # filtered_df1.drop('fam_id', inplace=True, axis=1)\n","    # # Optionally, drop the 'distance_to_centroid' column if no longer needed\n","\n","    df_original.reset_index(inplace=True, drop=True)\n","\n","    labels = df_original.fam_id\n","    selected_columns = df_original.drop(['fam_id'], axis = 1)\n","    sil_perf = silhouette_score(selected_columns, labels)\n","    calinski_harabasz = calinski_harabasz_score(selected_columns, labels)\n","    sil_perf_list.append(sil_perf)\n","    calinski_perf_list.append(calinski_harabasz)\n","    unique_fam_list.append(df_original.fam_id.nunique())\n","    iteration_list.append(ir)\n","\n","    if df_original.fam_id.nunique() == breaker:\n","        break\n","\n","data = {\n","    'iteration_list': iteration_list,\n","    'sil_perf_list': sil_perf_list,\n","    'calinski_perf_list': calinski_perf_list,\n","    'unique_fam_list': unique_fam_list\n","}\n","\n","# Create DataFrame\n","df_eval = pd.DataFrame(data)\n","\n","df_eval\n","#555"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"elapsed":4846,"status":"ok","timestamp":1714577362188,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"hI_jAPgbJmg9","jupyter":{"source_hidden":true},"outputId":"f5008416-e8e3-40b4-b4ea-481cb27555c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 500 ms\n","Wall time: 515 ms\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>iteration_list</th>\n","      <th>sil_perf_list</th>\n","      <th>calinski_perf_list</th>\n","      <th>unique_fam_list</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.319307</td>\n","      <td>961.621764</td>\n","      <td>257</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.393971</td>\n","      <td>569.983230</td>\n","      <td>148</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   iteration_list  sil_perf_list  calinski_perf_list  unique_fam_list\n","0               0       0.319307          961.621764              257\n","1               1       0.393971          569.983230              148"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","\n","breaker = 148\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import pairwise_distances\n","# df_original = df_iris1.copy(deep=True)\n","#df_original = df_credit_card_3.copy(deep=True)\n","all_outliers = pd.DataFrame()\n","\n","all_df_original = pd.DataFrame()\n","df = df_14.copy(deep=True)\n","\n","# Compute the pairwise distances\n","dist_matrix = pairwise_distances(df, metric='euclidean')\n","\n","# Replace diagonal with np.inf to ignore self-distance\n","np.fill_diagonal(dist_matrix, np.inf)\n","\n","# Find the index of the closest row and the distance for each row\n","closest_indices = dist_matrix.argmin(axis=1)\n","closest_distances = dist_matrix.min(axis=1)\n","\n","# Add the results to the original DataFrame\n","df['Closest_Row'] = closest_indices\n","df['Distance'] = closest_distances\n","\n","# 1. Remove outliers after\n","# We will modify the function to also return the outliers\n","\n","def remove_upper_outliers_and_save(df, column_name):\n","    quartile_1, quartile_3 = np.percentile(df[column_name], [25, 75])\n","    iqr = quartile_3 - quartile_1\n","    upper_bound = quartile_3 + (iqr * 1.5) # 1.5\n","\n","    # Filter the DataFrame to remove data points above the upper bound\n","    filtered_df = df[df[column_name] <= upper_bound]\n","    # Save the outliers in a separate DataFrame\n","    outliers_df = df[df[column_name] > upper_bound]\n","\n","    return filtered_df, outliers_df\n","\n","# Apply the modified function to remove upper outliers from 'Distance' column and save the removed outliers\n","filtered_df, outliers_df = remove_upper_outliers_and_save(df, 'Distance')\n","\n","# Now we have two DataFrames: `filtered_df` without outliers and `outliers_df` with only the removed outliers\n","# (filtered_df, outliers_df)\n","\n","outliers_df0 = outliers_df.copy(deep=True)\n","del(outliers_df)\n","#filtered_df.reset_index(inplace=True, drop=True)\n","\n","df_original = filtered_df.copy(deep=True)\n","df_original.drop(['Closest_Row','Distance'], axis = 1, inplace=True)\n","outliers_df0.drop(['Closest_Row','Distance'], axis = 1, inplace=True)\n","outliers_df0['fam_id'] = -1\n","all_outliers = pd.concat([all_outliers, outliers_df0], ignore_index=True)\n","# del(outliers_df0)\n","\n","df_clean1 = df_cleaned.loc[df_original.index].copy(deep=True)\n","df_clean1.reset_index(inplace=True, drop=True)\n","\n","# df_original['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_original))]\n","df_original.reset_index(drop=True, inplace=True)\n","#\n","#\n","#\n","# Using factorize to encode 'fam_id'\n","# df_original['fam_id'] = pd.factorize(df_original['fam_id'])[0]\n","\n","# # Convert the encoded numbers to strings\n","# df_original['fam_id'] = df_original['fam_id'].astype(str)\n","\n","\n","\n","\n","#\n","#\n","#\n","#\n","#44\n","# df_original = df_iris.copy(deep=True)\n","df_original['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_original))]\n","\n","# 2. MAIN NETWORK CLUSTERING\n","\n","## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","import networkx as nx\n","import numpy as np\n","import pandas as pd\n","from scipy.spatial.distance import cdist\n","\n","from sklearn.metrics import pairwise_distances\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","# Assuming df is your DataFrame and it includes 'fam_id' along with the features.\n","\n","# Compute the pairwise distances using only the feature columns.\n","features = df_original.drop('fam_id', axis=1)\n","dist_matrix0 = pairwise_distances(features, metric='euclidean')\n","\n","\n","# # Solution to all my issues.. recenter\n","def calculate_centroids(df):\n","        \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","        centroids = df.groupby('fam_id').mean()\n","        return centroids\n","\n","def reassign_families(df, centroids):\n","    \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","    # Calculate distances between each row and each centroid\n","    distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","    # Find the closest centroid for each row\n","    closest_centroids = np.argmin(distances, axis=1)\n","\n","    # Map centroid indices back to family labels\n","    index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","    df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","    return df\n","\n","\n","\n","iteration_list = []\n","sil_perf_list = []\n","calinski_perf_list = []\n","unique_fam_list = []\n","\n","\n","# NUCER\n","\n","for ir in range(50):\n","\n","    # 2. MAIN NETWORK CLUSTERING\n","\n","    ## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","    # dist_matrix = dist_matrix0.copy()\n","    features = df_original.drop('fam_id', axis=1)\n","    dist_matrix0 = pairwise_distances(features, metric='euclidean')\n","\n","    dist_matrix = np.copy(dist_matrix0)\n","    # Reset the index of the DataFrame to keep track of original indices\n","\n","    # OPTIMIZATION 1\n","    #############\n","    # WAS\n","    # # Get a boolean matrix where True represents rows with the same 'fam_id'.\n","    # same_fam_id = df_original['fam_id'].values[:, None] == df_original['fam_id'].values\n","\n","    # # Set the distances for rows with the same 'fam_id' to infinity.\n","    # # np.inf is used here to ensure these distances are never considered as the minimum.\n","    # dist_matrix[same_fam_id] = np.inf\n","\n","    # NOW..SIGNIFICANTLY FASTER\n","    # df_with_index = df_original.reset_index()\n","\n","    # # Perform the merge to find all pairs with the same 'fam_id'\n","    # same_fam_id = df_with_index.merge(\n","    #     df_with_index[['index', 'fam_id']], on='fam_id', suffixes=('_left', '_right'))\n","\n","    # num_rows = len(df_original)\n","    # # Initialize a boolean matrix of size (num_rows, num_rows)\n","    # same_fam_id_matrix = np.zeros((num_rows, num_rows), dtype=bool)\n","\n","    # # Fill the boolean matrix with True where indices have the same 'fam_id'\n","    # same_fam_id_matrix[same_fam_id['index_left'].values, same_fam_id['index_right'].values] = True\n","\n","    # dist_matrix[same_fam_id_matrix]= np.inf\n","    ##########\n","\n","    # Get a boolean matrix where True represents rows with the same 'fam_id'.\n","    same_fam_id = df_original['fam_id'].values[:, None] == df_original['fam_id'].values\n","\n","    # Set the distances for rows with the same 'fam_id' to infinity.\n","    # np.inf is used here to ensure these distances are never considered as the minimum.\n","    dist_matrix[same_fam_id] = np.inf\n","\n","    # Find the index of the closest row and the distance for each row, ignoring those with the same 'fam_id'.\n","    closest_indices = dist_matrix.argmin(axis=1)\n","    closest_distances = dist_matrix.min(axis=1)\n","\n","    # Add the results back to the original DataFrame.\n","    df_original['closest_row'] = closest_indices\n","    df_original['closest_distance'] = closest_distances\n","\n","    # OPTIMIZATION 2\n","    #############\n","    # WAS\n","\n","    # Assuming df_original is your original DataFrame and it includes the 'fam_id' column.\n","\n","    # 2B set up families to migrate to and solve conflicts if mulitple row in the same family want to migrate to the different families\n","    # Add a column for the family that the closest row belongs to for easier reference\n","    # df_original['closest_family'] = df_original['closest_row'].apply(lambda x: df_original.at[x, 'fam_id'] if pd.notnull(x) else None)\n","\n","    # # Determine the closest family for each original family based on the smallest distance\n","    # closest_family_for_group = df_original.groupby('fam_id').apply(lambda x: x.loc[x['closest_distance'].idxmin(), 'closest_family'])\n","\n","    # # Map the target family for migration back to the original DataFrame\n","    # df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","    # NOW\n","\n","    df_original['closest_family'] = df_original.loc[df_original['closest_row'], 'fam_id'].values\n","\n","\n","    # Sort the DataFrame first by 'fam_id', then by 'closest_distance'\n","    sorted_df = df_original.sort_values(by=['fam_id', 'closest_distance'])\n","\n","    # Drop duplicates, keeping the first entry for each 'fam_id'\n","    # which will be the one with the smallest 'closest_distance' due to the sort order\n","    result_df = sorted_df.drop_duplicates(subset='fam_id', keep='first')\n","\n","    # Select only the 'fam_id' and 'closest_family' columns if necessary\n","    closest_family_for_group = result_df.set_index('fam_id')['closest_family']\n","\n","    # Map the target family for migration back to the original DataFrame\n","    df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","    ##################\n","\n","    # OPTIMIZATION 3\n","    #############\n","\n","    # WAS\n","    # Choosing only the closest family on the migrate_to_family side\n","    # Step 1: Determine the eligible family for each target family\n","\n","    # Group by 'migrate_to_family' and find the family with the closest distance for each target\n","    # eligible_families = df_original.groupby('migrate_to_family').apply(\n","    #     lambda x: x.loc[x['closest_distance'].idxmin(), 'fam_id']\n","    # ).reset_index().rename(columns={0: 'eligible_fam_id'})\n","\n","    # NOW\n","    # Sort the DataFrame by 'migrate_to_family' and 'closest_distance'\n","    sorted_df = df_original.sort_values(by=['migrate_to_family', 'closest_distance'])\n","\n","    # Drop duplicates, keeping the first entry for each 'migrate_to_family'\n","    # which will be the one with the smallest 'closest_distance' due to the sort order\n","    unique_families = sorted_df.drop_duplicates(subset='migrate_to_family', keep='first')\n","\n","    # Rename and clean up the result if necessary\n","    eligible_families = unique_families.rename(columns={'fam_id': 'eligible_fam_id'})[['migrate_to_family', 'eligible_fam_id']]\n","\n","    ##############\n","\n","    # Step 2: Assign migration permission based on eligibility\n","    # Merge the eligible families DataFrame back to the original DataFrame to assign migration permission\n","    df_original = df_original.merge(\n","        eligible_families,\n","        how='left',\n","        left_on=['migrate_to_family', 'fam_id'],\n","        right_on=['migrate_to_family', 'eligible_fam_id']\n","    )\n","\n","\n","    # Step 1 & 2: Group by 'migrate_to_family' and find all families with the smallest distance for each target\n","    # This involves creating a temporary DataFrame to calculate minimum distances for each migration target group\n","    min_distances = df_original.groupby('migrate_to_family')['closest_distance'].transform('min')\n","\n","    # Step 3: Assign migration permission based on matching the minimum distance\n","    # Families that match the minimum distance to their target are allowed to migrate\n","    df_original['can_migrate'] = df_original['closest_distance'] == min_distances\n","\n","    # Step 1: Find all fam_id values where at least one row has can_migrate = True\n","    fam_ids_to_update = df_original[df_original['can_migrate'] == True]['fam_id'].unique()\n","\n","    # Step 2: Update can_migrate for rows where fam_id matches the identified fam_ids\n","    df_original.loc[df_original['fam_id'].isin(fam_ids_to_update), 'can_migrate'] = True\n","\n","    # Logic to update 'migrate_to_family' where 'can_migrate' is False\n","    df_original.loc[df_original['can_migrate'] == False, 'migrate_to_family'] = df_original['fam_id']\n","\n","    # Drop the 'eligible_fam_id' column as it's no longer needed\n","    df_original.drop('eligible_fam_id', axis=1, inplace=True)\n","\n","    # Initialize NetworkX UnionFind\n","\n","\n","\n","    ## OPTIMIZATION 4\n","    ##################################\n","\n","    # WAS\n","    # # Using DataFrame.apply() to perform Union operations on all rows at once\n","    # uf = nx.utils.UnionFind()\n","    # df_original.apply(lambda row: uf.union(row['fam_id'], row['migrate_to_family']), axis=1)\n","\n","    # # Mapping the results back to the DataFrame can be optimized by\n","    # # creating a mapping dictionary from the UnionFind structure and then using map()\n","    # fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","\n","    # df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping)\n","\n","    # Drop unnecessary columns in a more concise way\n","    # columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","    # df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","\n","    # NOW\n","    # Iterate over DataFrame rows using itertuples() for better performance\n","    uf = nx.utils.UnionFind()\n","    for row in df_original.itertuples():\n","        uf.union(row.fam_id, row.migrate_to_family)\n","\n","    # Creating a mapping dictionary from the UnionFind structure\n","    fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","\n","    # Ensure that all original IDs are covered in the mapping\n","    df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping).fillna(df_original['fam_id'])\n","\n","    # Drop unnecessary columns in a more concise way\n","    columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","    df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","    ####################################\n","\n","    # # Calculate initial centroids\n","    centroids = calculate_centroids(df_original)\n","\n","    # Reassign families based on closest centroid\n","    df_original = reassign_families(df_original, centroids)\n","    # df_original.reset_index(drop=True,inplace=True)\n","\n","    # Outlier removal\n","\n","\n","    #Assuming df_original is your DataFrame and it already contains 'fam_id'\n","    #Select numeric columns for centroid calculation, excluding 'fam_id' or any other non-relevant columns\n","    features = [col for col in df_original.select_dtypes(include=np.number).columns if col != 'fam_id']\n","\n","    # Calculate the centroid for each family using a groupby operation, this remains efficient\n","    centroids = df_original.groupby('fam_id')[features].transform('mean')\n","\n","    # Calculate pairwise distances to centroid for each member without using apply()\n","    # Using scipy's cdist function for efficient distance calculation\n","    distances_to_centroid = cdist(df_original[features], centroids, metric='euclidean').diagonal()\n","    df_original['distance_to_centroid'] = distances_to_centroid\n","\n","    # Identify outliers using the IQR method for each family (only above upper bound), vectorized for efficiency\n","    Q1 = df_original.groupby('fam_id')['distance_to_centroid'].transform(lambda x: x.quantile(0.25))\n","    Q3 = df_original.groupby('fam_id')['distance_to_centroid'].transform(lambda x: x.quantile(0.75))\n","    IQR = Q3 - Q1\n","    upper_bound = Q3 + 1.5 * IQR\n","\n","    # Assigning -1 to fam_id for outliers, more directly\n","    df_original.loc[df_original['distance_to_centroid'] > upper_bound, 'fam_id'] = -1\n","\n","    # Splitting outliers and non-outliers into separate DataFrames as needed\n","    outliers_df = df_original[df_original['fam_id'] == -1].copy(deep=True)\n","    df_original = df_original[df_original['fam_id'] != -1].copy(deep=True)\n","\n","    del(all_df_original)\n","    all_df_original = pd.DataFrame()\n","    df_original.drop('distance_to_centroid', axis=1, inplace=True)\n","    outliers_df.drop('distance_to_centroid', axis=1, inplace=True)\n","\n","    # filtered_df1 = df_clean1.loc[df_original.index].copy()\n","    # filtered_df1 = filtered_df1.join(df_original['fam_id'], how='left')\n","    if ir == 0:\n","      filtered_df1 = df_clean1.join(df_original['fam_id'], how='right')\n","      filtered_outlier_df = df_clean1.loc[outliers_df.index]\n","      filtered_df1.reset_index(inplace=True, drop=True)\n","    else:\n","      filtered_df2 = filtered_df1.join(df_original['fam_id'], how='right')\n","      filtered_outlier_df = filtered_df1.loc[outliers_df.index]\n","      filtered_outlier_df['fam_id'] = -1\n","      filtered_df2.reset_index(inplace=True, drop=True)\n","      filtered_df1 = filtered_df2.copy(deep=True)\n","\n","\n","    all_df_original = pd.concat([all_df_original, filtered_df1], ignore_index=True)\n","    all_outliers = pd.concat([all_outliers, filtered_outlier_df], ignore_index=True)\n","    filtered_df1.drop('fam_id', inplace=True, axis=1)\n","    # Optionally, drop the 'distance_to_centroid' column if no longer needed\n","\n","    df_original.reset_index(inplace=True, drop=True)\n","\n","    labels = df_original.fam_id\n","    selected_columns = df_original.drop(['fam_id'], axis = 1)\n","    sil_perf = silhouette_score(selected_columns, labels)\n","    calinski_harabasz = calinski_harabasz_score(selected_columns, labels)\n","    sil_perf_list.append(sil_perf)\n","    calinski_perf_list.append(calinski_harabasz)\n","    unique_fam_list.append(df_original.fam_id.nunique())\n","    iteration_list.append(ir)\n","\n","    if df_original.fam_id.nunique() == breaker:\n","        break\n","\n","data = {\n","    'iteration_list': iteration_list,\n","    'sil_perf_list': sil_perf_list,\n","    'calinski_perf_list': calinski_perf_list,\n","    'unique_fam_list': unique_fam_list\n","}\n","\n","# Create DataFrame\n","df_eval = pd.DataFrame(data)\n","\n","df_eval\n","#555"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["2"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["all_df_original.fam_id.nunique()"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":221,"status":"ok","timestamp":1714577367074,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"k1M2PCjGMoT-"},"outputs":[],"source":["df_comb1 = pd.concat([all_df_original, all_outliers], ignore_index=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":11489,"status":"ok","timestamp":1714577396376,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"93nuAR6WO__U"},"outputs":[],"source":["df_comb1.to_excel('Godiva_true_umap_148.xlsx')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a772nP2uaUh7"},"outputs":[],"source":["df_comb1.isna().mean()*100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YoClhVdaJmg-"},"outputs":[],"source":["##456\n","outliers_mask = ~not_outliers_mask  # Get the outliers mask\n","outliers_df11 = df11.iloc[outliers_mask].copy(deep=True)  # Extract outliers\n","outliers_df11['label'] = -1\n","df_comb = pd.concat([filtered_df11, outliers_df11], axis=0)\n","df21 = df_comb.drop(['gpt_3_large_emb'],axis=1)\n","df21.to_excel('Godiva_done_32_pca.xlsx')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NXL0-_TUJmg-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6W7fqqORJmg-"},"outputs":[],"source":["df_original1=df_original.copy(deep=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qt_LdpioQEID"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":337,"status":"ok","timestamp":1714568626565,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"IvQ3BA6YQOcM","outputId":"0a82b457-400c-430f-a387-427cd23a3d33"},"outputs":[],"source":["import pandas as pd\n","\n","# Example DataFrames\n","data1 = {\n","    'A': [1, 2, 3, 4, 5],\n","    'B': ['a', 'b', 'c', 'd', 'e']\n","}\n","df1 = pd.DataFrame(data1)\n","\n","data2 = {\n","    'C': [10, 20, 30],\n","    'D': ['x', 'y', 'z']\n","}\n","df2 = pd.DataFrame(data2, index=[1, 3, 4])\n","df2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":299,"status":"ok","timestamp":1714568638291,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"gOOGTUcKQEbi","outputId":"64103ee3-149d-41f8-8b5f-5b5f4d577611"},"outputs":[],"source":["\n","# Filter df1 using the index of df2\n","filtered_df1 = df1.loc[df2.index]\n","\n","print(filtered_df1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fPa77djiJmg-"},"outputs":[],"source":["# Recoding 'fam_id' column\n","# recode_dict_fam_id = {\n","#     'unq_id_22': '0',\n","#     'unq_id_53': '1',\n","#     'unq_id_77': '2'\n","# }\n","# df_original1['fam_id'] = df_original1['fam_id'].replace(recode_dict_fam_id)\n","\n","# Recoding 'species' column\n","recode_dict_species = {\n","    'unq_id_174': '0',\n","    'unq_id_234': '1',\n","    'unq_id_150': '2'\n","}\n","\n","df_original1['fam_id'] = df_original1['fam_id'].replace(recode_dict_species)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LwSSRGBlJmg-"},"outputs":[],"source":["df_original1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-bRu4zhJmg-","scrolled":true},"outputs":[],"source":["import pandas as pd\n","\n","# Adding a new column 'comparison' based on the comparison of 'fam_id' and 'species'\n","df_original1['comparison'] = ['CORRECT' if str(fam_id) == str(color) else 'NOT CORRECT' for fam_id, color in zip(df_original1['fam_id'], df_original1['color'])]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZY4b3ZMJmg-"},"outputs":[],"source":["df_original1.comparison.value_counts()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZu_64jNJmg-"},"outputs":[],"source":["percentages = df_original1['comparison'].value_counts(normalize=True) * 100\n","percentages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QAPZ5o3nJmg-"},"outputs":[],"source":["\n","percentages = df_original00['comparison'].value_counts(normalize=True) * 100\n","percentages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Lc6yCktJmg-"},"outputs":[],"source":["# Plotting the data\n","\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(df_eval['iteration_list'], df_eval['sil_perf_list'] * 100, label='Silhouette Performance', marker='o')\n","plt.plot(df_eval['iteration_list'], df_eval['calinski_perf_list'], label='Calinski Performance', marker='x')\n","plt.plot(df_eval['iteration_list'], df_eval['unique_fam_list'], label='Unique Families', marker='s')\n","\n","# Adding titles and labels\n","plt.title('Performance Metrics over Iterations')\n","plt.xlabel('Iterations')\n","plt.ylabel('Metrics')\n","\n","# Display the legend\n","plt.legend()\n","\n","# Show the plot with a grid\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fISXNAiFJmg-"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"_enS0yiHJmg-"},"source":["## TEST V4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDd1TtaEJmg-"},"outputs":[],"source":["%%time\n","\n","# 2. MAIN NETWORK CLUSTERING\n","\n","## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","import pandas as pd\n","import networkx as nx\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import pairwise_distances\n","\n","# Assuming df is your DataFrame and it includes 'fam_id' along with the features.\n","\n","# Compute the pairwise distances using only the feature columns.\n","features = df_original.drop('fam_id', axis=1)\n","dist_matrix0 = pairwise_distances(features, metric='euclidean')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZ_YYlknJmg-"},"outputs":[],"source":["%%time\n","\n","# 2. MAIN NETWORK CLUSTERING\n","\n","dist_matrix = dist_matrix0.copy()\n","# Reset the index of the DataFrame to keep track of original indices\n","\n","\n","# OPTIMIZATION 1\n","#############\n","# WAS\n","# # Get a boolean matrix where True represents rows with the same 'fam_id'.\n","# same_fam_id = df_original['fam_id'].values[:, None] == df_original['fam_id'].values\n","\n","# # Set the distances for rows with the same 'fam_id' to infinity.\n","# # np.inf is used here to ensure these distances are never considered as the minimum.\n","# dist_matrix[same_fam_id] = np.inf\n","\n","\n","# NOW..SIGNIFICANTLY FASTER\n","df_with_index = df_original.reset_index()\n","\n","# Perform the merge to find all pairs with the same 'fam_id'\n","same_fam_id = df_with_index.merge(\n","    df_with_index[['index', 'fam_id']], on='fam_id', suffixes=('_left', '_right'))\n","\n","num_rows = len(df_original)\n","# Initialize a boolean matrix of size (num_rows, num_rows)\n","same_fam_id_matrix = np.zeros((num_rows, num_rows), dtype=bool)\n","\n","# Fill the boolean matrix with True where indices have the same 'fam_id'\n","same_fam_id_matrix[same_fam_id['index_left'].values, same_fam_id['index_right'].values] = True\n","\n","dist_matrix[same_fam_id_matrix]= np.inf\n","##########\n","\n","# Find the index of the closest row and the distance for each row, ignoring those with the same 'fam_id'.\n","closest_indices = dist_matrix.argmin(axis=1)\n","closest_distances = dist_matrix.min(axis=1)\n","\n","# Add the results back to the original DataFrame.\n","df_original['closest_row'] = closest_indices\n","df_original['closest_distance'] = closest_distances\n","\n","# Assuming df_original is your original DataFrame and it includes the 'fam_id' column.\n","\n","# 2B set up families to migrate to and solve conflicts if mulitple row in the same family want to migrate to the different families\n","# Add a column for the family that the closest row belongs to for easier reference\n","df_original['closest_family'] = df_original['closest_row'].apply(lambda x: df_original.at[x, 'fam_id'] if pd.notnull(x) else None)\n","\n","# Determine the closest family for each original family based on the smallest distance\n","closest_family_for_group = df_original.groupby('fam_id').apply(lambda x: x.loc[x['closest_distance'].idxmin(), 'closest_family'])\n","\n","# Map the target family for migration back to the original DataFrame\n","df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","# Choosing only the closest family on the migrate_to_family side\n","# Step 1: Determine the eligible family for each target family\n","# Group by 'migrate_to_family' and find the family with the closest distance for each target\n","eligible_families = df_original.groupby('migrate_to_family').apply(\n","    lambda x: x.loc[x['closest_distance'].idxmin(), 'fam_id']\n",").reset_index().rename(columns={0: 'eligible_fam_id'})\n","\n","# Step 2: Assign migration permission based on eligibility\n","# Merge the eligible families DataFrame back to the original DataFrame to assign migration permission\n","df_original = df_original.merge(\n","    eligible_families,\n","    how='left',\n","    left_on=['migrate_to_family', 'fam_id'],\n","    right_on=['migrate_to_family', 'eligible_fam_id']\n",")\n","\n","\n","# Step 1 & 2: Group by 'migrate_to_family' and find all families with the smallest distance for each target\n","# This involves creating a temporary DataFrame to calculate minimum distances for each migration target group\n","min_distances = df_original.groupby('migrate_to_family')['closest_distance'].transform('min')\n","\n","# Step 3: Assign migration permission based on matching the minimum distance\n","# Families that match the minimum distance to their target are allowed to migrate\n","df_original['can_migrate'] = df_original['closest_distance'] == min_distances\n","\n","# Step 1: Find all fam_id values where at least one row has can_migrate = True\n","fam_ids_to_update = df_original[df_original['can_migrate'] == True]['fam_id'].unique()\n","\n","# Step 2: Update can_migrate for rows where fam_id matches the identified fam_ids\n","df_original.loc[df_original['fam_id'].isin(fam_ids_to_update), 'can_migrate'] = True\n","\n","# Logic to update 'migrate_to_family' where 'can_migrate' is False\n","df_original.loc[df_original['can_migrate'] == False, 'migrate_to_family'] = df_original['fam_id']\n","\n","# Drop the 'eligible_fam_id' column as it's no longer needed\n","df_original.drop('eligible_fam_id', axis=1, inplace=True)\n","\n","# Initialize NetworkX UnionFind\n","\n","\n","\n","## OPTIMIZATION 4\n","##################################\n","\n","# WAS\n","# # Using DataFrame.apply() to perform Union operations on all rows at once\n","# uf = nx.utils.UnionFind()\n","# df_original.apply(lambda row: uf.union(row['fam_id'], row['migrate_to_family']), axis=1)\n","\n","# # Mapping the results back to the DataFrame can be optimized by\n","# # creating a mapping dictionary from the UnionFind structure and then using map()\n","# fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","\n","# df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping)\n","\n","# Drop unnecessary columns in a more concise way\n","# columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","# df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","\n","# NOW\n","# Iterate over DataFrame rows using itertuples() for better performance\n","uf = nx.utils.UnionFind()\n","for row in df_original.itertuples():\n","    uf.union(row.fam_id, row.migrate_to_family)\n","\n","# Creating a mapping dictionary from the UnionFind structure\n","fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","\n","# Ensure that all original IDs are covered in the mapping\n","df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping).fillna(df_original['fam_id'])\n","\n","# Drop unnecessary columns in a more concise way\n","columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","####################################\n","\n","# # Solution to all my issues.. recenter\n","def calculate_centroids(df):\n","        \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","        centroids = df.groupby('fam_id').mean()\n","        return centroids\n","\n","def reassign_families(df, centroids):\n","    \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","    # Calculate distances between each row and each centroid\n","    distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","    # Find the closest centroid for each row\n","    closest_centroids = np.argmin(distances, axis=1)\n","\n","    # Map centroid indices back to family labels\n","    index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","    df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","    return df\n","\n","# # Calculate initial centroids\n","centroids = calculate_centroids(df_original)\n","\n","# Reassign families based on closest centroid\n","df_reassigned = reassign_families(df_original, centroids)\n","df_original = df_reassigned.copy(deep=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","\n","# 2. MAIN NETWORK CLUSTERING\n","\n","## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","import pandas as pd\n","import networkx as nx\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import pairwise_distances\n","\n","# Assuming df is your DataFrame and it includes 'fam_id' along with the features.\n","\n","# Compute the pairwise distances using only the feature columns.\n","features = df_original.drop('fam_id', axis=1)\n","dist_matrix0 = pairwise_distances(features, metric='euclidean')\n","\n","dist_matrix = dist_matrix0.copy()\n","# Reset the index of the DataFrame to keep track of original indices\n","\n","# NOW..SIGNIFICANTLY FASTER\n","df_with_index = df_original.reset_index()\n","\n","# Perform the merge to find all pairs with the same 'fam_id'\n","same_fam_id = df_with_index.merge(\n","    df_with_index[['index', 'fam_id']], on='fam_id', suffixes=('_left', '_right'))\n","\n","num_rows = len(df_original)\n","# Initialize a boolean matrix of size (num_rows, num_rows)\n","same_fam_id_matrix = np.zeros((num_rows, num_rows), dtype=bool)\n","\n","# Fill the boolean matrix with True where indices have the same 'fam_id'\n","same_fam_id_matrix[same_fam_id['index_left'].values, same_fam_id['index_right'].values] = True\n","\n","dist_matrix[same_fam_id_matrix]= np.inf\n","##########\n","\n","# Find the index of the closest row and the distance for each row, ignoring those with the same 'fam_id'.\n","closest_indices = dist_matrix.argmin(axis=1)\n","closest_distances = dist_matrix.min(axis=1)\n","\n","# Add the results back to the original DataFrame.\n","df_original['closest_row'] = closest_indices\n","df_original['closest_distance'] = closest_distances\n","\n","# Assuming df_original is your original DataFrame and it includes the 'fam_id' column.\n","\n","# 2B set up families to migrate to and solve conflicts if mulitple row in the same family want to migrate to the different families\n","# Add a column for the family that the closest row belongs to for easier reference\n","df_original['closest_family'] = df_original['closest_row'].apply(lambda x: df_original.at[x, 'fam_id'] if pd.notnull(x) else None)\n","\n","# Determine the closest family for each original family based on the smallest distance\n","closest_family_for_group = df_original.groupby('fam_id').apply(lambda x: x.loc[x['closest_distance'].idxmin(), 'closest_family'])\n","\n","# Map the target family for migration back to the original DataFrame\n","df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","# Choosing only the closest family on the migrate_to_family side\n","# Step 1: Determine the eligible family for each target family\n","# Group by 'migrate_to_family' and find the family with the closest distance for each target\n","eligible_families = df_original.groupby('migrate_to_family').apply(\n","    lambda x: x.loc[x['closest_distance'].idxmin(), 'fam_id']\n",").reset_index().rename(columns={0: 'eligible_fam_id'})\n","\n","# Step 2: Assign migration permission based on eligibility\n","# Merge the eligible families DataFrame back to the original DataFrame to assign migration permission\n","df_original = df_original.merge(\n","    eligible_families,\n","    how='left',\n","    left_on=['migrate_to_family', 'fam_id'],\n","    right_on=['migrate_to_family', 'eligible_fam_id']\n",")\n","\n","\n","# Step 1 & 2: Group by 'migrate_to_family' and find all families with the smallest distance for each target\n","# This involves creating a temporary DataFrame to calculate minimum distances for each migration target group\n","min_distances = df_original.groupby('migrate_to_family')['closest_distance'].transform('min')\n","\n","# Step 3: Assign migration permission based on matching the minimum distance\n","# Families that match the minimum distance to their target are allowed to migrate\n","df_original['can_migrate'] = df_original['closest_distance'] == min_distances\n","\n","# Step 1: Find all fam_id values where at least one row has can_migrate = True\n","fam_ids_to_update = df_original[df_original['can_migrate'] == True]['fam_id'].unique()\n","\n","# Step 2: Update can_migrate for rows where fam_id matches the identified fam_ids\n","df_original.loc[df_original['fam_id'].isin(fam_ids_to_update), 'can_migrate'] = True\n","\n","# Logic to update 'migrate_to_family' where 'can_migrate' is False\n","df_original.loc[df_original['can_migrate'] == False, 'migrate_to_family'] = df_original['fam_id']\n","\n","# Drop the 'eligible_fam_id' column as it's no longer needed\n","df_original.drop('eligible_fam_id', axis=1, inplace=True)\n","\n","# Initialize NetworkX UnionFind\n","\n","\n","# NOW\n","# Iterate over DataFrame rows using itertuples() for better performance\n","uf = nx.utils.UnionFind()\n","for row in df_original.itertuples():\n","    uf.union(row.fam_id, row.migrate_to_family)\n","\n","# Creating a mapping dictionary from the UnionFind structure\n","fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","\n","# Ensure that all original IDs are covered in the mapping\n","df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping).fillna(df_original['fam_id'])\n","\n","# Drop unnecessary columns in a more concise way\n","columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","####################################\n","\n","# # Solution to all my issues.. recenter\n","def calculate_centroids(df):\n","        \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","        centroids = df.groupby('fam_id').mean()\n","        return centroids\n","\n","def reassign_families(df, centroids):\n","    \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","    # Calculate distances between each row and each centroid\n","    distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","    # Find the closest centroid for each row\n","    closest_centroids = np.argmin(distances, axis=1)\n","\n","    # Map centroid indices back to family labels\n","    index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","    df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","    return df\n","\n","# # Calculate initial centroids\n","centroids = calculate_centroids(df_original)\n","\n","# Reassign families based on closest centroid\n","df_reassigned = reassign_families(df_original, centroids)\n","df_original = df_reassigned.copy(deep=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YY8F5r2hJmg_"},"outputs":[],"source":["df_original.fam_id.nunique()"]},{"cell_type":"markdown","metadata":{"id":"e3yacqFoJmg_"},"source":["# TEST V3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9r0lrEgUJmg_"},"outputs":[],"source":["%%time\n","\n","# 2. MAIN NETWORK CLUSTERING\n","\n","## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","import pandas as pd\n","import networkx as nx\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import pairwise_distances\n","\n","# Assuming df is your DataFrame and it includes 'fam_id' along with the features.\n","\n","# Compute the pairwise distances using only the feature columns.\n","features = df_original.drop('fam_id', axis=1)\n","dist_matrix0 = pairwise_distances(features, metric='euclidean')\n","\n","dist_matrix = dist_matrix0.copy()\n","\n","# Get a boolean matrix where True represents rows with the same 'fam_id'.\n","same_fam_id = df_original['fam_id'].values[:, None] == df_original['fam_id'].values\n","\n","# Set the distances for rows with the same 'fam_id' to infinity.\n","# np.inf is used here to ensure these distances are never considered as the minimum.\n","dist_matrix[same_fam_id] = np.inf\n","\n","# Find the index of the closest row and the distance for each row, ignoring those with the same 'fam_id'.\n","closest_indices = dist_matrix.argmin(axis=1)\n","closest_distances = dist_matrix.min(axis=1)\n","\n","# Add the results back to the original DataFrame.\n","df_original['closest_row'] = closest_indices\n","df_original['closest_distance'] = closest_distances\n","\n","# Assuming df_original is your original DataFrame and it includes the 'fam_id' column.\n","\n","# 2B set up families to migrate to and solve conflicts if mulitple row in the same family want to migrate to the different families\n","# Add a column for the family that the closest row belongs to for easier reference\n","df_original['closest_family'] = df_original['closest_row'].apply(lambda x: df_original.at[x, 'fam_id'] if pd.notnull(x) else None)\n","\n","# Determine the closest family for each original family based on the smallest distance\n","closest_family_for_group = df_original.groupby('fam_id').apply(lambda x: x.loc[x['closest_distance'].idxmin(), 'closest_family'])\n","\n","# Map the target family for migration back to the original DataFrame\n","df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","# Choosing only the closest family on the migrate_to_family side\n","# Step 1: Determine the eligible family for each target family\n","# Group by 'migrate_to_family' and find the family with the closest distance for each target\n","eligible_families = df_original.groupby('migrate_to_family').apply(\n","    lambda x: x.loc[x['closest_distance'].idxmin(), 'fam_id']\n",").reset_index().rename(columns={0: 'eligible_fam_id'})\n","\n","# Step 2: Assign migration permission based on eligibility\n","# Merge the eligible families DataFrame back to the original DataFrame to assign migration permission\n","df_original = df_original.merge(\n","    eligible_families,\n","    how='left',\n","    left_on=['migrate_to_family', 'fam_id'],\n","    right_on=['migrate_to_family', 'eligible_fam_id']\n",")\n","\n","\n","# Step 1 & 2: Group by 'migrate_to_family' and find all families with the smallest distance for each target\n","# This involves creating a temporary DataFrame to calculate minimum distances for each migration target group\n","min_distances = df_original.groupby('migrate_to_family')['closest_distance'].transform('min')\n","\n","# Step 3: Assign migration permission based on matching the minimum distance\n","# Families that match the minimum distance to their target are allowed to migrate\n","df_original['can_migrate'] = df_original['closest_distance'] == min_distances\n","\n","# Step 1: Find all fam_id values where at least one row has can_migrate = True\n","fam_ids_to_update = df_original[df_original['can_migrate'] == True]['fam_id'].unique()\n","\n","# Step 2: Update can_migrate for rows where fam_id matches the identified fam_ids\n","df_original.loc[df_original['fam_id'].isin(fam_ids_to_update), 'can_migrate'] = True\n","\n","# Logic to update 'migrate_to_family' where 'can_migrate' is False\n","df_original.loc[df_original['can_migrate'] == False, 'migrate_to_family'] = df_original['fam_id']\n","\n","# Drop the 'eligible_fam_id' column as it's no longer needed\n","df_original.drop('eligible_fam_id', axis=1, inplace=True)\n","\n","\n","\n","# Initialize NetworkX UnionFind\n","# uf = nx.utils.UnionFind()\n","\n","# # Iterate over each row in the DataFrame to merge the original and new family IDs\n","# for _, row in df_original.iterrows():\n","#     uf.union(row['fam_id'], row['migrate_to_family'])\n","\n","# # Assign new family groups\n","# df_original['fam_id1'] = df_original['fam_id'].apply(lambda x: uf[x])\n","\n","\n","# #,connected_components\n","# df_original = df_original.drop(['closest_row', 'closest_distance', 'closest_family',\n","#        'migrate_to_family','fam_id','can_migrate'], axis=1).copy(deep=True)\n","\n","\n","# df_original.rename(columns={'fam_id1': 'fam_id'}, inplace=True)\n","\n","\n","# Initialize NetworkX UnionFind\n","uf = nx.utils.UnionFind()\n","\n","# Using DataFrame.apply() to perform Union operations on all rows at once\n","df_original.apply(lambda row: uf.union(row['fam_id'], row['migrate_to_family']), axis=1)\n","\n","# Mapping the results back to the DataFrame can be optimized by\n","# creating a mapping dictionary from the UnionFind structure and then using map()\n","fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping)\n","\n","# Drop unnecessary columns in a more concise way\n","columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","\n","\n","\n","\n","\n","# def reassign_families_efficient(df):\n","#     \"\"\"Calculate centroids of families and reassign rows to the closest family based on Euclidean distance.\"\"\"\n","#     # Calculate initial centroids\n","#     centroids = df.groupby('fam_id').mean()\n","\n","#     # Avoid dropping 'fam_id' every time by excluding it from the distance calculation\n","#     data_points = df.drop('fam_id', axis=1)\n","\n","#     # Calculate distances between each row and each centroid and find the closest centroid for each row\n","#     distances = pairwise_distances(data_points, centroids, metric='euclidean')\n","#     closest_centroids = np.argmin(distances, axis=1)\n","\n","#     # Map centroid indices back to family labels\n","#     index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","#     df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","#     return df\n","\n","# # Assuming df_original is your original DataFrame\n","# df_original = reassign_families_efficient(df_original)\n","\n","\n","# # Solution to all my issues.. recenter\n","def calculate_centroids(df):\n","        \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","        centroids = df.groupby('fam_id').mean()\n","        return centroids\n","\n","def reassign_families(df, centroids):\n","    \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","    # Calculate distances between each row and each centroid\n","    distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","    # Find the closest centroid for each row\n","    closest_centroids = np.argmin(distances, axis=1)\n","\n","    # Map centroid indices back to family labels\n","    index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","    df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","    return df\n","\n","# # Calculate initial centroids\n","centroids = calculate_centroids(df_original)\n","\n","# Reassign families based on closest centroid\n","df_reassigned = reassign_families(df_original, centroids)\n","df_original = df_reassigned.copy(deep=True)\n","\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","\n","\n","\n","\n","\n","# OUTLIER NEW11\n","#Compute the pairwise distances\n","features = df_original.drop('fam_id', axis=1)\n","dist_matrix = pairwise_distances(features, metric='euclidean')\n","\n","# Replace diagonal with np.inf to ignore self-distance\n","np.fill_diagonal(dist_matrix, np.inf)\n","\n","# Find the index of the closest row and the distance for each row\n","closest_indices = dist_matrix.argmin(axis=1)\n","closest_distances = dist_matrix.min(axis=1)\n","\n","# Add the results to the original DataFrame\n","df_original['Closest_Row'] = closest_indices\n","df_original['Distance'] = closest_distances\n","\n","# 1. Remove outliers after\n","# We will modify the function to also return the outliers\n","\n","def remove_upper_outliers_and_save(df, column_name):\n","    quartile_1, quartile_3 = np.percentile(df[column_name], [25, 75])\n","    iqr = quartile_3 - quartile_1\n","    upper_bound = quartile_3 + (iqr * 1.5) # 1.5\n","\n","    # Filter the DataFrame to remove data points above the upper bound\n","    filtered_df = df[df[column_name] <= upper_bound]\n","    # Save the outliers in a separate DataFrame\n","    outliers_df = df[df[column_name] > upper_bound]\n","\n","    return filtered_df, outliers_df\n","\n","# Apply the modified function to remove upper outliers from 'Distance' column and save the removed outliers\n","filtered_df, outliers_df = remove_upper_outliers_and_save(df_original, 'Distance')\n","\n","# Now we have two DataFrames: `filtered_df` without outliers and `outliers_df` with only the removed outliers\n","# (filtered_df, outliers_df)\n","\n","outliers_df0 = outliers_df.copy(deep=True)\n","del(outliers_df)\n","#filtered_df.reset_index(inplace=True, drop=True)\n","\n","df_original = filtered_df.copy(deep=True)\n","df_original.drop(['Closest_Row','Distance'], axis = 1, inplace=True)\n","outliers_df0.drop(['Closest_Row','Distance'], axis = 1, inplace=True)\n","outliers_df0['fam_id'] = -1\n","all_outliers = pd.concat([all_outliers, outliers_df0], ignore_index=True)\n","del(outliers_df0)\n","df_original.reset_index(drop=True, inplace=True)\n","\n","\n","# OUTLIER REMOVAL\n","\n","# import numpy as np\n","# import pandas as pd\n","# from scipy.spatial.distance import cdist\n","\n","# ##Assuming df_original is your DataFrame and it already contains 'fam_id'\n","# ##Select numeric columns for centroid calculation, excluding 'fam_id' or any other non-relevant columns\n","# features = [col for col in df_original.select_dtypes(include=np.number).columns if col != 'fam_id']\n","\n","# # Calculate the centroid for each family using a groupby operation, this remains efficient\n","# centroids = df_original.groupby('fam_id')[features].transform('mean')\n","\n","# # Calculate pairwise distances to centroid for each member without using apply()\n","# # Using scipy's cdist function for efficient distance calculation\n","# distances_to_centroid = cdist(df_original[features], centroids, metric='euclidean').diagonal()\n","# df_original['distance_to_centroid'] = distances_to_centroid\n","\n","# # Identify outliers using the IQR method for each family (only above upper bound), vectorized for efficiency\n","# Q1 = df_original.groupby('fam_id')['distance_to_centroid'].transform(lambda x: x.quantile(0.25))\n","# Q3 = df_original.groupby('fam_id')['distance_to_centroid'].transform(lambda x: x.quantile(0.75))\n","# IQR = Q3 - Q1\n","# upper_bound = Q3 + 1.5 * IQR\n","\n","# # Assigning -1 to fam_id for outliers, more directly\n","# df_original.loc[df_original['distance_to_centroid'] > upper_bound, 'fam_id'] = -1\n","\n","# # Splitting outliers and non-outliers into separate DataFrames as needed\n","# outliers_df = df_original[df_original['fam_id'] == -1].copy()\n","# df_original = df_original[df_original['fam_id'] != -1].copy()\n","\n","# # Optionally, drop the 'distance_to_centroid' column if no longer needed\n","# df_original.drop('distance_to_centroid', axis=1, inplace=True)\n","# df_original.reset_index(inplace=True, drop=True)\n","\n","\n","# OUTLIER REMOVAL\n","\n","# import numpy as np\n","# import pandas as pd\n","# from scipy.spatial import distance\n","\n","# # Assuming df_original0 is your original dataframe\n","# # Selecting numeric columns for centroid calculation, excluding 'fam_id' or any other non-relevant columns\n","# features = df_original.select_dtypes(include=np.number).columns.tolist()\n","# if 'fam_id' in features:\n","#     features.remove('fam_id')\n","\n","# # Calculate the centroid for each family\n","# centroids = df_original.groupby('fam_id')[features].mean().reset_index()\n","\n","# # Calculate pairwise distances to centroid for each member\n","# def calculate_distance(row):\n","#     centroid = centroids[centroids['fam_id'] == row['fam_id']][features].iloc[0]\n","#     return distance.euclidean(row[features], centroid)\n","\n","# df_original['distance_to_centroid'] = df_original.apply(calculate_distance, axis=1)\n","\n","# # Identify outliers using the IQR method for each family (only above upper bound)\n","# def is_outlier(group):\n","#     q3 = group['distance_to_centroid'].quantile(0.75)\n","#     iqr = q3 - group['distance_to_centroid'].quantile(0.25)\n","#     upper_bound = q3 + 1.5 * iqr # 1.5\n","#     group['outlier'] = (group['distance_to_centroid'] > upper_bound)\n","#     return group\n","\n","\n","# df_original['original_index'] = df_original.index\n","# df_original = df_original.groupby('fam_id', group_keys=True).apply(is_outlier)\n","# df_original.reset_index(drop=True, inplace=True)\n","# # df_original.set_index('original_index', inplace=True)\n","# df_original.index = df_original['original_index'].values\n","# df_original.drop('original_index', axis=1, inplace=True)\n","# # Assign -1 to outliers\n","# df_original.loc[df_original['outlier'], 'fam_id'] = -1\n","\n","# # Optionally, drop the helper columns\n","# df_original.drop(['distance_to_centroid', 'outlier'], axis=1, inplace=True)\n","\n","# outliers_df1 = df_original[df_original.fam_id == -1]\n","\n","# all_outliers = pd.concat([all_outliers, outliers_df1], ignore_index=True)\n","# del(outliers_df1)\n","# df_original = df_original[df_original.fam_id != -1].copy(deep=True)\n","# df_original.reset_index(inplace=True, drop=True)\n","\n","\n","# # #New assignment method\n","# from sklearn.metrics import pairwise_distances\n","# df=df_original.drop('fam_id', axis =1).copy()\n","# # Compute the pairwise distances\n","# dist_matrix = pairwise_distances(df, metric='euclidean')\n","\n","# # Replace diagonal with np.inf to ignore self-distance\n","# np.fill_diagonal(dist_matrix, np.inf)\n","\n","# # Find the index of the closest row and the distance for each row\n","# closest_indices = dist_matrix.argmin(axis=1)\n","# closest_distances = dist_matrix.min(axis=1)\n","\n","# # Add the results to the original DataFrame\n","# df_original['Closest_Row'] = closest_indices\n","# df_original['Distance'] = closest_distances\n","\n","# # Method 2: Direct indexing (more efficient)\n","# df_original['new_fam_id'] = df_original['fam_id'].iloc[df_original['Closest_Row']].values\n","\n","# df_original.drop([\"fam_id\",\"Closest_Row\", \"Distance\"], axis=1, inplace=True)\n","# df_original.rename(columns={'new_fam_id': 'fam_id'}, inplace=True)\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guvKaQXnJmg_"},"outputs":[],"source":["df_original.fam_id.nunique()"]},{"cell_type":"markdown","metadata":{"id":"WUNJNpflJmg_"},"source":["# TEST V2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8F1HaCzJmg_"},"outputs":[],"source":["%%time\n","\n","# 2. MAIN NETWORK CLUSTERING\n","\n","## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","import pandas as pd\n","import networkx as nx\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import pairwise_distances\n","\n","\n","# Assuming df_original is your original DataFrame and it includes the 'fam_id' column.\n","\n","# Step 1: Calculate the Euclidean Distance Matrix\n","features = df_original.drop('fam_id', axis=1)\n","dist_matrix = pd.DataFrame(pairwise_distances(features, metric='euclidean'),\n","                           columns=df_original.index,\n","                           index=df_original.index)\n","\n","# Step 2: Mask distances within the same family by setting them to np.inf\n","for i in df_original.index:\n","    same_family_indices = df_original[df_original['fam_id'] == df_original.at[i, 'fam_id']].index\n","    dist_matrix.loc[i, same_family_indices] = np.inf\n","\n","# Step 3: Find the minimum distance and the corresponding index for each row\n","min_distances = dist_matrix.min(axis=1)\n","min_distance_indices = dist_matrix.idxmin(axis=1)\n","\n","df_original['closest_row'] = min_distance_indices\n","df_original['closest_distance'] = min_distances\n","\n","\n","# 2B set up families to migrate to and solve conflicts if mulitple row in the same family want to migrate to the different families\n","# Add a column for the family that the closest row belongs to for easier reference\n","df_original['closest_family'] = df_original['closest_row'].apply(lambda x: df_original.at[x, 'fam_id'] if pd.notnull(x) else None)\n","\n","# Determine the closest family for each original family based on the smallest distance\n","closest_family_for_group = df_original.groupby('fam_id').apply(lambda x: x.loc[x['closest_distance'].idxmin(), 'closest_family'])\n","\n","# Map the target family for migration back to the original DataFrame\n","df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","\n","# Choosing only the closest family on the migrate_to_family side\n","# Step 1: Determine the eligible family for each target family\n","# Group by 'migrate_to_family' and find the family with the closest distance for each target\n","eligible_families = df_original.groupby('migrate_to_family').apply(\n","    lambda x: x.loc[x['closest_distance'].idxmin(), 'fam_id']\n",").reset_index().rename(columns={0: 'eligible_fam_id'})\n","\n","# Step 2: Assign migration permission based on eligibility\n","# Merge the eligible families DataFrame back to the original DataFrame to assign migration permission\n","df_original = df_original.merge(\n","    eligible_families,\n","    how='left',\n","    left_on=['migrate_to_family', 'fam_id'],\n","    right_on=['migrate_to_family', 'eligible_fam_id']\n",")\n","\n","\n","# Step 1 & 2: Group by 'migrate_to_family' and find all families with the smallest distance for each target\n","# This involves creating a temporary DataFrame to calculate minimum distances for each migration target group\n","min_distances = df_original.groupby('migrate_to_family')['closest_distance'].transform('min')\n","\n","# Step 3: Assign migration permission based on matching the minimum distance\n","# Families that match the minimum distance to their target are allowed to migrate\n","df_original['can_migrate'] = df_original['closest_distance'] == min_distances\n","\n","# Step 1: Find all fam_id values where at least one row has can_migrate = True\n","fam_ids_to_update = df_original[df_original['can_migrate'] == True]['fam_id'].unique()\n","\n","# Step 2: Update can_migrate for rows where fam_id matches the identified fam_ids\n","df_original.loc[df_original['fam_id'].isin(fam_ids_to_update), 'can_migrate'] = True\n","\n","# Logic to update 'migrate_to_family' where 'can_migrate' is False\n","df_original.loc[df_original['can_migrate'] == False, 'migrate_to_family'] = df_original['fam_id']\n","\n","# Drop the 'eligible_fam_id' column as it's no longer needed\n","df_original.drop('eligible_fam_id', axis=1, inplace=True)\n","\n","\n","# Initialize NetworkX UnionFind\n","uf = nx.utils.UnionFind()\n","\n","# Iterate over each row in the DataFrame to merge the original and new family IDs\n","for _, row in df_original.iterrows():\n","    uf.union(row['fam_id'], row['migrate_to_family'])\n","\n","# Assign new family groups\n","df_original['fam_id1'] = df_original['fam_id'].apply(lambda x: uf[x])\n","\n","\n","#,connected_components\n","df_original = df_original.drop(['closest_row', 'closest_distance', 'closest_family',\n","       'migrate_to_family','fam_id','can_migrate'], axis=1).copy(deep=True)\n","\n","\n","df_original.rename(columns={'fam_id1': 'fam_id'}, inplace=True)\n","\n","# def reassign_families_efficient(df):\n","#     \"\"\"Calculate centroids of families and reassign rows to the closest family based on Euclidean distance.\"\"\"\n","#     # Calculate initial centroids\n","#     centroids = df.groupby('fam_id').mean()\n","\n","#     # Avoid dropping 'fam_id' every time by excluding it from the distance calculation\n","#     data_points = df.drop('fam_id', axis=1)\n","\n","#     # Calculate distances between each row and each centroid and find the closest centroid for each row\n","#     distances = pairwise_distances(data_points, centroids, metric='euclidean')\n","#     closest_centroids = np.argmin(distances, axis=1)\n","\n","#     # Map centroid indices back to family labels\n","#     index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","#     df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","#     return df\n","\n","# # Assuming df_original is your original DataFrame\n","# df_original = reassign_families_efficient(df_original)\n","\n","\n","# # Solution to all my issues.. recenter\n","# Impact.. low\n","def calculate_centroids(df):\n","        \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","        centroids = df.groupby('fam_id').mean()\n","        return centroids\n","\n","def reassign_families(df, centroids):\n","    \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","    # Calculate distances between each row and each centroid\n","    distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","    # Find the closest centroid for each row\n","    closest_centroids = np.argmin(distances, axis=1)\n","\n","    # Map centroid indices back to family labels\n","    index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","    df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","    return df\n","\n","# # Calculate initial centroids\n","centroids = calculate_centroids(df_original)\n","\n","# Reassign families based on closest centroid\n","df_reassigned = reassign_families(df_original, centroids)\n","df_original = df_reassigned.copy(deep=True)\n","\n","## OUTLIER REMOVAL NEW\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.spatial.distance import cdist\n","\n","##Assuming df_original is your DataFrame and it already contains 'fam_id'\n","##Select numeric columns for centroid calculation, excluding 'fam_id' or any other non-relevant columns\n","features = [col for col in df_original.select_dtypes(include=np.number).columns if col != 'fam_id']\n","\n","# Calculate the centroid for each family using a groupby operation, this remains efficient\n","centroids = df_original.groupby('fam_id')[features].transform('mean')\n","\n","# Calculate pairwise distances to centroid for each member without using apply()\n","# Using scipy's cdist function for efficient distance calculation\n","distances_to_centroid = cdist(df_original[features], centroids, metric='euclidean').diagonal()\n","df_original['distance_to_centroid'] = distances_to_centroid\n","\n","# Identify outliers using the IQR method for each family (only above upper bound), vectorized for efficiency\n","Q1 = df_original.groupby('fam_id')['distance_to_centroid'].transform(lambda x: x.quantile(0.25))\n","Q3 = df_original.groupby('fam_id')['distance_to_centroid'].transform(lambda x: x.quantile(0.75))\n","IQR = Q3 - Q1\n","upper_bound = Q3 + 1.5 * IQR\n","\n","# Assigning -1 to fam_id for outliers, more directly\n","df_original.loc[df_original['distance_to_centroid'] > upper_bound, 'fam_id'] = -1\n","\n","# Splitting outliers and non-outliers into separate DataFrames as needed\n","outliers_df = df_original[df_original['fam_id'] == -1].copy()\n","df_original = df_original[df_original['fam_id'] != -1].copy()\n","\n","# Optionally, drop the 'distance_to_centroid' column if no longer needed\n","df_original.drop('distance_to_centroid', axis=1, inplace=True)\n","\n","\n","\n","# OUTLIER REMOVAL\n","\n","# import numpy as np\n","# import pandas as pd\n","# from scipy.spatial import distance\n","\n","# # Assuming df_original0 is your original dataframe\n","# # Selecting numeric columns for centroid calculation, excluding 'fam_id' or any other non-relevant columns\n","# features = df_original.select_dtypes(include=np.number).columns.tolist()\n","# if 'fam_id' in features:\n","#     features.remove('fam_id')\n","\n","# # Calculate the centroid for each family\n","# centroids = df_original.groupby('fam_id')[features].mean().reset_index()\n","\n","# # Calculate pairwise distances to centroid for each member\n","# def calculate_distance(row):\n","#     centroid = centroids[centroids['fam_id'] == row['fam_id']][features].iloc[0]\n","#     return distance.euclidean(row[features], centroid)\n","\n","# df_original['distance_to_centroid'] = df_original.apply(calculate_distance, axis=1)\n","\n","# # Identify outliers using the IQR method for each family (only above upper bound)\n","# def is_outlier(group):\n","#     q3 = group['distance_to_centroid'].quantile(0.75)\n","#     iqr = q3 - group['distance_to_centroid'].quantile(0.25)\n","#     upper_bound = q3 + 1.5 * iqr # 1.5\n","#     group['outlier'] = (group['distance_to_centroid'] > upper_bound)\n","#     return group\n","\n","\n","# df_original['original_index'] = df_original.index\n","# df_original = df_original.groupby('fam_id', group_keys=True).apply(is_outlier)\n","# df_original.reset_index(drop=True, inplace=True)\n","# # df_original.set_index('original_index', inplace=True)\n","# df_original.index = df_original['original_index'].values\n","# df_original.drop('original_index', axis=1, inplace=True)\n","# # Assign -1 to outliers\n","# df_original.loc[df_original['outlier'], 'fam_id'] = -1\n","\n","# # Optionally, drop the helper columns\n","# df_original.drop(['distance_to_centroid', 'outlier'], axis=1, inplace=True)\n","\n","# outliers_df1 = df_original[df_original.fam_id == -1]\n","\n","# all_outliers = pd.concat([all_outliers, outliers_df1], ignore_index=True)\n","# del(outliers_df1)\n","# df_original = df_original[df_original.fam_id != -1].copy(deep=True)\n","# df_original.reset_index(inplace=True, drop=True)\n","\n","\n","# # #New assignment method\n","# from sklearn.metrics import pairwise_distances\n","# df=df_original.drop('fam_id', axis =1).copy()\n","# # Compute the pairwise distances\n","# dist_matrix = pairwise_distances(df, metric='euclidean')\n","\n","# # Replace diagonal with np.inf to ignore self-distance\n","# np.fill_diagonal(dist_matrix, np.inf)\n","\n","# # Find the index of the closest row and the distance for each row\n","# closest_indices = dist_matrix.argmin(axis=1)\n","# closest_distances = dist_matrix.min(axis=1)\n","\n","# # Add the results to the original DataFrame\n","# df_original['Closest_Row'] = closest_indices\n","# df_original['Distance'] = closest_distances\n","\n","# # Method 2: Direct indexing (more efficient)\n","# df_original['new_fam_id'] = df_original['fam_id'].iloc[df_original['Closest_Row']].values\n","\n","# df_original.drop([\"fam_id\",\"Closest_Row\", \"Distance\"], axis=1, inplace=True)\n","# df_original.rename(columns={'new_fam_id': 'fam_id'}, inplace=True)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C3dZHeANJmhA"},"outputs":[],"source":["df_original.fam_id.nunique()"]},{"cell_type":"markdown","metadata":{"id":"mS7mw31NJmhA"},"source":["# TEST V1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YW7rjjLrJmhA"},"outputs":[],"source":["%%time\n","# 2. MAIN NETWORK CLUSTERING\n","\n","## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import pairwise_distances\n","\n","### Outlier removal test\n","\n","# Calculate the Euclidean distance matrix using pairwise_distances\n","features = df_original.drop('fam_id', axis=1).copy(deep=True)\n","dist_matrix = pd.DataFrame(pairwise_distances(features, metric='euclidean'), columns=df_original.index, index=df_original.index)\n","\n","# Initialize a column to store the closest row from a different family\n","df_original['closest_row'] = None\n","df_original['closest_distance'] = np.inf\n","\n","# Iterate over each row to find the closest row from a different family\n","for i, row_i in df_original.iterrows():\n","    for j, row_j in df_original.iterrows():\n","        if row_i['fam_id'] != row_j['fam_id']:  # Ensure different families\n","            distance = dist_matrix.at[i, j]\n","            if distance < df_original.at[i, 'closest_distance']:\n","                df_original.at[i, 'closest_distance'] = distance\n","                df_original.at[i, 'closest_row'] = j\n","\n","# 2B set up families to migrate to and solve conflicts if mulitple row in the same family want to migrate to the different families\n","# Add a column for the family that the closest row belongs to for easier reference\n","df_original['closest_family'] = df_original['closest_row'].apply(lambda x: df_original.at[x, 'fam_id'] if pd.notnull(x) else None)\n","\n","# Determine the closest family for each original family based on the smallest distance\n","closest_family_for_group = df_original.groupby('fam_id').apply(lambda x: x.loc[x['closest_distance'].idxmin(), 'closest_family'])\n","\n","# Map the target family for migration back to the original DataFrame\n","df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","import pandas as pd\n","import networkx as nx\n","#df_original.reset_index(drop=True, inplace=True)\n","# Example DataFrame\n","df = df_original.copy(deep=True)\n","G = nx.Graph()\n","\n","# Add edges based on the 'original_family' and 'families_to_join_with' relationships\n","for _, row in df.iterrows():\n","    G.add_edge(row['fam_id'], row['migrate_to_family'])\n","\n","# Find the connected components (i.e., the groups of families to join)\n","connected_components = list(nx.connected_components(G))\n","\n","# Create a new column in the DataFrame to assign each row to its connected component (family group)\n","family_group_mapping = {}\n","for i, component in enumerate(connected_components):\n","    for family_id in component:\n","        family_group_mapping[family_id] = i + 1  # Group numbering starts from 1\n","\n","df['fam_id'] = df['migrate_to_family'].map(family_group_mapping)\n","\n","#,connected_components\n","df_original = df.drop(['closest_row', 'closest_distance', 'closest_family',\n","       'migrate_to_family'], axis=1).copy(deep=True)\n","\n","\n","\n","\n","# # Solution to all my issues.. recenter\n","def calculate_centroids(df):\n","        \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","        centroids = df.groupby('fam_id').mean()\n","        return centroids\n","\n","def reassign_families(df, centroids):\n","    \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","    # Calculate distances between each row and each centroid\n","    distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","    # Find the closest centroid for each row\n","    closest_centroids = np.argmin(distances, axis=1)\n","\n","    # Map centroid indices back to family labels\n","    index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","    df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","    return df\n","\n","# # Calculate initial centroids\n","centroids = calculate_centroids(df_original)\n","\n","# Reassign families based on closest centroid\n","df_reassigned = reassign_families(df_original, centroids)\n","df_original = df_reassigned.copy(deep=True)\n","\n","\n","# OUTLIER REMOVAL\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.spatial import distance\n","\n","# Assuming df_original0 is your original dataframe\n","# Selecting numeric columns for centroid calculation, excluding 'fam_id' or any other non-relevant columns\n","features = df_original.select_dtypes(include=np.number).columns.tolist()\n","if 'fam_id' in features:\n","    features.remove('fam_id')\n","\n","# Calculate the centroid for each family\n","centroids = df_original.groupby('fam_id')[features].mean().reset_index()\n","\n","# Calculate pairwise distances to centroid for each member\n","def calculate_distance(row):\n","    centroid = centroids[centroids['fam_id'] == row['fam_id']][features].iloc[0]\n","    return distance.euclidean(row[features], centroid)\n","\n","df_original['distance_to_centroid'] = df_original.apply(calculate_distance, axis=1)\n","\n","# Identify outliers using the IQR method for each family (only above upper bound)\n","def is_outlier(group):\n","    q3 = group['distance_to_centroid'].quantile(0.75)\n","    iqr = q3 - group['distance_to_centroid'].quantile(0.25)\n","    upper_bound = q3 + 1.5 * iqr # 1.5\n","    group['outlier'] = (group['distance_to_centroid'] > upper_bound)\n","    return group\n","\n","df_original = df_original.groupby('fam_id', group_keys=True).apply(is_outlier)\n","\n","# Assign -1 to outliers\n","df_original.loc[df_original['outlier'], 'fam_id'] = -1\n","\n","# Optionally, drop the helper columns\n","df_original.drop(['distance_to_centroid', 'outlier'], axis=1, inplace=True)\n","\n","df_original = df_original[df_original.fam_id != -1].copy(deep=True)\n","df_original.reset_index(inplace=True, drop=True)\n","\n","\n","\n","\n","# # #New assignment method\n","# from sklearn.metrics import pairwise_distances\n","# df=df_original.drop('fam_id', axis =1).copy()\n","# # Compute the pairwise distances\n","# dist_matrix = pairwise_distances(df, metric='euclidean')\n","\n","# # Replace diagonal with np.inf to ignore self-distance\n","# np.fill_diagonal(dist_matrix, np.inf)\n","\n","# # Find the index of the closest row and the distance for each row\n","# closest_indices = dist_matrix.argmin(axis=1)\n","# closest_distances = dist_matrix.min(axis=1)\n","\n","# # Add the results to the original DataFrame\n","# df_original['Closest_Row'] = closest_indices\n","# df_original['Distance'] = closest_distances\n","\n","# # Method 2: Direct indexing (more efficient)\n","# df_original['new_fam_id'] = df_original['fam_id'].iloc[df_original['Closest_Row']].values\n","\n","# df_original.drop([\"fam_id\",\"Closest_Row\", \"Distance\"], axis=1, inplace=True)\n","# df_original.rename(columns={'new_fam_id': 'fam_id'}, inplace=True)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5PN9JGuJmhA"},"outputs":[],"source":["df_original.fam_id.nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sV_RIL1JJmhA"},"outputs":[],"source":["df_original0 = df_original.copy(deep=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tcukOJkfJmhA"},"outputs":[],"source":["outliers_df.drop([\"Closest_Row\",\"Distance\"], axis=1, inplace=True)\n","outliers_df['fam_id'] = -1\n","\n","# Merge (concatenate) DataFrames row-wise\n","df_merged = pd.concat([df_original, outliers_df], ignore_index=True)\n","\n","df_merged"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pjDNw-lYJmhA"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"p-8NEtmkJmhA"},"source":["# TEST 2 MEAN NUC\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6sCCCyCgJmhA"},"outputs":[],"source":["%%time\n","# #New assignment method\n","from sklearn.metrics import pairwise_distances\n","import networkx as nx\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.spatial import distance\n","\n","mean_each_group = df_original.groupby('fam_id').mean(numeric_only=True).reset_index()\n","# Rename fam_id to track iteration number, and prepare dataframe for next iteration\n","\n","mean_each_group.rename(columns={'fam_id': 'fam_id_0'}, inplace=True)\n","\n","# fam_id_df = mean_each_group['fam_id_0'].copy()\n","\n","features = mean_each_group.drop('fam_id_0', axis=1).copy(deep=True)\n","\n","# df_original.drop('fam_id', axis = 1).copy()\n","\n","# Compute the pairwise distances\n","dist_matrix = pairwise_distances(features, metric='euclidean')\n","\n","# Replace diagonal with np.inf to ignore self-distance\n","np.fill_diagonal(dist_matrix, np.inf)\n","\n","# Find the index of the closest row and the distance for each row\n","closest_indices = dist_matrix.argmin(axis=1)\n","closest_distances = dist_matrix.min(axis=1)\n","\n","# Add the results to the original DataFrame\n","mean_each_group['closest_row'] = closest_indices\n","mean_each_group['distance'] = closest_distances\n","mean_each_group['closest_family'] = mean_each_group['closest_row'].apply(lambda x: mean_each_group.at[x, 'fam_id_0'] if pd.notnull(x) else None)\n","\n","# This involves creating a temporary DataFrame to calculate minimum distances for each migration target group\n","min_distances = mean_each_group.groupby('closest_family')['distance'].transform('min')\n","\n","# Step 0: Assign migration permission based on matching the minimum distance\n","# Families that match the minimum distance to their target are allowed to migrate\n","mean_each_group['can_migrate'] = mean_each_group['distance'] == min_distances\n","\n","# Step 1: Find all fam_id values where at least one row has can_migrate = True\n","fam_ids_to_update = mean_each_group[mean_each_group['can_migrate'] == True]['fam_id_0'].unique()\n","\n","# Step 2: Update can_migrate for rows where fam_id matches the identified fam_ids\n","mean_each_group.loc[mean_each_group['fam_id_0'].isin(fam_ids_to_update), 'can_migrate'] = True\n","\n","# Step 3:\n","# Logic to update 'migrate_to_family' where 'can_migrate' is False\n","mean_each_group.loc[mean_each_group['can_migrate'] == False, 'migrate_to_family'] = mean_each_group['fam_id_0']\n","\n","# Logic to update 'migrate_to_family' where 'can_migrate' is False\n","mean_each_group.loc[mean_each_group['can_migrate'] == True, 'migrate_to_family'] = mean_each_group['closest_family']\n","\n","# Initialize NetworkX UnionFind\n","uf = nx.utils.UnionFind()\n","\n","# Iterate over each row in the DataFrame to merge the original and new family IDs\n","for _, row in mean_each_group.iterrows():\n","    uf.union(row['fam_id_0'], row['migrate_to_family'])\n","\n","# Assign new family groups\n","mean_each_group['fam_id1'] = mean_each_group['fam_id_0'].apply(lambda x: uf[x])\n","\n","mean_each_group = mean_each_group.drop(['closest_row', 'distance', 'closest_family',\n","       'migrate_to_family','can_migrate'], axis=1).copy(deep=True)\n","\n","mean_each_group.rename(columns={'fam_id1': 'fam_id'}, inplace=True)\n","\n","\n","df_merged = pd.merge(df_original.reset_index(), mean_each_group[['fam_id_0','fam_id']],\n","                    left_on='fam_id', right_on='fam_id_0')\n","\n","\n","# Set the original index back\n","df_merged.index = df_original.index\n","df_merged = df_merged.drop(['index'],axis=1)\n","# print(df_merged)\n","#df_merged = df_merged.set_index('index')\n","\n","\n","# Drop unnecessary columns\n","df_final = df_merged.drop(columns=['fam_id_x','fam_id_0'], axis=1)\n","\n","# Assuming df_merged is your DataFrame after merging\n","columns_to_rename = {'fam_id_y': 'fam_id'}\n","\n","df_final.rename(columns=columns_to_rename, inplace=True)\n","df_original = df_final.copy(deep=True)\n","del(df_final)\n","\n","\n","# Solution to all my issues.. recenter\n","def calculate_centroids(df):\n","        \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","        centroids = df.groupby('fam_id').mean()\n","        return centroids\n","\n","def reassign_families(df, centroids):\n","    \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","    # Calculate distances between each row and each centroid\n","    distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","    # Find the closest centroid for each row\n","    closest_centroids = np.argmin(distances, axis=1)\n","\n","    # Map centroid indices back to family labels\n","    index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","    df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","    return df\n","\n","# # Calculate initial centroids\n","centroids = calculate_centroids(df_original)\n","\n","# Reassign families based on closest centroid\n","df_reassigned = reassign_families(df_original, centroids)\n","df_original = df_reassigned.copy(deep=True)\n","\n","# OUTLIER REMOVAL\n","\n","# Assuming df_original is your DataFrame and it already contains 'fam_id'\n","# Select numeric columns for centroid calculation, excluding 'fam_id' or any other non-relevant columns\n","features = [col for col in df_original.select_dtypes(include=np.number).columns if col != 'fam_id']\n","\n","# Calculate the centroid for each family using a groupby operation, this remains efficient\n","centroids = df_original.groupby('fam_id')[features].transform('mean')\n","\n","# Calculate pairwise distances to centroid for each member without using apply()\n","# Using scipy's cdist function for efficient distance calculation\n","distances_to_centroid = cdist(df_original[features], centroids, metric='euclidean').diagonal()\n","df_original['distance_to_centroid'] = distances_to_centroid\n","\n","# Identify outliers using the IQR method for each family (only above upper bound), vectorized for efficiency\n","Q1 = df_original.groupby('fam_id')['distance_to_centroid'].transform(lambda x: x.quantile(0.25))\n","Q3 = df_original.groupby('fam_id')['distance_to_centroid'].transform(lambda x: x.quantile(0.75))\n","IQR = Q3 - Q1\n","upper_bound = Q3 + 1.5 * IQR\n","\n","# Assigning -1 to fam_id for outliers, more directly\n","df_original.loc[df_original['distance_to_centroid'] > upper_bound, 'fam_id'] = -1\n","\n","# Splitting outliers and non-outliers into separate DataFrames as needed\n","outliers_df = df_original[df_original['fam_id'] == -1].copy()\n","df_original = df_original[df_original['fam_id'] != -1].copy()\n","\n","# Optionally, drop the 'distance_to_centroid' column if no longer needed\n","df_original.drop('distance_to_centroid', axis=1, inplace=True)\n","\n","\n","\n","# OUTLIER REMOVAL\n","# Assuming df_original0 is your original dataframe\n","# Selecting numeric columns for centroid calculation, excluding 'fam_id' or any other non-relevant columns\n","# features = df_original.select_dtypes(include=np.number).columns.tolist()\n","# if 'fam_id' in features:\n","#     features.remove('fam_id')\n","\n","# # Calculate the centroid for each family\n","# centroids = df_original.groupby('fam_id')[features].mean().reset_index()\n","\n","# # Calculate pairwise distances to centroid for each member\n","# def calculate_distance(row):\n","#     centroid = centroids[centroids['fam_id'] == row['fam_id']][features].iloc[0]\n","#     return distance.euclidean(row[features], centroid)\n","\n","# df_original['distance_to_centroid'] = df_original.apply(calculate_distance, axis=1)\n","\n","# # Identify outliers using the IQR method for each family (only above upper bound)\n","# def is_outlier(group):\n","#     q3 = group['distance_to_centroid'].quantile(0.75)\n","#     iqr = q3 - group['distance_to_centroid'].quantile(0.25)\n","#     upper_bound = q3 + 1.5 * iqr # 1.5\n","#     group['outlier'] = (group['distance_to_centroid'] > upper_bound)\n","#     return group\n","\n","\n","# df_original['original_index'] = df_original.index\n","# df_original = df_original.groupby('fam_id', group_keys=True).apply(is_outlier)\n","# df_original.reset_index(drop=True, inplace=True)\n","# # df_original.set_index('original_index', inplace=True)\n","# df_original.index = df_original['original_index'].values\n","# df_original.drop('original_index', axis=1, inplace=True)\n","# # Assign -1 to outliers\n","# df_original.loc[df_original['outlier'], 'fam_id'] = -1\n","\n","# # Optionally, drop the helper columns\n","# df_original.drop(['distance_to_centroid', 'outlier'], axis=1, inplace=True)\n","\n","# outliers_df1 = df_original[df_original.fam_id == -1]\n","\n","# all_outliers = pd.concat([all_outliers, outliers_df1], ignore_index=True)\n","# del(outliers_df1)\n","# df_original = df_original[df_original.fam_id != -1].copy(deep=True)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HYamy6W3JmhA"},"outputs":[],"source":["df_original.fam_id.nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSUR_3OkJmhA"},"outputs":[],"source":["# UNION FIND"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WMuVw4S5JmhA"},"outputs":[],"source":["\n","\n","# import pandas as pd\n","# import networkx as nx\n","\n","# df=df_original.drop('fam_id', axis =1).copy(deep=True)\n","# # Compute the pairwise distances\n","# dist_matrix = pairwise_distances(df, metric='euclidean')\n","\n","# # Replace diagonal with np.inf to ignore self-distance\n","# np.fill_diagonal(dist_matrix, np.inf)\n","\n","# # Find the index of the closest row and the distance for each row\n","# closest_indices = dist_matrix.argmin(axis=1)\n","# closest_distances = dist_matrix.min(axis=1)\n","\n","# # Add the results to the original DataFrame\n","# df_original['Closest_Row'] = closest_indices\n","# df_original['Distance'] = closest_distances\n","\n","# # Method 2: Direct indexing (more efficient)\n","# df_original['new_fam_id'] = df_original['fam_id'].iloc[df_original['Closest_Row']].values\n","\n","# # df_original.drop([\"fam_id\",\"Closest_Row\", \"Distance\"], axis=1, inplace=True)\n","# # df_original.rename(columns={'new_fam_id': 'fam_id'}, inplace=True)\n","\n","# # Initialize NetworkX UnionFind\n","# uf = nx.utils.UnionFind()\n","\n","# # Iterate over each row in the DataFrame to merge the original and new family IDs\n","# for _, row in df_original.iterrows():\n","#     uf.union(row['fam_id'], row['new_fam_id'])\n","\n","# # Assign new family groups\n","# df_original['fam_id1'] = df_original['fam_id'].apply(lambda x: uf[x])\n","\n","# df_original.drop([\"fam_id\",\"Closest_Row\", \"Distance\",\"new_fam_id\"], axis=1, inplace=True)\n","# df_original.rename(columns={'fam_id1': 'fam_id'}, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E486FMLsJmhB"},"outputs":[],"source":["df_original['fam_id'].nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vGu87N9JmhB"},"outputs":[],"source":["df_original"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2B24FkIJmhB"},"outputs":[],"source":["df_original0 = df_original.copy(deep=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_xe4P2nJmhB"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"-oybeAPJJmhB"},"source":["## NODE WITH RANGE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vj8VjkfIJmhB"},"outputs":[],"source":["# it seems model is sensitve to outliers.\n","# i am removing outliers before starting shall i also remove within cluster outiers at every state or at the end\n","# Perhaps measure distance of points to mean or median and outlier that way within the cluster.\n","# Consider speeding this up by searching with nodes at the tips\n","# Also try and sort several loops out.\n","\n","\n","breaker=4\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XK4OfXSQJmhB"},"outputs":[],"source":["# Initialize new columns in the DataFrame for future assignment\n","df_original=df_country.copy(deep=True)\n","df_original['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_original))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z6I_lQQ4JmhB"},"outputs":[],"source":["%%time\n","# Node with loop\n","\n","# df_country2 = df_country.copy(deep=True)\n","# df_country2['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_country2))]\n","# df_original = df_country2.copy(deep=True)\n","\n","# NUCER\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","iteration_list = []\n","sil_perf_list = []\n","calinski_perf_list = []\n","unique_fam_list = []\n","\n","df_original=df_country.copy(deep=True)\n","df_original['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_original))]\n","\n","for iteration_num in range(10):\n","    # 2. MAIN NETWORK CLUSTERING\n","\n","    ## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","\n","    import pandas as pd\n","    import numpy as np\n","    from sklearn.metrics import pairwise_distances\n","\n","    ### Outlier removal test\n","\n","    # Calculate the Euclidean distance matrix using pairwise_distances\n","    features = df_original.drop('fam_id', axis=1).copy(deep=True)\n","    dist_matrix = pd.DataFrame(pairwise_distances(features, metric='euclidean'), columns=df_original.index, index=df_original.index)\n","\n","    # Initialize a column to store the closest row from a different family\n","    df_original['closest_row'] = None\n","    df_original['closest_distance'] = np.inf\n","\n","    # Iterate over each row to find the closest row from a different family\n","    for i, row_i in df_original.iterrows():\n","        for j, row_j in df_original.iterrows():\n","            if row_i['fam_id'] != row_j['fam_id']:  # Ensure different families\n","                distance = dist_matrix.at[i, j]\n","                if distance < df_original.at[i, 'closest_distance']:\n","                    df_original.at[i, 'closest_distance'] = distance\n","                    df_original.at[i, 'closest_row'] = j\n","\n","    # 2B set up families to migrate to and solve conflicts if mulitple row in the same family want to migrate to the different families\n","    # Add a column for the family that the closest row belongs to for easier reference\n","    df_original['closest_family'] = df_original['closest_row'].apply(lambda x: df_original.at[x, 'fam_id'] if pd.notnull(x) else None)\n","\n","    # Determine the closest family for each original family based on the smallest distance\n","    closest_family_for_group = df_original.groupby('fam_id').apply(lambda x: x.loc[x['closest_distance'].idxmin(), 'closest_family'])\n","\n","    # Map the target family for migration back to the original DataFrame\n","    df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","    import pandas as pd\n","    import networkx as nx\n","    #df_original.reset_index(drop=True, inplace=True)\n","    # Example DataFrame\n","    df = df_original.copy(deep=True)\n","    G = nx.Graph()\n","\n","    # Add edges based on the 'original_family' and 'families_to_join_with' relationships\n","    for _, row in df.iterrows():\n","        G.add_edge(row['fam_id'], row['migrate_to_family'])\n","\n","    # Find the connected components (i.e., the groups of families to join)\n","    connected_components = list(nx.connected_components(G))\n","\n","    # Create a new column in the DataFrame to assign each row to its connected component (family group)\n","    family_group_mapping = {}\n","    for i, component in enumerate(connected_components):\n","        for family_id in component:\n","            family_group_mapping[family_id] = i + 1  # Group numbering starts from 1\n","\n","    df['fam_id'] = df['migrate_to_family'].map(family_group_mapping)\n","\n","    #,connected_components\n","    df_original = df.drop(['closest_row', 'closest_distance', 'closest_family',\n","        'migrate_to_family'], axis=1).copy(deep=True)\n","\n","    # Solution to all my issues.. recenter\n","    def calculate_centroids(df):\n","            \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","            centroids = df.groupby('fam_id').mean()\n","            return centroids\n","\n","    def reassign_families(df, centroids):\n","        \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","        # Calculate distances between each row and each centroid\n","        distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","        # Find the closest centroid for each row\n","        closest_centroids = np.argmin(distances, axis=1)\n","\n","        # Map centroid indices back to family labels\n","        index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","        df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","        return df\n","\n","    # Calculate initial centroids\n","    centroids = calculate_centroids(df_original)\n","\n","    # Reassign families based on closest centroid\n","    df_reassigned = reassign_families(df_original, centroids)\n","\n","    df_original = df_reassigned.copy(deep=True)\n","\n","\n","    if df_original.fam_id.nunique() == 1:\n","        break\n","\n","    labels = df_original.fam_id\n","    selected_columns = df_original.drop(['fam_id'], axis = 1)\n","    sil_perf = silhouette_score(selected_columns, labels)\n","    calinski_harabasz = calinski_harabasz_score(selected_columns, labels)\n","    sil_perf_list.append(sil_perf)\n","    calinski_perf_list.append(calinski_harabasz)\n","    unique_fam_list.append(df_original.fam_id.nunique())\n","    iteration_list.append(iteration_num)\n","\n","    if df_original.fam_id.nunique() == breaker:\n","        break\n","\n","data = {\n","    'iteration_list': iteration_list,\n","    'sil_perf_list': sil_perf_list,\n","    'calinski_perf_list': calinski_perf_list,\n","    'unique_fam_list': unique_fam_list\n","}\n","\n","# Create DataFrame\n","df_eval = pd.DataFrame(data)\n","\n","df_eval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nPyewjpbJmhB"},"outputs":[],"source":["print(df_original.fam_id.nunique())\n","df_original.reset_index(drop=True, inplace=True)\n","df_original_no_outliers = df_original.copy(deep=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjEsbunZJmhB"},"outputs":[],"source":["# %%time\n","# # OUtlier removal with Node\n","# # Node with loop\n","\n","# # df_country2 = df_country.copy(deep=True)\n","# # df_country2['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_country2))]\n","# # df_original = df_country2.copy(deep=True)\n","\n","# # NUCER\n","# from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","# iteration_list = []\n","# sil_perf_list = []\n","# calinski_perf_list = []\n","# unique_fam_list = []\n","\n","# df_original = df_wine2.copy(deep=True)\n","# for iteration_num in range(10):\n","#     # 2. MAIN NETWORK CLUSTERING\n","\n","#     ## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","\n","#     import pandas as pd\n","#     import numpy as np\n","#     from sklearn.metrics import pairwise_distances\n","\n","#     ### Outlier removal test\n","\n","#     # Calculate the Euclidean distance matrix using pairwise_distances\n","#     features = df_original.drop('fam_id', axis=1).copy(deep=True)\n","#     dist_matrix = pd.DataFrame(pairwise_distances(features, metric='euclidean'), columns=df_original.index, index=df_original.index)\n","\n","#     # Initialize a column to store the closest row from a different family\n","#     df_original['closest_row'] = None\n","#     df_original['closest_distance'] = np.inf\n","\n","#     # Iterate over each row to find the closest row from a different family\n","#     for i, row_i in df_original.iterrows():\n","#         for j, row_j in df_original.iterrows():\n","#             if row_i['fam_id'] != row_j['fam_id']:  # Ensure different families\n","#                 distance = dist_matrix.at[i, j]\n","#                 if distance < df_original.at[i, 'closest_distance']:\n","#                     df_original.at[i, 'closest_distance'] = distance\n","#                     df_original.at[i, 'closest_row'] = j\n","\n","#     # 2B set up families to migrate to and solve conflicts if mulitple row in the same family want to migrate to the different families\n","#     # Add a column for the family that the closest row belongs to for easier reference\n","#     df_original['closest_family'] = df_original['closest_row'].apply(lambda x: df_original.at[x, 'fam_id'] if pd.notnull(x) else None)\n","\n","#     # Determine the closest family for each original family based on the smallest distance\n","#     closest_family_for_group = df_original.groupby('fam_id').apply(lambda x: x.loc[x['closest_distance'].idxmin(), 'closest_family'])\n","\n","#     # Map the target family for migration back to the original DataFrame\n","#     df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","#     import pandas as pd\n","#     import networkx as nx\n","#     #df_original.reset_index(drop=True, inplace=True)\n","#     # Example DataFrame\n","#     df = df_original.copy(deep=True)\n","#     G = nx.Graph()\n","\n","#     # Add edges based on the 'original_family' and 'families_to_join_with' relationships\n","#     for _, row in df.iterrows():\n","#         G.add_edge(row['fam_id'], row['migrate_to_family'])\n","\n","#     # Find the connected components (i.e., the groups of families to join)\n","#     connected_components = list(nx.connected_components(G))\n","\n","#     # Create a new column in the DataFrame to assign each row to its connected component (family group)\n","#     family_group_mapping = {}\n","#     for i, component in enumerate(connected_components):\n","#         for family_id in component:\n","#             family_group_mapping[family_id] = i + 1  # Group numbering starts from 1\n","\n","#     df['fam_id'] = df['migrate_to_family'].map(family_group_mapping)\n","\n","#     #,connected_components\n","#     df_original = df.drop(['closest_row', 'closest_distance', 'closest_family',\n","#         'migrate_to_family'], axis=1).copy(deep=True)\n","\n","#     # Solution to all my issues.. recenter\n","#     def calculate_centroids(df):\n","#             \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","#             centroids = df.groupby('fam_id').mean()\n","#             return centroids\n","\n","#     def reassign_families(df, centroids):\n","#         \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","#         # Calculate distances between each row and each centroid\n","#         distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","#         # Find the closest centroid for each row\n","#         closest_centroids = np.argmin(distances, axis=1)\n","\n","#         # Map centroid indices back to family labels\n","#         index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","#         df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","#         return df\n","\n","#     # Calculate initial centroids\n","#     centroids = calculate_centroids(df_original)\n","\n","#     # Reassign families based on closest centroid\n","#     df_reassigned = reassign_families(df_original, centroids)\n","\n","#     ### Outlier test\n","\n","#     from scipy.spatial.distance import cdist\n","#     from scipy.spatial.distance import cdist\n","#     import numpy as np\n","\n","#     from scipy.spatial.distance import cdist\n","\n","#     def remove_outliers(df, centroids, z_thresh=3):\n","#         \"\"\"\n","#         Removes rows that are outliers based on their distance from the centroid of the family they have joined.\n","\n","#         Parameters:\n","#         - df: DataFrame with rows assigned to families.\n","#         - centroids: DataFrame containing the centroids of each family.\n","#         - z_thresh: Z-score threshold to identify outliers. Rows with a z-score higher than this are considered outliers.\n","\n","#         Returns:\n","#         - DataFrame with outliers removed.\n","#         \"\"\"\n","#         # Calculate the distance of each row to its centroid\n","#         distances = cdist(df.drop('fam_id', axis=1), centroids, metric='euclidean')\n","#         df['distance_to_centroid'] = np.min(distances, axis=1)\n","\n","#         # Calculate the mean and std of distances within each family\n","#         distance_stats = df.groupby('fam_id')['distance_to_centroid'].agg(['mean', 'std']).reset_index()\n","\n","#         # Merge distance stats back to the original dataframe\n","#         df = df.merge(distance_stats, on='fam_id', how='left')\n","\n","#         # Calculate the z-score for distances\n","#         df['distance_z_score'] = (df['distance_to_centroid'] - df['mean']) / df['std']\n","\n","#         # Remove outliers based on the z-score\n","#         df_filtered = df[df['distance_z_score'] <= z_thresh].drop(['distance_to_centroid', 'mean', 'std', 'distance_z_score'], axis=1)\n","\n","#         return df_filtered\n","\n","#     # Recalculate centroids after family reassignment\n","#     centroids = calculate_centroids(df_reassigned)\n","\n","#     # Remove outliers\n","#     df_no_outliers = remove_outliers(df_reassigned, centroids)\n","\n","\n","#     # Recalculate centroids after family reassignment\n","#     #centroids = calculate_centroids(df_reassigned)\n","\n","#     # Remove outliers\n","\n","#     df_original = df_no_outliers.copy(deep=True)\n","\n","\n","#     if df_original.fam_id.nunique() == 1:\n","#         break\n","\n","#     labels = df_original.fam_id\n","#     selected_columns = df_original.drop(['fam_id'], axis = 1)\n","#     sil_perf = silhouette_score(selected_columns, labels)\n","#     calinski_harabasz = calinski_harabasz_score(selected_columns, labels)\n","#     sil_perf_list.append(sil_perf)\n","#     calinski_perf_list.append(calinski_harabasz)\n","#     unique_fam_list.append(df_original.fam_id.nunique())\n","#     iteration_list.append(iteration_num)\n","\n","#     if df_original.fam_id.nunique() == breaker:\n","#         break\n","\n","# data = {\n","#     'iteration_list': iteration_list,\n","#     'sil_perf_list': sil_perf_list,\n","#     'calinski_perf_list': calinski_perf_list,\n","#     'unique_fam_list': unique_fam_list\n","# }\n","\n","# # Create DataFrame\n","# df_eval = pd.DataFrame(data)\n","\n","# df_eval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_CP7_hQzJmhB"},"outputs":[],"source":["df_country0[\"fam_id\"] = df_original.fam_id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NdfBkkO9JmhB"},"outputs":[],"source":["df_country0[\"country\"] = coun_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vBpkkP9cJmhB"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","df_original1 = df_country.copy(deep=True)#.drop(\"fam_id\", axis=1)\n","kmeans = KMeans(n_clusters=3, max_iter= 1000).fit(df_original1)\n","df_original1['class'] = kmeans.labels_\n","df_original1['KmeansLabel'] = df_original1['class']\n","df_original1.drop('class', axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N58v0OlGJmhB"},"outputs":[],"source":["df_country0[\"HDBSCANLabel\"] = df_original2.HDBSCANLabel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwz_fnXSJmhC"},"outputs":[],"source":["df_country0.groupby([\"HDBSCANLabel\"]).gdpp.agg(['mean','count'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c5cRq_zeJmhC"},"outputs":[],"source":["df_country0.groupby([\"KmeansLabel\"]).gdpp.agg(['mean','count'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNhlAQqjJmhC"},"outputs":[],"source":["df_country0[df_country0.country == 'Ghana']"]},{"cell_type":"markdown","metadata":{"id":"7ziDTzuxJmhC"},"source":["# Fam_id map"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8wOF_FLJmhC"},"outputs":[],"source":["df_country0[df_country0.fam_id == 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8H3kLXUeJmhC"},"outputs":[],"source":["df_country0.groupby([\"fam_id\"]).gdpp.agg(['mean','count'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXPZYx18JmhC"},"outputs":[],"source":["df_country23 = df_country0.copy(deep=True)\n","#df_country23.drop('KmeansLabel', axis =1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFpWjmJ-JmhC"},"outputs":[],"source":["# Mapping dictionary\n","mapping = {10: '1', 119: '119'}\n","\n","# Replace values in the column\n","df_country23['fam_id'] = df_country23['fam_id'].replace(mapping)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_CQzprfZJmhC"},"outputs":[],"source":["import plotly.express as px\n","#!pip install -U kaleido\n","import kaleido"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWJYHiRaJmhC"},"outputs":[],"source":["# Fam_id\n","fig = px.choropleth(df_country23[['country','fam_id']],\n","                    locationmode = 'country names',\n","                    locations = 'country',\n","                    title = 'Needed Help Per Country (World)',\n","                    color = df_country23['fam_id'])\n","fig.update_geos(fitbounds = \"locations\", visible = True)\n","fig.update_layout(legend_title_text = 'Labels',legend_title_side = 'top',title_pad_l = 260,title_y = 0.86)\n","fig.show(engine = 'kaleido')"]},{"cell_type":"markdown","metadata":{"id":"5511TWKlJmhC"},"source":["# Kmeans_viz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F47Po3SoJmhC"},"outputs":[],"source":["df_country0.groupby([\"KmeansLabel\"]).gdpp.agg(['median','count'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vuwj4WEZJmhC"},"outputs":[],"source":["# Kmeans Label\n","df_country23 = df_country0.copy(deep=True)\n","df_country23.drop('fam_id', axis =1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43C-b_lWJmhC"},"outputs":[],"source":["# Mapping dictionary\n","mapping = {1: 'medium', 2: 'low', 0: 'high'}\n","# Replace values in the column\n","df_country23['KmeansLabel'] = df_country23['KmeansLabel'].replace(mapping)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fagOs_rmJmhC"},"outputs":[],"source":["# Kmeans\n","fig = px.choropleth(df_country23[['country','KmeansLabel']],\n","                    locationmode = 'country names',\n","                    locations = 'country',\n","                    title = 'Needed Help Per Country (World)',\n","                    color = df_country23['KmeansLabel'],\n","                    color_discrete_map={'low':'Red',\n","                                        'medium':'Yellow',\n","                                        'medium-high':'Orange',\n","                                        'high':'Green'})\n","fig.update_geos(fitbounds = \"locations\", visible = True)\n","fig.update_layout(legend_title_text = 'Labels',legend_title_side = 'top',title_pad_l = 260,title_y = 0.86)\n","fig.show(engine = 'kaleido')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"skzz29nvJmhC"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yNdrKcnCJmhC"},"outputs":[],"source":["# df_complex_nested.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xa79WSZ2JmhC"},"outputs":[],"source":["# Was k means label sneaking its way into nucer before?\n","# recenter not based on centeroid but count of family vs count of own family ? Perhaps only move if count if more than its own\n","# removing oultliers every stage does lead to better clusters but then it will affect data quantity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5lTuZT5JmhC"},"outputs":[],"source":["# Initialize new columns in the DataFrame for future assignment\n","df_complex_nested_sample1['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_complex_nested_sample1))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MwdHjmJYJmhC"},"outputs":[],"source":["# STRICT NETWORK CLUSTERING\n","df_original = df_complex_nested_sample1.copy(deep=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQlkjM2qJmhD"},"outputs":[],"source":["df_complex_nested_sample1.drop(\"fam_id\", axis =1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LchMDfwJmhD"},"outputs":[],"source":["df_original = dsc(df_complex_nested_sample1) # , random_df_state=random_integer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7SA_Y6lyJmhD"},"outputs":[],"source":["# 2. STRICT NETWORK CLUSTERING\n","\n","## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import pairwise_distances\n","\n","\n","# Calculate the Euclidean distance matrix using pairwise_distances\n","# features = df_original.drop('fam_id', axis=1)\n","features = df_original.drop('fam_id', axis=1)\n","dist_matrix = pd.DataFrame(pairwise_distances(features, metric='euclidean'), columns=df_original.index, index=df_original.index)\n","\n","# Initialize a column to store the closest row from a different family\n","df_original['closest_row'] = None\n","df_original['closest_distance'] = np.inf\n","\n","# Iterate over each row to find the closest row from a different family\n","for i, row_i in df_original.iterrows():\n","    for j, row_j in df_original.iterrows():\n","        if row_i['fam_id'] != row_j['fam_id']:  # Ensure different families\n","            distance = dist_matrix.at[i, j]\n","            if distance < df_original.at[i, 'closest_distance']:\n","                df_original.at[i, 'closest_distance'] = distance\n","                df_original.at[i, 'closest_row'] = j\n","\n","\n","# 2B set up families to migrate to and solve conflicts if mulitple row in the same family want to migrate to the different families\n","# Add a column for the family that the closest row belongs to for easier reference\n","df_original['closest_family'] = df_original['closest_row'].apply(lambda x: df_original.at[x, 'fam_id'] if pd.notnull(x) else None)\n","\n","# Determine the closest family for each original family based on the smallest distance\n","closest_family_for_group = df_original.groupby('fam_id').apply(lambda x: x.loc[x['closest_distance'].idxmin(), 'closest_family'])\n","\n","# Map the target family for migration back to the original DataFrame\n","df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","# Roundup\n","df_original['closest_distance'] = df_original['closest_distance'].round(5)\n","\n","import pandas as pd\n","\n","# 2C\n","#now if there is another family with a row that is closest to say family B be but the distance is larger\n","# than the closeset distance of another row in another family to the same family B, then that row cant migrate\n","\n","# To implement the additional condition where a row (and by extension, its family) can't migrate to a target family if there exists another row (from a different family)\n","# that is closer to the same target family, we need to adjust our approach. This condition adds a layer of priority based on the closest distance,\n","#ensuring that only the family with the closest row to a target family can migrate, preventing other families with larger distances from migrating to the same target.\n","\n","# 2A 1: Potential migrations have already been identified in 'migrate_to_family'.\n","\n","# 2B 2: Resolve conflicts where multiple families aim to migrate to the same target family.\n","\n","# 1. Add fam_count column\n","fam_counts = df_original['fam_id'].value_counts()\n","df_original['fam_count'] = df_original['fam_id'].map(fam_counts)\n","\n","# Calculate the minimum distance to target for each migrate_to_family\n","min_distance_to_target = df_original.groupby('migrate_to_family')['closest_distance'].min()\n","\n","# Check if its family has the smallest distance to the target it wishes to migrate to\n","df_original['can_migrate'] = df_original.apply(\n","    lambda x: x['closest_distance'] == min_distance_to_target.get(x['migrate_to_family'], float('inf')), axis=1)\n","\n","# Adjust migration logic\n","def adjust_migration(row):\n","    if row['fam_count'] == 1 and not row['can_migrate']:\n","        return row['fam_id']\n","    elif row['fam_count'] > 1:\n","        migrate_values = df_original[df_original['fam_id'] == row['fam_id']]['migrate_to_family']\n","        if migrate_values.nunique() == 1 and pd.notna(migrate_values.iloc[0]):\n","            return migrate_values.iloc[0]\n","        else:\n","            return row['fam_id']\n","    return row['migrate_to_family']\n","\n","df_original['migrate_to_family'] = df_original.apply(adjust_migration, axis=1)\n","\n","# Remove rows that cannot migrate (if needed according to your logic)\n","# This step is conditional based on whether you still want to remove non-migratable families or not\n","#df_original.loc[~df_original['can_migrate'], 'migrate_to_family'] = None\n","\n","# Finally, show the dataframe\n","\n","import pandas as pd\n","import networkx as nx\n","#df_original.reset_index(drop=True, inplace=True)\n","# Example DataFrame\n","df = df_original.copy(deep=True)\n","G = nx.Graph()\n","\n","# Add edges based on the 'original_family' and 'families_to_join_with' relationships\n","for _, row in df.iterrows():\n","    G.add_edge(row['fam_id'], row['migrate_to_family'])\n","\n","# Find the connected components (i.e., the groups of families to join)\n","connected_components = list(nx.connected_components(G))\n","\n","# Create a new column in the DataFrame to assign each row to its connected component (family group)\n","family_group_mapping = {}\n","for i, component in enumerate(connected_components):\n","    for family_id in component:\n","        family_group_mapping[family_id] = i + 1  # Group numbering starts from 1\n","\n","df['fam_id'] = df['migrate_to_family'].map(family_group_mapping)\n","\n","#,connected_components\n","df_original = df.drop(['closest_row', 'closest_distance', 'closest_family',\n","       'migrate_to_family', 'fam_count', 'can_migrate'], axis=1).copy(deep=True)\n","\n","def calculate_centroids(df):\n","        \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","        centroids = df.groupby('fam_id').mean()\n","        return centroids\n","\n","def reassign_families(df, centroids):\n","    \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","    # Calculate distances between each row and each centroid\n","    distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","    # Find the closest centroid for each row\n","    closest_centroids = np.argmin(distances, axis=1)\n","\n","    # Map centroid indices back to family labels\n","    index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","    df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","    return df\n","\n","# Calculate initial centroids\n","centroids = calculate_centroids(df_original)\n","\n","# Reassign families based on closest centroid\n","df_reassigned = reassign_families(df_original, centroids)\n","df_original = df_reassigned.copy(deep=True)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLCxSzInJmhD"},"outputs":[],"source":["print(df_original.fam_id.nunique())\n","df_original.reset_index(drop=True, inplace=True)\n","df_original_no_outliers = df_original.copy(deep=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0kpE_XtJmhD"},"outputs":[],"source":["# # 2. STRICT NETWORK CLUSTERING with outlier removal and new assignment\n","\n","# ## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","\n","# import pandas as pd\n","# import numpy as np\n","# from sklearn.metrics import pairwise_distances\n","\n","\n","# # Calculate the Euclidean distance matrix using pairwise_distances\n","# # features = df_original.drop('fam_id', axis=1)\n","# features = df_original.drop('fam_id', axis=1)\n","# dist_matrix = pd.DataFrame(pairwise_distances(features, metric='euclidean'), columns=df_original.index, index=df_original.index)\n","\n","# # Initialize a column to store the closest row from a different family\n","# df_original['closest_row'] = None\n","# df_original['closest_distance'] = np.inf\n","\n","# # Iterate over each row to find the closest row from a different family\n","# for i, row_i in df_original.iterrows():\n","#     for j, row_j in df_original.iterrows():\n","#         if row_i['fam_id'] != row_j['fam_id']:  # Ensure different families\n","#             distance = dist_matrix.at[i, j]\n","#             if distance < df_original.at[i, 'closest_distance']:\n","#                 df_original.at[i, 'closest_distance'] = distance\n","#                 df_original.at[i, 'closest_row'] = j\n","\n","\n","# # 2B set up families to migrate to and solve conflicts if mulitple row in the same family want to migrate to the different families\n","# # Add a column for the family that the closest row belongs to for easier reference\n","# df_original['closest_family'] = df_original['closest_row'].apply(lambda x: df_original.at[x, 'fam_id'] if pd.notnull(x) else None)\n","\n","# # Determine the closest family for each original family based on the smallest distance\n","# closest_family_for_group = df_original.groupby('fam_id').apply(lambda x: x.loc[x['closest_distance'].idxmin(), 'closest_family'])\n","\n","# # Map the target family for migration back to the original DataFrame\n","# df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","# # Roundup\n","# df_original['closest_distance'] = df_original['closest_distance'].round(5)\n","\n","# import pandas as pd\n","\n","# # 2C\n","# #now if there is another family with a row that is closest to say family B be but the distance is larger\n","# # than the closeset distance of another row in another family to the same family B, then that row cant migrate\n","\n","# # To implement the additional condition where a row (and by extension, its family) can't migrate to a target family if there exists another row (from a different family)\n","# # that is closer to the same target family, we need to adjust our approach. This condition adds a layer of priority based on the closest distance,\n","# #ensuring that only the family with the closest row to a target family can migrate, preventing other families with larger distances from migrating to the same target.\n","\n","# # 2A 1: Potential migrations have already been identified in 'migrate_to_family'.\n","\n","# # 2B 2: Resolve conflicts where multiple families aim to migrate to the same target family.\n","\n","# # 1. Add fam_count column\n","# fam_counts = df_original['fam_id'].value_counts()\n","# df_original['fam_count'] = df_original['fam_id'].map(fam_counts)\n","\n","# # Calculate the minimum distance to target for each migrate_to_family\n","# min_distance_to_target = df_original.groupby('migrate_to_family')['closest_distance'].min()\n","\n","# # Check if its family has the smallest distance to the target it wishes to migrate to\n","# df_original['can_migrate'] = df_original.apply(\n","#     lambda x: x['closest_distance'] == min_distance_to_target.get(x['migrate_to_family'], float('inf')), axis=1)\n","\n","# # Adjust migration logic\n","# def adjust_migration(row):\n","#     if row['fam_count'] == 1 and not row['can_migrate']:\n","#         return row['fam_id']\n","#     elif row['fam_count'] > 1:\n","#         migrate_values = df_original[df_original['fam_id'] == row['fam_id']]['migrate_to_family']\n","#         if migrate_values.nunique() == 1 and pd.notna(migrate_values.iloc[0]):\n","#             return migrate_values.iloc[0]\n","#         else:\n","#             return row['fam_id']\n","#     return row['migrate_to_family']\n","\n","# df_original['migrate_to_family'] = df_original.apply(adjust_migration, axis=1)\n","\n","# # Remove rows that cannot migrate (if needed according to your logic)\n","# # This step is conditional based on whether you still want to remove non-migratable families or not\n","# #df_original.loc[~df_original['can_migrate'], 'migrate_to_family'] = None\n","\n","# # Finally, show the dataframe\n","\n","# import pandas as pd\n","# import networkx as nx\n","# #df_original.reset_index(drop=True, inplace=True)\n","# # Example DataFrame\n","# df = df_original.copy(deep=True)\n","# G = nx.Graph()\n","\n","# # Add edges based on the 'original_family' and 'families_to_join_with' relationships\n","# for _, row in df.iterrows():\n","#     G.add_edge(row['fam_id'], row['migrate_to_family'])\n","\n","# # Find the connected components (i.e., the groups of families to join)\n","# connected_components = list(nx.connected_components(G))\n","\n","# # Create a new column in the DataFrame to assign each row to its connected component (family group)\n","# family_group_mapping = {}\n","# for i, component in enumerate(connected_components):\n","#     for family_id in component:\n","#         family_group_mapping[family_id] = i + 1  # Group numbering starts from 1\n","\n","# df['fam_id'] = df['migrate_to_family'].map(family_group_mapping)\n","\n","# #,connected_components\n","# df_original = df.drop(['closest_row', 'closest_distance', 'closest_family',\n","#        'migrate_to_family', 'fam_count', 'can_migrate'], axis=1).copy(deep=True)\n","\n","# def calculate_centroids(df):\n","#         \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","#         centroids = df.groupby('fam_id').mean()\n","#         return centroids\n","\n","# def reassign_families(df, centroids):\n","#     \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","#     # Calculate distances between each row and each centroid\n","#     distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","#     # Find the closest centroid for each row\n","#     closest_centroids = np.argmin(distances, axis=1)\n","\n","#     # Map centroid indices back to family labels\n","#     index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","#     df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","#     return df\n","\n","# # Calculate initial centroids\n","# centroids = calculate_centroids(df_original)\n","\n","# # Reassign families based on closest centroid\n","# df_reassigned = reassign_families(df_original, centroids)\n","# df_original = df_reassigned.copy(deep=True)\n","\n","\n","# import numpy as np\n","# import pandas as pd\n","# from scipy.spatial import distance\n","\n","# # Assuming df_original0 is your original dataframe\n","# # Selecting numeric columns for centroid calculation, excluding 'fam_id' or any other non-relevant columns\n","# features = df_original.select_dtypes(include=np.number).columns.tolist()\n","# if 'fam_id' in features:\n","#     features.remove('fam_id')\n","\n","# # Calculate the centroid for each family\n","# centroids = df_original.groupby('fam_id')[features].mean().reset_index()\n","\n","# # Calculate pairwise distances to centroid for each member\n","# def calculate_distance(row):\n","#     centroid = centroids[centroids['fam_id'] == row['fam_id']][features].iloc[0]\n","#     return distance.euclidean(row[features], centroid)\n","\n","# df_original['distance_to_centroid'] = df_original.apply(calculate_distance, axis=1)\n","\n","# # Identify outliers using the IQR method for each family (only above upper bound)\n","# def is_outlier(group):\n","#     q3 = group['distance_to_centroid'].quantile(0.75)\n","#     iqr = q3 - group['distance_to_centroid'].quantile(0.25)\n","#     upper_bound = q3 + 1.5 * iqr # 1.5\n","#     group['outlier'] = (group['distance_to_centroid'] > upper_bound)\n","#     return group\n","\n","# df_original = df_original.groupby('fam_id', group_keys=True).apply(is_outlier)\n","\n","# # Assign -1 to outliers\n","# df_original.loc[df_original['outlier'], 'fam_id'] = -1\n","\n","# # Optionally, drop the helper columns\n","# df_original.drop(['distance_to_centroid', 'outlier'], axis=1, inplace=True)\n","\n","# df_original = df_original[df_original.fam_id != -1].copy(deep=True)\n","# df_original.reset_index(inplace=True, drop=True)\n","\n","\n","# # # #New assignment method\n","# from sklearn.metrics import pairwise_distances\n","# df=df_original.drop('fam_id', axis =1).copy()\n","# # Compute the pairwise distances\n","# dist_matrix = pairwise_distances(df, metric='euclidean')\n","\n","# # Replace diagonal with np.inf to ignore self-distance\n","# np.fill_diagonal(dist_matrix, np.inf)\n","\n","# # Find the index of the closest row and the distance for each row\n","# closest_indices = dist_matrix.argmin(axis=1)\n","# closest_distances = dist_matrix.min(axis=1)\n","\n","# # Add the results to the original DataFrame\n","# df_original['Closest_Row'] = closest_indices\n","# df_original['Distance'] = closest_distances\n","\n","# # Method 2: Direct indexing (more efficient)\n","# df_original['new_fam_id'] = df_original['fam_id'].iloc[df_original['Closest_Row']].values\n","\n","# df_original.drop([\"fam_id\",\"Closest_Row\", \"Distance\"], axis=1, inplace=True)\n","# df_original.rename(columns={'new_fam_id': 'fam_id'}, inplace=True)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6fUytZ1JmhD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDKGoTKeJmhD"},"outputs":[],"source":["# import numpy as np\n","# import matplotlib.pyplot as plt\n","\n","# # Generate some random data (replace this with your actual data)\n","# data = df_1['distance_to_closest_row']\n","\n","# import numpy as np\n","# import seaborn as sns\n","# import matplotlib.pyplot as plt\n","\n","# # Generate some random data (replace this with your actual data)\n","\n","\n","# # Plot the density distribution using KDE\n","# plt.figure(figsize=(8, 6))\n","# sns.kdeplot(data, fill=True, color='g')\n","# plt.title('Density Distribution of Numerical Data')\n","# plt.xlabel('Value')\n","# plt.ylabel('Density')\n","# plt.grid(True)\n","# plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7OXiwu0JmhD"},"outputs":[],"source":["dsc(df_iris)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggVSMk2gJmhD"},"outputs":[],"source":["sds = dsc(df_iris)\n","sds.fam_id.nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WtTbDuO4JmhD"},"outputs":[],"source":["breaker = 2\n","#df_iris.drop('fam_id', axis =1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ow8LXGbJmhD"},"outputs":[],"source":["# NUCER\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","import pandas as pd\n","import random\n","import numpy as np\n","\n","# Assuming dsc() is a defined function elsewhere and df_iris is available\n","# Generate a random integer between 1 and 1000\n","random_integer = random.randint(1, 1000)\n","\n","# Initialize DataFrame outside the loop\n","df_template = pd.DataFrame(columns=['iteration', 'silhouette_score', 'calinski_harabasz_score', 'unique_families'])\n","\n","# Initialize lists outside the loop to accumulate values across iterations\n","iteration_list = []\n","sil_perf_list = []\n","calinski_perf_list = []\n","unique_fam_list = []\n","\n","for i in range(200): # Loop for Nuc iteration\n","    # Reset random_integer for each iteration (if needed)\n","\n","\n","    if i == 0:\n","        df_original = dsc(df_iris)  # Initial clustering operation\n","        # Outlier removal.. dont make sense this way\n","        # def iqr_outliers_indexer(df, column_name):\n","        #     Q1 = df[column_name].quantile(0.25)\n","        #     Q3 = df[column_name].quantile(0.75)\n","        #     IQR = Q3 - Q1\n","\n","        #     # Removed the lower_bound as it's not used in this scenario\n","        #     upper_bound = Q3 + 1.5 * IQR\n","\n","        #     # Only checking for values greater than the upper_bound\n","        #     # Only points that are really far away are outliers\n","        #     outliers = df[df[column_name] > upper_bound]\n","        #     return outliers.index\n","\n","        # out_index = iqr_outliers_indexer(df_original, 'searcher_prop')\n","        # df_original = df_original.drop(out_index, axis=0)\n","        #df_original.reset_index(drop=True, inplace=True)\n","        # Calculate initial metrics\n","\n","    labels = df_original.fam_id\n","    selected_columns = df_original.drop(['fam_id'], axis = 1)\n","    sil_perf = silhouette_score(selected_columns, labels)\n","    calinski_harabasz = calinski_harabasz_score(selected_columns, labels)\n","    sil_perf_list.append(sil_perf)\n","    calinski_perf_list.append(calinski_harabasz)\n","    unique_fam_list.append(df_original.fam_id.nunique())\n","    iteration_list.append(i)\n","\n","    # if i != 0:\n","\n","    #     labels = df_original.fam_id\n","    #     selected_columns = df_original.drop(['fam_id', 'fam_count'], axis = 1)\n","    #     sil_perf = silhouette_score(selected_columns, labels)\n","    #     calinski_harabasz = calinski_harabasz_score(selected_columns, labels)\n","\n","\n","    #     # Append initial or updated metrics to lists\n","    #     sil_perf_list.append(sil_perf)\n","    #     calinski_perf_list.append(calinski_harabasz)\n","    #     unique_fam_list.append(df_original.fam_id.nunique())\n","    #     iteration_list.append(i)\n","\n","    if df_original.fam_id.nunique() == breaker:\n","     break\n","\n","\n","    #df_original = df_original.sort_values(by='searcher_prop')\n","    #first_row_each_group = df_original.groupby('fam_id').first().reset_index()\n","\n","    # RATIONAL: I would like to select the median row based on the median distance within the family or searcher_prop\n","    # Function to return the row with the median searcher_prop for each fam\n","    # def row_with_median_searcher_prop(group):\n","    #     # Calculate the median searcher_prop for the current group\n","    #     median_searcher_prop = group['searcher_prop'].median()\n","    #     # Find the row where searcher_prop is closest to the median\n","    #     closest_row = group.iloc[(group['searcher_prop'] - median_searcher_prop).abs().argsort()[:1]]\n","    #     return closest_row\n","\n","    # df_catcher = df_original.copy()\n","    # Apply the function to each group and concatenate the results\n","    #first_row_each_group = df_original.groupby('fam_id', group_keys=False).apply(row_with_median_searcher_prop) #.reset_index(drop=True)\n","    #df_original.drop(['fam_count'], axis=1, inplace=True)\n","    first_row_each_group = df_original.groupby('fam_id').mean(numeric_only=True).reset_index()\n","\n","    # Rename fam_id to track iteration number, and prepare dataframe for next iteration\n","    first_row_each_group.rename(columns={'fam_id': 'fam_id_0'}, inplace=True)\n","\n","    fam_id_df = first_row_each_group['fam_id_0'].copy()\n","\n","\n","    # For mean\n","    # first_row_each_group.drop([\"fam_id_0\"], axis=1, inplace=True)\n","\n","    # first_row_each_group.drop([\"fam_id_0\", \"fam_prop\", \"searcher_prop\", \"fam_count\", 'cond',\n","    #     'initial_searched', 'initial_searched_prop', 'initial_searched_fam_id',\n","    #     'fam_distance', 'initial_searcher_distance'], axis=1, inplace=True)\n","\n","\n","\n","    #first_row_each_group.drop(['fam_id_0','initial_searched_prop', 'fam_count'], axis=1, inplace=True)\n","\n","    #first_row_each_group.drop([\"fam_count\"], axis=1, inplace=True)\n","    #print(first_row_each_group.columns)\n","    df2 = dsc(first_row_each_group) # 0.2\n","\n","\n","\n","    df2['fam_id_0'] = fam_id_df\n","\n","\n","\n","    df_merged = pd.merge(df_original.reset_index(), df2[['fam_id_0','fam_id']],\n","                        left_on='fam_id', right_on='fam_id_0')\n","\n","\n","    # Set the original index back\n","    df_merged.index = df_original.index\n","    df_merged = df_merged.drop(['index'],axis=1)\n","    # print(df_merged)\n","    #df_merged = df_merged.set_index('index')\n","\n","\n","    # Drop unnecessary columns\n","    df_final = df_merged.drop(columns=['fam_id_x','fam_id_0'], axis=1)\n","\n","    # Assuming df_merged is your DataFrame after merging\n","    columns_to_rename = {'fam_id_y': 'fam_id'}\n","\n","    df_final.rename(columns=columns_to_rename, inplace=True)\n","    df_original = df_final.copy(deep=True)\n","\n","    # if i == 0:\n","\n","    #     df_original.drop([ 'fam_prop', 'searcher_prop', 'cond', 'initial_searched',\n","    #     'initial_searched_prop', 'fam_distance', 'initial_searcher_distance',], axis=1, inplace = True)\n","\n","\n","    # Solution to all my issues.. recenter\n","    def calculate_centroids(df):\n","            \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","            centroids = df.groupby('fam_id').mean()\n","            return centroids\n","\n","    def reassign_families(df, centroids):\n","        \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","        # Calculate distances between each row and each centroid\n","        distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","        # Find the closest centroid for each row\n","        closest_centroids = np.argmin(distances, axis=1)\n","\n","        # Map centroid indices back to family labels\n","        index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","        df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","        return df\n","\n","    # Calculate initial centroids\n","    centroids = calculate_centroids(df_original)\n","\n","    # Reassign families based on closest centroid\n","    df_reassigned = reassign_families(df_original, centroids)\n","    df_original = df_reassigned.copy(deep=True)\n","   # print(df_original.fam_id.nunique())\n","\n","    #df_original['fam_count'] = df_original.groupby('fam_id')['fam_id'].transform('count') # Count family id\n","\n","\n","    # Check and potentially break out of the loop if your exit condition is met\n","    # Check and potentially break out of the loop if your exit condition is met\n","\n","\n","# After the loop, create a DataFrame from the accumulated lists\n","df_template = pd.DataFrame({\n","    'iteration': iteration_list,\n","    'silhouette_score': sil_perf_list,\n","    'calinski_harabasz_score': calinski_perf_list,\n","    'unique_families': unique_fam_list\n","})\n","\n","# Now df_template contains metrics from all iterations\n","df_template\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZ_mVlf9JmhD"},"outputs":[],"source":["df_template"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0d8483SWJmhD"},"outputs":[],"source":["df_original.fam_id.nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7q-OIbcDJmhE"},"outputs":[],"source":["df_original"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mAfEXMrMJmhE"},"outputs":[],"source":["# # GPT NUC\n","# breaker =\n","# # NUCER\n","# from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","# import pandas as pd\n","# import random\n","\n","# # Assuming dsc() is a defined function elsewhere and df_iris is available\n","# # Generate a random integer between 1 and 1000\n","# random_integer = random.randint(1, 1000)\n","\n","# # Initialize DataFrame outside the loop\n","# df_template = pd.DataFrame(columns=['iteration', 'silhouette_score', 'calinski_harabasz_score', 'unique_families'])\n","\n","# # Initialize lists outside the loop to accumulate values across iterations\n","# iteration_list = []\n","# sil_perf_list = []\n","# calinski_perf_list = []\n","# unique_fam_list = []\n","\n","# for i in range(20): # Loop for Nuc iteration\n","#     # Reset random_integer for each iteration (if needed)\n","#     random_integer = random.randint(1, 10020)\n","\n","#     if i == 0:\n","#         df_original = dsc(df=df_iris ,random_df_state=random_integer)  # Initial clustering operation\n","#         # Outlier removal\n","\n","#         def iqr_outliers_indexer(df, column_name):\n","#             Q1 = df[column_name].quantile(0.25)\n","#             Q3 = df[column_name].quantile(0.75)\n","#             IQR = Q3 - Q1\n","\n","#             # Removed the lower_bound as it's not used in this scenario\n","#             upper_bound = Q3 + 1.5 * IQR\n","\n","#             # Only checking for values greater than the upper_bound\n","#             # Only points that are really far away are outliers\n","#             outliers = df[df[column_name] > upper_bound]\n","#             return outliers.index\n","\n","#         out_index = iqr_outliers_indexer(df_original, 'searcher_prop')\n","#         df_original = df_original.drop(out_index, axis=0)\n","#         #df_original.reset_index(drop=True, inplace=True)\n","#         # Calculate initial metrics\n","#         labels = df_original.fam_id\n","#         selected_columns = df_original.iloc[:, :4]\n","#         sil_perf = silhouette_score(selected_columns, labels)\n","#         calinski_harabasz = calinski_harabasz_score(selected_columns, labels)\n","\n","#         df_original.drop(['fam_prop', 'searcher_prop', 'cond', 'initial_searched',\n","#         'initial_searched_prop', 'initial_searched_fam_id', 'fam_distance',\n","#         'initial_searcher_distance', 'fam_count'], axis=1, inplace = True)\n","\n","\n","#         def calculate_centroids(df):\n","#                 \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","#                 centroids = df.groupby('fam_id').mean()\n","#                 return centroids\n","\n","#         def reassign_families(df, centroids):\n","#             \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","#             # Calculate distances between each row and each centroid\n","#             distances = pairwise_distances(df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']], centroids, metric='euclidean')\n","\n","#             # Find the closest centroid for each row\n","#             closest_centroids = np.argmin(distances, axis=1)\n","\n","#             # Map centroid indices back to family labels\n","#             index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","#             df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","#             return df\n","\n","#         # Calculate initial centroids\n","#         centroids = calculate_centroids(df_original)\n","\n","#         # Reassign families based on closest centroid\n","#         df_original = reassign_families(df_original, centroids)\n","\n","\n","\n","#     labels = df_original.fam_id\n","#     selected_columns = df_original.iloc[:, :4]\n","#     sil_perf = silhouette_score(selected_columns, labels)\n","#     calinski_harabasz = calinski_harabasz_score(selected_columns, labels)\n","\n","\n","#     # Append initial or updated metrics to lists\n","#     sil_perf_list.append(sil_perf)\n","#     calinski_perf_list.append(calinski_harabasz)\n","#     unique_fam_list.append(df_original.fam_id.nunique())\n","#     iteration_list.append(i)\n","#     print(df_original.fam_id.nunique())\n","#     if df_original.fam_id.nunique() == breaker:\n","#      break\n","\n","\n","#     #df_original = df_original.sort_values(by='searcher_prop')\n","#     #first_row_each_group = df_original.groupby('fam_id').first().reset_index()\n","\n","#     # RATIONAL: I would like to select the median row based on the median distance within the family or searcher_prop\n","#     # Function to return the row with the median searcher_prop for each fam\n","#     # def row_with_median_searcher_prop(group):\n","#     #     # Calculate the median searcher_prop for the current group\n","#     #     median_searcher_prop = group['searcher_prop'].median()\n","#     #     # Find the row where searcher_prop is closest to the median\n","#     #     closest_row = group.iloc[(group['searcher_prop'] - median_searcher_prop).abs().argsort()[:1]]\n","#     #     return closest_row\n","\n","#     # df_catcher = df_original.copy()\n","#     # Apply the function to each group and concatenate the results\n","#     #first_row_each_group = df_original.groupby('fam_id', group_keys=False).apply(row_with_median_searcher_prop) #.reset_index(drop=True)\n","\n","#     first_row_each_group = df_original.groupby('fam_id').median(numeric_only=True).reset_index()\n","#     # Rename fam_id to track iteration number, and prepare dataframe for next iteration\n","#     first_row_each_group.rename(columns={'fam_id': 'fam_id_0'}, inplace=True)\n","\n","#     fam_id_df = first_row_each_group['fam_id_0'].copy()\n","\n","\n","#     # For mean\n","#     first_row_each_group.drop([\"fam_id_0\"], axis=1, inplace=True)\n","\n","#     # first_row_each_group.drop([\"fam_id_0\", \"fam_prop\", \"searcher_prop\", \"fam_count\", 'cond',\n","#     #     'initial_searched', 'initial_searched_prop', 'initial_searched_fam_id',\n","#     #     'fam_distance', 'initial_searcher_distance'], axis=1, inplace=True)\n","\n","\n","\n","#     #first_row_each_group.drop(['fam_id_0','initial_searched_prop', 'fam_count'], axis=1, inplace=True)\n","\n","#     #first_row_each_group.drop([\"fam_count\"], axis=1, inplace=True)\n","#     #print(first_row_each_group.columns)\n","#     df2 = dsc(df = first_row_each_group) # 0.2\n","\n","#     # def calculate_centroids(df):\n","#     #     \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","#     #     centroids = df.groupby('Family').mean()\n","#     #     return centroids\n","\n","#     # def reassign_families(df, centroids):\n","#     #     \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","#     #     # Calculate distances between each row and each centroid\n","#     #     distances = pairwise_distances(df[['X', 'Y']], centroids, metric='euclidean')\n","\n","#     #     # Find the closest centroid for each row\n","#     #     closest_centroids = np.argmin(distances, axis=1)\n","\n","#     #     # Map centroid indices back to family labels\n","#     #     index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","#     #     df['Family'] = [index_to_family[index] for index in closest_centroids]\n","\n","#     #     return df\n","\n","#     # # Calculate initial centroids\n","#     # centroids = calculate_centroids(df)\n","\n","#     # # Reassign families based on closest centroid\n","#     # df = reassign_families(df, centroids)\n","\n","\n","\n","\n","#     df2.drop(['fam_prop', 'searcher_prop', 'cond', 'initial_searched',\n","#     'initial_searched_prop', 'initial_searched_fam_id', 'fam_distance',\n","#     'initial_searcher_distance', 'fam_count'], axis=1, inplace = True)\n","\n","\n","#     def calculate_centroids(df):\n","#             \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","#             centroids = df.groupby('fam_id').mean()\n","#             return centroids\n","\n","#     def reassign_families(df, centroids):\n","#         \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","#         # Calculate distances between each row and each centroid\n","#         distances = pairwise_distances(df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']], centroids, metric='euclidean')\n","\n","#         # Find the closest centroid for each row\n","#         closest_centroids = np.argmin(distances, axis=1)\n","\n","#         # Map centroid indices back to family labels\n","#         index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","#         df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","#         return df\n","\n","#     # Calculate initial centroids\n","#     centroids = calculate_centroids(df2)\n","\n","#     # Reassign families based on closest centroid\n","#     df2 = reassign_families(df2, centroids)\n","\n","#     df2['fam_id_0'] = fam_id_df\n","\n","#     #print(df_original.columns)\n","#     # df_original = df_original.drop([\"fam_prop\", \"searcher_prop\", \"fam_count\", 'cond',\n","#     #    'initial_searched', 'initial_searched_prop',\n","#     #    'initial_searched_fam_id',\n","#     #    'fam_distance',\n","#     #    'initial_searcher_distance'], axis=1)\n","\n","\n","\n","#     df_merged = pd.merge(df_original.reset_index(), df2[['fam_id_0','fam_id']],\n","#                         left_on='fam_id', right_on='fam_id_0')\n","\n","\n","#     # Set the original index back\n","#     df_merged.index = df_original.index\n","#     df_merged = df_merged.drop(['index'],axis=1)\n","#     # print(df_merged)\n","#     #df_merged = df_merged.set_index('index')\n","\n","\n","#     # Drop unnecessary columns\n","#     df_final = df_merged.drop(columns=['fam_id_x','fam_id_0'], axis=1)\n","\n","#     # Assuming df_merged is your DataFrame after merging\n","#     columns_to_rename = {'fam_id_y': 'fam_id'}\n","\n","#     df_final.rename(columns=columns_to_rename, inplace=True)\n","#     df_original = df_final.copy(deep=True)\n","#     df_original['fam_count'] = df_original.groupby('fam_id')['fam_id'].transform('count') # Count family id\n","\n","\n","#     # Check and potentially break out of the loop if your exit condition is met\n","#     print(df_original.fam_id.nunique())\n","\n","# # After the loop, create a DataFrame from the accumulated lists\n","# df_template = pd.DataFrame({\n","#     'iteration': iteration_list,\n","#     'silhouette_score': sil_perf_list,\n","#     'calinski_harabasz_score': calinski_perf_list,\n","#     'unique_families': unique_fam_list\n","# })\n","\n","# # Now df_template contains metrics from all iterations\n","# df_template\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6K63GhzXJmhE"},"outputs":[],"source":["# # NUCER\n","# from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","# import pandas as pd\n","# import random\n","\n","# # Assuming dsc() is a defined function elsewhere and df_iris is available\n","# # Generate a random integer between 1 and 1000\n","# random_integer = random.randint(1, 1000)\n","\n","# # Initialize DataFrame outside the loop\n","# df_template = pd.DataFrame(columns=['iteration', 'silhouette_score', 'calinski_harabasz_score', 'unique_families'])\n","\n","# # Initialize lists outside the loop to accumulate values across iterations\n","# iteration_list = []\n","# sil_perf_list = []\n","# calinski_perf_list = []\n","# unique_fam_list = []\n","\n","# for i in range(20): # Loop for Nuc iteration\n","#     # Reset random_integer for each iteration (if needed)\n","#     random_integer = random.randint(1, 1000)\n","\n","#     if i == 0:\n","#         df_original = dsc(df=df_country)  # Initial clustering operation\n","#         # Outlier removal\n","\n","#         def iqr_outliers_indexer(df, column_name):\n","#             Q1 = df[column_name].quantile(0.25)\n","#             Q3 = df[column_name].quantile(0.75)\n","#             IQR = Q3 - Q1\n","\n","#             # Removed the lower_bound as it's not used in this scenario\n","#             upper_bound = Q3 + 1.5 * IQR\n","\n","#             # Only checking for values greater than the upper_bound\n","#             # Only points that are really far away are outliers\n","#             outliers = df[df[column_name] > upper_bound]\n","#             return outliers.index\n","\n","#         out_index = iqr_outliers_indexer(df_original, 'searcher_prop')\n","#         df_original = df_original.drop(out_index, axis=0)\n","#         #df_original.reset_index(drop=True, inplace=True)\n","#         # Calculate initial metrics\n","#         labels = df_original.fam_id\n","#         selected_columns = df_original.iloc[:, :9]\n","#         sil_perf = silhouette_score(selected_columns, labels)\n","#         calinski_harabasz = calinski_harabasz_score(selected_columns, labels)\n","\n","#     labels = df_original.fam_id\n","#     selected_columns = df_original.iloc[:, :9]\n","#     sil_perf = silhouette_score(selected_columns, labels)\n","#     calinski_harabasz = calinski_harabasz_score(selected_columns, labels)\n","\n","\n","#     # Append initial or updated metrics to lists\n","#     sil_perf_list.append(sil_perf)\n","#     calinski_perf_list.append(calinski_harabasz)\n","#     unique_fam_list.append(df_original.fam_id.nunique())\n","#     iteration_list.append(i)\n","\n","#     # if df_original.fam_id.nunique() == breaker:\n","#     #  break\n","\n","\n","#     #df_original = df_original.sort_values(by='searcher_prop')\n","#     #first_row_each_group = df_original.groupby('fam_id').first().reset_index()\n","\n","#     # RATIONAL: I would like to select the median row based on the median distance within the family or searcher_prop\n","#     # Function to return the row with the median searcher_prop for each fam\n","#     # def row_with_median_searcher_prop(group):\n","#     #     # Calculate the median searcher_prop for the current group\n","#     #     median_searcher_prop = group['searcher_prop'].median()\n","#     #     # Find the row where searcher_prop is closest to the median\n","#     #     closest_row = group.iloc[(group['searcher_prop'] - median_searcher_prop).abs().argsort()[:1]]\n","#     #     return closest_row\n","\n","#     df_catcher = df_original.copy()\n","#     # Apply the function to each group and concatenate the results\n","#     #first_row_each_group = df_original.groupby('fam_id', group_keys=False).apply(row_with_median_searcher_prop) #.reset_index(drop=True)\n","\n","#     first_row_each_group = df_original.groupby('fam_id').median(numeric_only=True).reset_index()\n","#     # Rename fam_id to track iteration number, and prepare dataframe for next iteration\n","#     first_row_each_group.rename(columns={'fam_id': 'fam_id_0'}, inplace=True)\n","\n","#     fam_id_df = first_row_each_group['fam_id_0'].copy()\n","#     # print(first_row_each_group.columns)\n","\n","#     if i == 0:\n","#         first_row_each_group.drop(['fam_id_0', 'initial_searched_prop', 'fam_count'], axis=1, inplace=True)\n","#     else:\n","#         first_row_each_group.drop(['fam_id_0',  'fam_count'], axis=1, inplace=True)\n","\n","\n","\n","#     #first_row_each_group.drop(['fam_id_0','initial_searched_prop', 'fam_count'], axis=1, inplace=True)\n","\n","#     #first_row_each_group.drop([\"fam_count\"], axis=1, inplace=True)\n","#     #print(first_row_each_group.columns)\n","#     df2 = dsc(df = first_row_each_group) # 0.2\n","#     df2['fam_id_0'] = fam_id_df\n","#     if i == 0:\n","#         df_original = df_original.drop([\"fam_prop\", \"searcher_prop\", \"fam_count\", 'cond',\n","#         'initial_searched', 'initial_searched_prop',\n","#         'initial_searched_fam_id',\n","#         'fam_distance',\n","#         'initial_searcher_distance'], axis=1)\n","#     else:\n","#         df_original = df_original.drop([\"fam_count\"], axis=1)\n","\n","\n","\n","#     df_merged = pd.merge(df_original.reset_index(), df2[['fam_id_0','fam_id',\"fam_prop\", \"searcher_prop\", \"fam_count\", 'cond',\n","#        'initial_searched', 'initial_searched_prop',\n","#        'initial_searched_fam_id',\n","#        'fam_distance',\n","#        'initial_searcher_distance']],\n","#                         left_on='fam_id', right_on='fam_id_0')\n","\n","\n","#     # Set the original index back\n","#     df_merged.index = df_original.index\n","#     df_merged = df_merged.drop(['index'],axis=1)\n","#     # print(df_merged)\n","#     #df_merged = df_merged.set_index('index')\n","\n","\n","#     # Drop unnecessary columns\n","#     df_final = df_merged.drop(columns=['fam_id_x','fam_id_0'], axis=1)\n","\n","#     # Assuming df_merged is your DataFrame after merging\n","#     columns_to_rename = {'fam_id_y': 'fam_id'}\n","\n","#     df_final.rename(columns=columns_to_rename, inplace=True)\n","#     df_original = df_final.copy(deep=True)\n","#     df_original['fam_count'] = df_original.groupby('fam_id')['fam_id'].transform('count') # Count family id\n","\n","#     from scipy.spatial import distance\n","#     df_original = df_original[['child_mort', 'exports', 'health', 'imports', 'income', 'inflation',\n","#        'life_expec', 'total_fer', 'gdpp', 'fam_id', 'fam_count']].copy(deep=True)\n","#     # Sort by fam_count to start with the smallest family\n","#     df_sorted = df_original.sort_values(by='fam_count').reset_index(drop=True)\n","\n","#     for i in df_sorted.fam_id.unique():\n","\n","#         # Step 2: Select a row - For illustration, selecting the first row with fam_id == 1\n","#         selected_rows = df_original[df_original['fam_id'] == i]\n","\n","\n","#         for ind in selected_rows.index:\n","#                 selected_row = selected_rows.loc[[ind]]\n","\n","#                 # Step 3: Exclude all rows with the same 'fam_id'\n","#                 filtered_df = df_original[df_original['fam_id'] != selected_row['fam_id'].values[0]].copy()\n","\n","#                 # Prepare your data for distance calculation (excluding the 'fam_id' column)\n","#                 selected_features = selected_row.drop(['fam_id','fam_count'], axis=1)\n","#                 filtered_features = filtered_df.drop(['fam_id','fam_count'], axis=1)\n","#                 selected_features_series = selected_features.squeeze()\n","\n","#                 # Step 4: Calculate Euclidean distances\n","#                 distances = filtered_features.apply(lambda row: distance.euclidean(row, selected_features_series), axis=1)\n","\n","#                 # Find the index of the row with the minimum distance\n","#                 min_distance_index = distances.idxmin()\n","\n","#                 # Retrieve the most similar row's fam_id and the DataFrame index\n","#                 searched_fam_id_c = filtered_df.loc[min_distance_index, 'fam_id']\n","#                 searched_fam_count_c = filtered_df.loc[min_distance_index, 'fam_count']\n","#                 searched_index_c = filtered_df.index[filtered_df.index == min_distance_index][0]\n","#                 searcher_fam_id = selected_row['fam_id'].values[0]\n","#                 searcher_fam_count = selected_row['fam_count'].values[0]\n","\n","#                 if searcher_fam_count > searched_fam_count_c:\n","#                     #  Capture - searched joins searchers family\n","#                     # mask = df_original[df_original['fam_id']==searched_fam_id_c].index\n","#                     # df_original.loc[mask, \"fam_id\"] = searcher_fam_id\n","#                     df_original.loc[searched_index_c, 'fam_id'] = searcher_fam_id\n","#                     continue\n","\n","#                 elif searcher_fam_count <= searched_fam_count_c:\n","#                     # # Captured searcher joins searched family\n","#                     # mask = df_original[df_original['fam_id']==searcher_fam_id].index\n","#                     # df_original.loc[mask, \"fam_id\"] = searched_fam_id_c\n","#                     # #df_original.loc[ind, 'fam_id'] =  searched_fam_id_c\n","#                     continue\n","\n","#                 # elif searcher_fam_count == searched_fam_count_c:\n","#                 #     # Captured searcher joins searched family\n","#                 #     df_original.loc[ind, 'fam_id'] =  searched_fam_id_c\n","#                 #     continue\n","\n","#                 df_original['fam_count'] = df_original.groupby('fam_id')['fam_id'].transform('count') # Count family id\n","#         print(df_original.fam_id.nunique())\n","#         if df_original.fam_id.nunique() == 2:\n","#             df_original_2 = df_original.copy()\n","#             break\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","#     # Check and potentially break out of the loop if your exit condition is met\n","\n","\n","# # After the loop, create a DataFrame from the accumulated lists\n","# df_template = pd.DataFrame({\n","#     'iteration': iteration_list,\n","#     'silhouette_score': sil_perf_list,\n","#     'calinski_harabasz_score': calinski_perf_list,\n","#     'unique_families': unique_fam_list\n","# })\n","\n","# # Now df_template contains metrics from all iterations\n","# df_template\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FfgS30sCJmhE"},"outputs":[],"source":["# df_catcher = pd.DataFrame({\n","#     'fam_id': [1, 1, 2, 2, 3, 3],\n","#     'feature1': [1, 2, 3, 4, 5, 6],\n","#     'feature2': [7, 8, 9, 10, 11, 12],\n","# })\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oDlmSoFqJmhE"},"outputs":[],"source":["# from scipy.spatial import distance\n","# #df_original = df_original[['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'fam_id', 'fam_count']].copy(deep=True)\n","# # Sort by fam_count to start with the smallest family\n","# df_sorted = df_original.sort_values(by='fam_count').reset_index(drop=True)\n","\n","# for i in df_sorted.fam_id.unique():\n","\n","#        # Step 2: Select a row - For illustration, selecting the first row with fam_id == 1\n","#     selected_rows = df_original[df_original['fam_id'] == i]\n","\n","\n","#     for ind in selected_rows.index:\n","#             selected_row = selected_rows.loc[[ind]]\n","\n","#             # Step 3: Exclude all rows with the same 'fam_id'\n","#             filtered_df = df_original[df_original['fam_id'] != selected_row['fam_id'].values[0]].copy()\n","\n","#             # Prepare your data for distance calculation (excluding the 'fam_id' column)\n","#             selected_features = selected_row.drop(['fam_id','fam_count'], axis=1)\n","#             filtered_features = filtered_df.drop(['fam_id','fam_count'], axis=1)\n","#             selected_features_series = selected_features.squeeze()\n","\n","#             # Step 4: Calculate Euclidean distances\n","#             distances = filtered_features.apply(lambda row: distance.euclidean(row, selected_features_series), axis=1)\n","\n","#             # Find the index of the row with the minimum distance\n","#             min_distance_index = distances.idxmin()\n","\n","#             # Retrieve the most similar row's fam_id and the DataFrame index\n","#             searched_fam_id_c = filtered_df.loc[min_distance_index, 'fam_id']\n","#             searched_fam_count_c = filtered_df.loc[min_distance_index, 'fam_count']\n","#             searched_index_c = filtered_df.index[filtered_df.index == min_distance_index][0]\n","#             searcher_fam_id = selected_row['fam_id'].values[0]\n","#             searcher_fam_count = selected_row['fam_count'].values[0]\n","\n","#             if searcher_fam_count > searched_fam_count_c:\n","#                   #  Capture - searched joins searchers family\n","#                   mask = df_original[df_original['fam_id']==searched_fam_id_c].index\n","#                   df_original.loc[mask, \"fam_id\"] = searcher_fam_id\n","#                   # df_original.loc[searched_index_c, 'fam_id'] = searcher_fam_id\n","#                   continue\n","\n","#             elif searcher_fam_count <= searched_fam_count_c:\n","#                   # # Captured searcher joins searched family\n","#                   # mask = df_original[df_original['fam_id']==searcher_fam_id].index\n","#                   # df_original.loc[mask, \"fam_id\"] = searched_fam_id_c\n","#                   # #df_original.loc[ind, 'fam_id'] =  searched_fam_id_c\n","#                   continue\n","\n","\n","#             # if searcher_fam_count > searched_fam_count_c:\n","#             #       #  Capture searched joins searchers family\n","#             #       df_original.loc[searched_index_c, 'fam_id'] = searcher_fam_id\n","\n","#             # elif searcher_fam_count <= searched_fam_count_c:\n","#             #       # Captured searcher joins searched family\n","#             #       df_original.loc[ind, 'fam_id'] =  searched_fam_id_c\n","\n","\n","#             df_original['fam_count'] = df_original.groupby('fam_id')['fam_id'].transform('count') # Count family id\n","#             if df_original.fam_id.nunique() == 3:\n","#                    break\n","#     print(df_original.fam_id.nunique())\n","\n","#     if df_original.fam_id.nunique() == 3:\n","#         break\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3mG1Al6zJmhE"},"outputs":[],"source":["# first_row_each_group\n","# dsc(first_row_each_group)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VCs9W61XJmhE"},"outputs":[],"source":["# #Compare\n","# import pandas as pd\n","# import numpy as np\n","\n","\n","\n","# def euclidean_distance(row1, row2):\n","#     return np.sqrt(np.sum((row1 - row2) ** 2))\n","\n","# # Indexes of the rows you want to compare\n","# index1 = 166\n","# index2 = 27\n","# df_e = df_iris_sample.iloc[:,:4]\n","# # Accessing rows by index and calculating Euclidean distance\n","# distance = euclidean_distance(df_e.loc[index1], df_e.loc[index2])\n","# np.round(distance,5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LXVQH-RuJmhE"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x-3wmkISJmhE"},"outputs":[],"source":["##Most similar\n","\n","target_index = 7\n","import pandas as pd\n","import numpy as np\n","\n","def euclidean_distance(row1, row2):\n","    return np.sqrt(np.sum((row1 - row2) ** 2))\n","\n","def find_most_similar(df, target_index):\n","    target_row = df.loc[target_index]\n","    min_distance = np.inf\n","    most_similar_index = None\n","\n","    for index, row in df.drop(target_index).iterrows():\n","        distance = euclidean_distance(target_row, row)\n","        if distance < min_distance:\n","            min_distance = distance\n","            most_similar_index = index\n","\n","    return most_similar_index, min_distance\n","\n","# Example: Find the most similar row to the row with index 0\n","df_e = df.copy(deep=True)\n","similar_index, distance = find_most_similar(df_e, target_index)\n","print(f\"Most similar row to index {target_index} is index {similar_index} with distance {distance:.5f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZJnHlTyJmhE"},"outputs":[],"source":["df_original[['Feature1','Feature2','Feature3']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9q_rhwVbJmhE"},"outputs":[],"source":["## DSCdf_original.iloc[:,:2]\n","df_original.iloc[:,:2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMw2FFelJmhE"},"outputs":[],"source":["df_original.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOlALStyJmhE"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Oe5otvBuJmhF"},"source":["## DSC norm\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":201,"status":"ok","timestamp":1714575779347,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"TZaneXA5JmhF"},"outputs":[],"source":["df_14['fam_id'] = cluster_labels\n","df_original = df_14.copy(deep=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":337,"status":"ok","timestamp":1714575780829,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"Yvt-Il3xJmhF","outputId":"2955b94a-5fc6-491c-ee2a-d309e5c6ba62"},"outputs":[],"source":["# DSC- PURE\n","# df_original = df_original_2.copy()\n","\n","### DOUBLE CHECK iloc\n","df_wine_selected_columns = df_original.drop('fam_id',axis=1).copy(deep=True)\n","\n","import pandas as pd\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","# Assume 'labels' is the array of cluster labels obtained from a clustering algorithm\n","# For example: labels = [0, 1, 0, 1] - replace this with your actual labels\n","labels = df_original.fam_id\n","# Calculate metrics\n","silhouette_avg = silhouette_score(df_wine_selected_columns, labels)\n","calinski_harabasz = calinski_harabasz_score(df_wine_selected_columns, labels)\n","davies_bouldin = davies_bouldin_score(df_wine_selected_columns, labels)\n","\n","print(f\"Silhouette Score: {silhouette_avg}\")\n","print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n","print(f\"Davies-Bouldin Index: {davies_bouldin}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CB5EcD9_JmhF"},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","# Assuming 'new_array' is your NumPy array with the last column as 'fam_id'\n","# Drop 'fam_id' column for feature data\n","features = new_array[:, :-1].astype(float)\n","# features = new_array[:, :-1]  # All rows, all columns except the last\n","\n","# Extract 'fam_id' for labels\n","labels = new_array[:, -1]  # Last column for labels\n","\n","# Calculate metrics\n","silhouette_avg = silhouette_score(features, labels)\n","calinski_harabasz = calinski_harabasz_score(features, labels)\n","davies_bouldin = davies_bouldin_score(features, labels)\n","\n","print(f\"Silhouette Score: {silhouette_avg}\")\n","print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n","print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n"]},{"cell_type":"markdown","metadata":{"id":"mGyGPnUkJmhF"},"source":["## Kmeans"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FRyS5bQJmhF"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","df_original1 = df_iris.copy(deep=True)#.drop(\"fam_id\", axis=1)\n","kmeans = KMeans(n_clusters=3).fit(df_original1)\n","df_original1['class'] = kmeans.labels_\n","df_original1['KmeansLabel'] = df_original1['class']\n","df_original1.drop('class', axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fiEfQ5ABJmhF"},"outputs":[],"source":["df_wine_selected_columns = df_original1.drop('KmeansLabel',axis=1).copy(deep=True)\n","\n","import pandas as pd\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","# Assume 'labels' is the array of cluster labels obtained from a clustering algorithm\n","# For example: labels = [0, 1, 0, 1] - replace this with your actual labels\n","labels = df_original1.KmeansLabel\n","# Calculate metrics\n","silhouette_avg = silhouette_score(df_wine_selected_columns, labels)\n","calinski_harabasz = calinski_harabasz_score(df_wine_selected_columns, labels)\n","davies_bouldin = davies_bouldin_score(df_wine_selected_columns, labels)\n","\n","print(f\"Silhouette Score: {silhouette_avg}\")\n","print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n","print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_dzm8n6kJmhF"},"source":["## HDBSCAN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0IauobzJmhF"},"outputs":[],"source":["# First, ensure you have hdbscan installed\n","# You can install it via pip if you haven't already:\n","# pip install hdbscan\n","\n","import hdbscan\n","df_original2 = df_credit_card_3.copy(deep=True)\n","\n","# Instantiate and fit the HDBSCAN model\n","# Note: HDBSCAN has many parameters you can tweak according to your dataset's needs.\n","# min_cluster_size is a key parameter to adjust.\n","clusterer = hdbscan.HDBSCAN().fit(df_original2)\n","\n","# Assign the labels generated by HDBSCAN to your dataframe\n","df_original2['HDBSCANLabel'] = clusterer.labels_\n","\n","# Optionally, if you want to identify and handle noise points (labeled as -1),\n","# you could handle them separately here.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EodtG4isJmhF"},"outputs":[],"source":["df_original2_1 = df_original2[df_original2['HDBSCANLabel'] != -1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ur5b3bc0JmhF"},"outputs":[],"source":["\n","df_wine_selected_columns = df_original2_1.drop('HDBSCANLabel',axis=1).copy(deep=True)\n","\n","import pandas as pd\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","# Assume 'labels' is the array of cluster labels obtained from a clustering algorithm\n","# For example: labels = [0, 1, 0, 1] - replace this with your actual labels\n","labels = df_original2_1.HDBSCANLabel\n","# Calculate metrics\n","silhouette_avg = silhouette_score(df_wine_selected_columns, labels)\n","calinski_harabasz = calinski_harabasz_score(df_wine_selected_columns, labels)\n","davies_bouldin = davies_bouldin_score(df_wine_selected_columns, labels)\n","\n","print(f\"Silhouette Score: {silhouette_avg}\")\n","print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n","print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p5BOXlTIJmhF"},"outputs":[],"source":["df_original2_1.HDBSCANLabel.nunique()"]},{"cell_type":"markdown","metadata":{"id":"0x9DTTSbJmhF"},"source":["## OPTICS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8CWyn_wzJmhH"},"outputs":[],"source":["from sklearn.cluster import OPTICS\n","df_original4 = df_wine1.copy(deep=True)  # Assuming df_wine1 is your dataset\n","\n","# Applying OPTICS clustering\n","optics = OPTICS()  # Default parameters, adjust as necessary\n","optics.fit(df_original4)\n","\n","# Adding the labels to your dataframe\n","df_original4['OPTICSLabel'] = optics.labels_\n","\n","# Note: OPTICS labels clusters starting from 0, with noise points labeled as -1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3NqYpq6aJmhH"},"outputs":[],"source":["df_original4['OPTICSLabel'].nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RuR-IPMIJmhH"},"outputs":[],"source":["df_original4_1 = df_original4[df_original4.OPTICSLabel != -1]\n","df_wine_selected_columns = df_original4_1.drop('OPTICSLabel',axis=1).copy(deep=True)\n","\n","import pandas as pd\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","# Assume 'labels' is the array of cluster labels obtained from a clustering algorithm\n","# For example: labels = [0, 1, 0, 1] - replace this with your actual labels\n","labels = df_original4_1.OPTICSLabel\n","# Calculate metrics\n","silhouette_avg = silhouette_score(df_wine_selected_columns, labels)\n","calinski_harabasz = calinski_harabasz_score(df_wine_selected_columns, labels)\n","davies_bouldin = davies_bouldin_score(df_wine_selected_columns, labels)\n","\n","print(f\"Silhouette Score: {silhouette_avg}\")\n","print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n","print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7o3YCda_JmhH"},"outputs":[],"source":["# import pandas as pd\n","# df_originaldsc= df_original1.copy()\n","# # Assuming 'df_originaldsc' is your DataFrame\n","# # And 'fam_id' is the column indicating the cluster or family ID\n","\n","# # Calculate the mean for each cluster for specified columns\n","# ider = 'fam_id'\n","# cluster_means = df_originaldsc.groupby(ider)[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].mean()\n","\n","# def squared_diff_from_mean(row):\n","#     cluster_mean = cluster_means.loc[row[ider]]\n","#     squared_diff = (row[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']] - cluster_mean) ** 2\n","#     # Return the sum of squared differences as a scalar\n","#     return squared_diff.sum()\n","\n","# # Apply the function, which now correctly returns a scalar for each row\n","# df_originaldsc['squared_difference'] = df_originaldsc.apply(squared_diff_from_mean, axis=1)\n","# # Now, group by ider and sum the squared differences to get the variation per cluster\n","# variation_per_cluster = df_originaldsc.groupby(ider)['squared_difference'].sum()\n","\n","# # Sum the variations of all clusters to get the total variation\n","# total_variation = variation_per_cluster.sum()\n","\n","# print('dsc: ', total_variation)\n","\n","\n","# # Assuming 'df_originaldsc' is your DataFrame\n","# # And 'fam_id' is the column indicating the cluster or family ID\n","\n","# # Calculate the mean for each cluster for specified columns\n","# ider = 'KmeansLabel'\n","# cluster_means = df_originaldsc.groupby(ider)[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].mean()\n","\n","# def squared_diff_from_mean(row):\n","#     cluster_mean = cluster_means.loc[row[ider]]\n","#     squared_diff = (row[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']] - cluster_mean) ** 2\n","#     # Return the sum of squared differences as a scalar\n","#     return squared_diff.sum()\n","\n","# # Apply the function, which now correctly returns a scalar for each row\n","# df_originaldsc['squared_difference'] = df_originaldsc.apply(squared_diff_from_mean, axis=1)\n","# # Now, group by ider and sum the squared differences to get the variation per cluster\n","# variation_per_cluster = df_originaldsc.groupby(ider)['squared_difference'].sum()\n","\n","# # Sum the variations of all clusters to get the total variation\n","# total_variation = variation_per_cluster.sum()\n","\n","# print('KmeansLabel: ', total_variation)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-RAj71jJmhH"},"outputs":[],"source":["# # Older\n","# from sklearn.manifold import TSNE\n","# import matplotlib.pyplot as plt\n","# import numpy as np\n","\n","# # Assuming df_original is your DataFrame and it's already defined\n","\n","# # Applying t-SNE\n","# tsne = TSNE(n_components=2, random_state=76)\n","# X_tsne = tsne.fit_transform(df_original.iloc[:, :4])\n","\n","# plt.figure(figsize=(12, 10))  # Adjust the figure size as needed\n","\n","# # Get unique categories of fam_id\n","# unique_categories = df_original['fam_id'].unique()\n","# num_categories = len(unique_categories)\n","\n","# # Manually specified color palette for better distinction\n","# # This is a basic set, feel free to add more distinct colors as needed\n","# color_palette = [\n","#     '#1f77b4', # muted blue\n","#     '#ff7f0e', # safety orange\n","#     '#2ca02c', # cooked asparagus green\n","#     '#d62728', # brick red\n","#     '#9467bd', # muted purple\n","#     '#8c564b', # chestnut brown\n","#     '#e377c2', # raspberry yogurt pink\n","#     '#7f7f7f', # middle gray\n","#     '#bcbd22', # curry yellow-green\n","#     '#17becf'  # blue-teal\n","# ]\n","# # Extend the color palette if there are more categories than colors\n","# if num_categories > len(color_palette):\n","#     extra_colors = plt.cm.get_cmap('tab20', num_categories - len(color_palette))\n","#     color_palette.extend([extra_colors(i) for i in range(num_categories - len(color_palette))])\n","\n","# # Plot each category\n","# for i, category in enumerate(unique_categories):\n","#     idx = df_original['fam_id'] == category\n","#     plt.scatter(X_tsne[idx, 0], X_tsne[idx, 1], color=color_palette[i % len(color_palette)], label=category, alpha=0.7)\n","#     for j in np.where(idx)[0]:  # Loop through indices of points in this category\n","#         plt.text(X_tsne[j, 0], X_tsne[j, 1], str(df_original.index[j]), fontdict={'weight': 'bold', 'size': 8}, color='black')\n","\n","# plt.title('t-SNE Clustering by fam_id')\n","# plt.xlabel('t-SNE feature 1')\n","# plt.ylabel('t-SNE feature 2')\n","# plt.legend(title='fam_id', bbox_to_anchor=(1.05, 1), loc='upper left')\n","# plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"o92tTmawJmhH"},"source":["## DSC Vis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QaZ73SOxJmhH"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8IhZAyo9JmhH","outputId":"08313f2a-673a-4f56-e4d9-c60746038005"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import umap.umap_ as umap\n","\n","# Assuming 'new_array' is a NumPy array and it's already loaded\n","# Prepare data: Assuming the first columns except the last are features since data is already standardized\n","features = new_array[:, :-1]  # All rows, all columns except the last\n","colors = new_array[:, -1]     # 'fam_id' is the last column\n","\n","# Run UMAP\n","reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n","embedding = reducer.fit_transform(features)\n","\n","# Plotting\n","plt.figure(figsize=(12, 10))\n","scatter = sns.scatterplot(\n","    x=embedding[:, 0],\n","    y=embedding[:, 1],\n","    hue=colors,\n","    palette=\"viridis\",\n","    legend='full'\n",")\n","plt.title('UMAP projection of the dataset clusters, colored by fam_id')\n","plt.xlabel('UMAP 1')\n","plt.ylabel('UMAP 2')\n","plt.legend(title='fam_id')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJW86U_lJmhH"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import umap.umap_ as umap\n","#import umap\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Assuming 'first_row_each_group' is your DataFrame and it's already loaded\n","# new_array = new_array_2.copy()\n","# Prepare data: Assuming the first four columns are features since data is already standardized\n","features = new_array.drop('fam_id',axis=1).copy(deep=True)\n","#features = new_array.copy(deep=True)\n","# Assuming 'fam_id' is a column you want to use for coloring the points\n","colors = new_array['fam_id']\n","\n","# Run UMAP\n","reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n","embedding = reducer.fit_transform(features)\n","\n","# Plotting\n","plt.figure(figsize=(12, 10))\n","# Plot with hue based on 'fam_id'\n","scatter = sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], hue=colors, palette=\"viridis\", legend='full')\n","#scatter = sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], palette=\"viridis\", legend='full')\n","#Label points with the DataFrame index\n","# for i, point in enumerate(embedding):\n","#     plt.text(point[0]+0.1,  # Adding a small offset to x position for clarity\n","#              point[1],\n","#              new_array.index[i],  # The text is the index of the row\n","#              horizontalalignment='left',\n","#              size='small',\n","#              color='black')\n","\n","plt.title('UMAP projection of the dataset clusters, colored by fam_id')\n","plt.xlabel('UMAP 1')\n","plt.ylabel('UMAP 2')\n","plt.legend(title='fam_id')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPSziS4nJmhH"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WNeIih8OJmhH"},"outputs":[],"source":["df_original = filtered_df11.copy(deep=True)\n","df_original = df_original.rename(columns={'label': 'fam_id'})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJio4nMoacb4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dy8ZafE3acWI"},"outputs":[],"source":["#!pip install umap-learn"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385},"executionInfo":{"elapsed":462,"status":"error","timestamp":1714571396217,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"nm9bQJfzJmhH","outputId":"a6251066-1297-4347-c1b9-724c865dc319"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1200x1000 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import numpy as np\n","import pandas as pd\n","#import umap.umap_ as umap\n","import umap\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Assuming 'first_row_each_group' is your DataFrame and it's already loaded\n","# df_original = df_original_2.copy()\n","# Prepare data: Assuming the first four columns are features since data is already standardized\n","features = df_original.drop('fam_id',axis=1).copy(deep=True)\n","#features = df_original.copy(deep=True)\n","# Assuming 'fam_id' is a column you want to use for coloring the points\n","colors = df_original['fam_id']\n","\n","# Run UMAP\n","reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n","embedding = reducer.fit_transform(features)\n","\n","# Plotting\n","plt.figure(figsize=(12, 10))\n","# Plot with hue based on 'fam_id'\n","scatter = sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], hue=colors, palette=\"viridis\", legend='full')\n","#scatter = sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], palette=\"viridis\", legend='full')\n","#Label points with the DataFrame index\n","# for i, point in enumerate(embedding):\n","#     plt.text(point[0]+0.1,  # Adding a small offset to x position for clarity\n","#              point[1],\n","#              df_original.index[i],  # The text is the index of the row\n","#              horizontalalignment='left',\n","#              size='small',\n","#              color='black')\n","\n","plt.title('UMAP projection of the dataset clusters, colored by fam_id')\n","plt.xlabel('UMAP 1')\n","plt.ylabel('UMAP 2')\n","plt.legend(title='fam_id')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"Z-lPrJg1JmhH"},"source":["# Kmean visualisation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cS9If6AEJmhI"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import umap\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Assuming 'first_row_each_group' is your DataFrame and it's already loaded\n","# df_original1 = df_original_2.copy()\n","# Prepare data: Assuming the first four columns are features since data is already standardized\n","features = df_original1.drop('KmeansLabel',axis=1).copy(deep=True)\n","\n","# Assuming 'fam_id' is a column you want to use for coloring the points\n","colors = df_original1['KmeansLabel']\n","\n","# Run UMAP\n","reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n","embedding = reducer.fit_transform(features)\n","\n","# Plotting\n","plt.figure(figsize=(12, 10))\n","# Plot with hue based on 'fam_id'\n","scatter = sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], hue=colors, palette=\"viridis\", legend='full')\n","\n","# Label points with the DataFrame index\n","# for i, point in enumerate(embedding):\n","#     plt.text(point[0]+0.1,  # Adding a small offset to x position for clarity\n","#              point[1],\n","#              df_original1.index[i],  # The text is the index of the row\n","#              horizontalalignment='left',\n","#              size='small',\n","#              color='black')\n","\n","plt.title('UMAP projection of the dataset clusters, colored by fam_id')\n","plt.xlabel('UMAP 1')\n","plt.ylabel('UMAP 2')\n","plt.legend(title='fam_id')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"7tNHXa_yJmhI"},"source":["## HDBSCAN Vis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HOMpLAOUJmhI"},"outputs":[],"source":["# DBSCANS LEAD TO TOO MUCH POINTS LOST\n","\n","print(df_original2.shape)\n","print(df_original2_1.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnDaS3R5JmhI"},"outputs":[],"source":["# Kmean visualisation\n","\n","import numpy as np\n","import pandas as pd\n","import umap\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Assuming 'first_row_each_group' is your DataFrame and it's already loaded\n","# df_original1 = df_original_2.copy()\n","# Prepare data: Assuming the first four columns are features since data is already standardized\n","features = df_original2_1.drop('HDBSCANLabel',axis=1).copy(deep=True)\n","\n","# Assuming 'fam_id' is a column you want to use for coloring the points\n","colors = df_original2_1['HDBSCANLabel']\n","\n","# Run UMAP\n","reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n","embedding = reducer.fit_transform(features)\n","\n","# Plotting\n","plt.figure(figsize=(12, 10))\n","# Plot with hue based on 'fam_id'\n","scatter = sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], hue=colors, palette=\"viridis\", legend='full')\n","\n","# Label points with the DataFrame index\n","# for i, point in enumerate(embedding):\n","#     plt.text(point[0]+0.1,  # Adding a small offset to x position for clarity\n","#              point[1],\n","#              df_original1.index[i],  # The text is the index of the row\n","#              horizontalalignment='left',\n","#              size='small',\n","#              color='black')\n","\n","plt.title('UMAP projection of the dataset clusters, colored by fam_id')\n","plt.xlabel('UMAP 1')\n","plt.ylabel('UMAP 2')\n","plt.legend(title='fam_id')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"rbiDG5yEJmhI"},"source":["## OPTICS Vis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DR5zT1HKJmhI"},"outputs":[],"source":["print(df_original4.shape)\n","print(df_original4_1.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-_4gfWBJmhI"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import umap\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Assuming 'first_row_each_group' is your DataFrame and it's already loaded\n","# df_original3 = df_original_2.copy()\n","# Prepare data: Assuming the first four columns are features since data is already standardized\n","features = df_original4_1.drop('OPTICSLabel',axis=1).copy(deep=True)\n","\n","# Assuming 'fam_id' is a column you want to use for coloring the points\n","colors = df_original4_1['OPTICSLabel']\n","\n","# Run UMAP\n","reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n","embedding = reducer.fit_transform(features)\n","\n","# Plotting\n","plt.figure(figsize=(12, 10))\n","# Plot with hue based on 'fam_id'\n","scatter = sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], hue=colors, palette=\"viridis\", legend='full')\n","\n","# Label points with the DataFrame index\n","# for i, point in enumerate(embedding):\n","#     plt.text(point[0]+0.1,  # Adding a small offset to x position for clarity\n","#              point[1],\n","#              df_original3.index[i],  # The text is the index of the row\n","#              horizontalalignment='left',\n","#              size='small',\n","#              color='black')\n","\n","plt.title('UMAP projection of the dataset clusters, colored by fam_id')\n","plt.xlabel('UMAP 1')\n","plt.ylabel('UMAP 2')\n","plt.legend(title='fam_id')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZj0FeY3JmhI"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_f6nKACKJmhI"},"outputs":[],"source":["1/0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33YFMjOvJmhI"},"outputs":[],"source":["# # tue Newer\n","\n","# from sklearn.manifold import TSNE\n","# import matplotlib.pyplot as plt\n","# import numpy as np\n","\n","# # Assuming df_original is your DataFrame and it's already defined\n","\n","# # Applying t-SNE\n","# tsne = TSNE(n_components=2, random_state=76)\n","# X_tsne = tsne.fit_transform(df_original.iloc[:, :4])\n","\n","# plt.figure(figsize=(12, 10))  # Adjust the figure size as needed\n","\n","# # Get unique categories of fam_id\n","# unique_categories = df_original['fam_id'].unique()\n","# num_categories = len(unique_categories)\n","\n","# # Manually specified color palette for better distinction\n","# # This is a basic set, feel free to add more distinct colors as needed\n","# color_palette = [\n","#     '#1f77b4', # muted blue\n","#     '#ff7f0e', # safety orange\n","#     '#2ca02c', # cooked asparagus green\n","#     '#d62728', # brick red\n","#     '#9467bd', # muted purple\n","#     '#8c564b', # chestnut brown\n","#     '#e377c2', # raspberry yogurt pink\n","#     '#7f7f7f', # middle gray\n","#     '#bcbd22', # curry yellow-green\n","#     '#17becf',  # blue-teal\n","#     '#4b818c', #(Sea Foam Green)\n","#     '#e3778c', #(Soft Coral Pink)\n","#     '#ce77e3', #(Light Lavender)\n","#     '#2322bd', #(Rich Periwinkle Blue)\n","#     '#1762cf', #(Deeper Ocean Blue)\n","#     '#17cf84' #(Bright Seafoam Green)\n","# ]\n","# # Extend the color palette if there are more categories than colors\n","# if num_categories > len(color_palette):\n","#     extra_colors = plt.cm.get_cmap('tab20', num_categories - len(color_palette))\n","#     color_palette.extend([extra_colors(i) for i in range(num_categories - len(color_palette))])\n","\n","# # Plot each category\n","# for i, category in enumerate(unique_categories):\n","#     idx = df_original['fam_id'] == category\n","#     plt.scatter(X_tsne[idx, 0], X_tsne[idx, 1], color=color_palette[i % len(color_palette)], label=category, alpha=0.7)\n","#     for j in np.where(idx)[0]:  # Loop through indices of points in this category\n","#         plt.text(X_tsne[j, 0], X_tsne[j, 1], str(df_original.index[j]), fontdict={'weight': 'bold', 'size': 8}, color='black')\n","\n","# plt.title('t-SNE Clustering by fam_id')\n","# plt.xlabel('t-SNE feature 1')\n","# plt.ylabel('t-SNE feature 2')\n","# plt.legend(title='fam_id', bbox_to_anchor=(1.05, 1), loc='upper left')\n","# plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F67IPiFgJmhI"},"outputs":[],"source":["1/0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cUy2uj3sJmhI"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import umap\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Assuming 'first_row_each_group' is your DataFrame and it's already loaded\n","\n","# Prepare data: Assuming the first four columns are features since data is already standardized\n","features = d1.iloc[:,:4]\n","\n","# Assuming 'fam_id' is a column you want to use for coloring the points\n","colors = d1['fam_id']\n","\n","# Run UMAP\n","reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n","embedding = reducer.fit_transform(features)\n","\n","# Plotting\n","plt.figure(figsize=(12, 10))\n","# Plot with hue based on 'fam_id'\n","scatter = sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], hue=colors, palette=\"viridis\", legend='full')\n","\n","# Label points with the DataFrame index\n","for i, point in enumerate(embedding):\n","    plt.text(point[0]+0.1,  # Adding a small offset to x position for clarity\n","             point[1],\n","             d1.index[i],  # The text is the index of the row\n","             horizontalalignment='left',\n","             size='small',\n","             color='black')\n","\n","plt.title('UMAP projection of the dataset clusters, colored by fam_id')\n","plt.xlabel('UMAP 1')\n","plt.ylabel('UMAP 2')\n","plt.legend(title='fam_id')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wj9NcWCVJmhI"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import umap\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Assuming 'first_row_each_group' is your DataFrame and it's already loaded\n","\n","# Prepare data: Assuming all columns are features since data is already standardized\n","features = first_row_each_group.iloc[:,:4]\n","\n","# Run UMAP\n","reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n","embedding = reducer.fit_transform(features)\n","\n","# Plotting\n","plt.figure(figsize=(12, 10))\n","sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], palette=\"viridis\", legend='full')\n","\n","# Label points with the DataFrame index\n","for i, point in enumerate(embedding):\n","    plt.text(point[0], point[1], first_row_each_group.index[i],\n","             horizontalalignment='left', size='medium', color='black', weight='semibold')\n","\n","plt.title('UMAP projection of the dataset clusters')\n","plt.xlabel('UMAP 1')\n","plt.ylabel('UMAP 2')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nq_G4MQ9JmhI"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aDnnfa-xJmhI"},"outputs":[],"source":["1/0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53SbZ-XMJmhI"},"outputs":[],"source":["# DSC\n","\n","import matplotlib.pyplot as plt\n","# Plotting\n","plt.figure(figsize=(12, 10))\n","\n","# Creating a scatter plot, coloring by 'fam_id'\n","scatter = plt.scatter(df_original['UMAP_1'], df_original['UMAP_2'], c=df_original['fam_id'], cmap='viridis')\n","\n","# Adding a legend\n","legend1 = plt.legend(*scatter.legend_elements(), title=\"fam_id\")\n","plt.gca().add_artist(legend1)\n","\n","# Adding labels and title\n","plt.xlabel('UMAP_1')\n","plt.ylabel('UMAP_2')\n","plt.title('UMAP Projection Colored by fam_id')\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aQVBcHmhJmhI"},"outputs":[],"source":["# DSC\n","\n","import matplotlib.pyplot as plt\n","# Plotting\n","plt.figure(figsize=(12, 10))\n","\n","# Creating a scatter plot, coloring by 'fam_id'\n","scatter = plt.scatter(df_original['UMAP_1'], df_original['UMAP_2'], c=df_original['KmeansLabel'], cmap='viridis')\n","\n","# Adding a legend\n","legend1 = plt.legend(*scatter.legend_elements(), title=\"KmeansLabel\")\n","plt.gca().add_artist(legend1)\n","\n","# Adding labels and title\n","plt.xlabel('UMAP_1')\n","plt.ylabel('UMAP_2')\n","plt.title('UMAP Projection Colored by fam_id')\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PRT8H3V1JmhJ"},"outputs":[],"source":["1/0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KgYrf76JmhJ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YuOa3fyxJmhJ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-TWVHE-JmhJ"},"outputs":[],"source":["1/0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVs-mH0ZJmhJ"},"outputs":[],"source":["# Dynamic Similarity Clustering algorithm\n","\n","import pandas as pd\n","from sklearn.metrics import pairwise_distances\n","\n","def dsc(df, nuclearisation = False): # ,\n","    import numpy as np\n","\n","    df = df.copy()\n","    # df = df.sample(frac=1, axis=0, random_state=random_df_state) #BP76\n","    # df.reset_index(drop=True, inplace=True)\n","\n","    df_numerical = df.copy()\n","\n","    # Initialize new columns in the DataFrame for future assignment\n","    df['unique_identification_col_##_76@_@'] = ['unq_id_' + str(i) for i in range(len(df))]\n","    unique_id_column = 'unique_identification_col_##_76@_@'\n","\n","    df[\"fam_id\"] = None\n","    df[\"fam_prop\"] = None\n","    df[\"searcher_prop\"] = None\n","    df['cond'] = None\n","    df['initial_searched'] = None # For whenever searcher captures a searched\n","    df['initial_searched_prop'] = None\n","    df['initial_searched_fam_id'] = None\n","    df['fam_distance'] = None\n","    df['initial_searcher_distance'] = None\n","\n","    # Convert DataFrame to numpy array for cdist\n","    # Convert DataFrame to numpy array with float64 data type\n","    array1 = df_numerical #.astype(np.float64)\n","    # array1 = df_numerical.copy()\n","\n","    # Compute the distance matrix in float64 precision, then round\n","    # distances = np.round(cdist(array1, array1, metric='cosine').astype(np.float64), decimals=8)\n","    distances = pairwise_distances(array1, metric='euclidean') #.astype(np.float64)\n","    # distances = pairwise_distances(array1, metric='cosine')\n","    distances = np.round(distances, decimals=5)\n","\n","    # Loop through each row in the DataFrame\n","    for searcher_index in range(len(df)):\n","        # Skip the distance from the row to itself by setting it to np.inf\n","        distances[searcher_index, searcher_index] = np.inf\n","\n","        # Find the minimum distance for the current row, excluding itself\n","        min_distance = np.min(distances[searcher_index])\n","\n","        # Find all indices (rows) where the distance equals the minimum distance\n","        closest_rows_indices = np.where(distances[searcher_index] == min_distance)[0]\n","\n","        # Assign searcher_prop\n","        df.loc[searcher_index, \"searcher_prop\"] = min_distance\n","\n","        # Save closest_rows_indices families\n","        searched_families = df.loc[closest_rows_indices, \"fam_id\"].dropna().unique() # drop na changes none to []\n","\n","        # If searcher does not have a family create a new one\n","        if pd.isnull(df['fam_id'][searcher_index]): # If searcher fam id is null. else could do but i wonna be sure\n","            fam_iden = \"fam_\" + df[unique_id_column][searcher_index]\n","            df.loc[searcher_index, 'fam_id'] = fam_iden\n","            df.loc[searcher_index, \"fam_prop\"] = min_distance\n","\n","        searcher_family_prop = df.loc[searcher_index, 'fam_prop']\n","\n","\n","        if searched_families.size > 0:  # Searched has a family. # There are None in here sort it out\n","                    # Find the none indexes and sort out\n","                    # filtered_df = df[df[\"fam_id\"].isin(searched_families)] # doing this will bring along rows you are trying to break away from\n","            # if searcher_index == 94: print(df.loc[94],closest_rows_indices\n","            # #df[df.fam_id=='fam_unq_id_69']\n","            # )\n","\n","            # do index not families, see image on desktop called index indeed\n","            for searched_index in set(closest_rows_indices):# change searched_index to searched_index\n","\n","                if pd.isnull(df.loc[searched_index, 'fam_id']):\n","\n","                   # N1\n","                    if min_distance > searcher_family_prop:\n","                        print(searcher_index)\n","                        df.loc[searcher_index, 'cond'] = '1N1'\n","                        df.loc[searcher_index, 'initial_searched'] = searched_index\n","                        continue\n","\n","                    # N2. if min_distance < searcher_family_prop\n","                    # isolate, orelse it may be hard for it to find a new family giving how strong and existing bond in a family may be, no adultration\n","                    elif min_distance < searcher_family_prop:\n","                        fam_iden = \"100fam_\" + df[unique_id_column][searcher_index] # what if it has a family with its name on it\n","                        df.loc[searcher_index, 'fam_id'] = fam_iden\n","                        df.loc[searcher_index, \"fam_prop\"] = min_distance\n","                        df.loc[searched_index, 'fam_id'] = fam_iden # searched gets searcher familyid\n","                        df.loc[searched_index, 'fam_prop'] = min_distance\n","                        df.loc[searcher_index, 'cond'] = '1N2'\n","                        df.loc[searcher_index, 'initial_searched'] = searched_index\n","                        continue\n","\n","                    # N3. if min_distance == searcher_family_prop\n","                    # Let searched join searcher family\n","                    elif min_distance == searcher_family_prop:\n","                        fam_iden = df.loc[searcher_index, 'fam_id']\n","                        df.loc[searched_index, \"fam_id\"] = fam_iden # select all searched\n","                        df.loc[searched_index, \"fam_prop\"] = searcher_family_prop\n","                        df.loc[searcher_index, 'cond'] = '1N3'\n","                        df.loc[searcher_index, 'initial_searched'] = searched_index\n","                        continue\n","\n","                else:\n","\n","                    searched_family_prop = df.loc[searched_index, 'fam_prop']\n","                    # NOW BOTH SEARCHED AND SEARCHER HAVE FAMILIES\n","\n","                    # 1. min_distance < searcher_family_prop and min_distance < searched_family_prop then do nothing\n","                        # Searcher to leave and form family with searched\n","                    if min_distance < searcher_family_prop and min_distance < searched_family_prop:\n","                        fam_iden = \"0fam_\" + df[unique_id_column][searcher_index] # Added 0 cus the family would have been caputred by that searcher_index\n","                        df.loc[searched_index, 'fam_id'] = fam_iden # searched gets searcher familyid\n","                        df.loc[searcher_index, 'fam_id'] = fam_iden\n","                        df.loc[searched_index, 'fam_prop'] = min_distance\n","                        df.loc[searcher_index, \"fam_prop\"] = min_distance\n","                        df.loc[searcher_index, 'cond'] = '1'\n","                        df.loc[searcher_index, 'initial_searched'] = searched_index\n","                        continue\n","\n","\n","                    # 2. min_distance < searcher_family_prop and min_distance > searched_family_prop\n","                    # OLD...Searcher can't leave family, searched to join searcher's family instead\n","                    # Do nothing or thres, which may affect purity ... THRES ??\n","                    elif min_distance < searcher_family_prop and min_distance > searched_family_prop:\n","                        df.loc[searcher_index, 'cond'] = '2'\n","                        df.loc[searcher_index, 'initial_searched'] = searched_index\n","                        continue\n","\n","                    # 3. min_distance < searcher_family_prop and min_distance == searched_family_prop\n","                    # Searcher to join searched family\n","                    elif min_distance < searcher_family_prop and min_distance == searched_family_prop:\n","                        searched_fam = df.loc[searched_index, \"fam_id\"]\n","                        df.loc[searcher_index, \"fam_id\"] = searched_fam\n","                        df.loc[searcher_index, \"fam_prop\"] = searched_family_prop\n","                        df.loc[searcher_index, 'cond'] = '3'\n","                        df.loc[searcher_index, 'initial_searched'] = searched_index\n","                        continue\n","\n","                    # 4. min_distance > searcher_family_prop and min_distance < searched_family_prop\n","                    # Do nothing or .....THRES ??\n","                    elif min_distance > searcher_family_prop and min_distance < searched_family_prop:\n","                        df.loc[searcher_index, 'cond'] = '4'\n","                        # Searched to join searcher family\n","                        # No nothing rather but test\n","                        df.loc[searcher_index, 'initial_searched'] = searched_index\n","                        continue\n","\n","                    # 5. min_distance > searcher_family_prop and min_distance > searched_family_prop\n","                        # Do nothing\n","                    elif min_distance > searcher_family_prop and min_distance > searched_family_prop:\n","                        df.loc[searcher_index, 'cond'] = '5'\n","                        df.loc[searcher_index, 'initial_searched'] = searched_index\n","                        continue\n","\n","                    # 6. min_distance > searcher_family_prop and min_distance = searched_family_prop\n","                    # OLD Searcher joins searched family\n","                    # Do nothing\n","                    elif min_distance > searcher_family_prop and min_distance == searched_family_prop:\n","                        df.loc[searcher_index, 'cond'] = '6'\n","                        df.loc[searcher_index, 'initial_searched'] = searched_index\n","                        continue\n","\n","                    # 7 min_distance == searcher_family_prop and min_distance < searched_family_prop\n","                        # Searched to join searchers family\n","                    elif min_distance == searcher_family_prop and min_distance < searched_family_prop:\n","                        fam_iden = df.loc[searcher_index, 'fam_id']\n","                        df.loc[searched_index, \"fam_id\"] = fam_iden #  select all searched\n","                        df.loc[searched_index, \"fam_prop\"] = searcher_family_prop\n","                        df.loc[searcher_index, 'cond'] = '7'\n","                        df.loc[searcher_index, 'initial_searched'] = searched_index\n","                        continue\n","\n","\n","                    # 8. min_distance == searcher_family_prop and min_distance > searched_family_prop\n","                    # Do nothing\n","                    elif min_distance == searcher_family_prop and min_distance > searched_family_prop:\n","                        df.loc[searcher_index, 'cond'] = '8'\n","                        df.loc[searcher_index, 'initial_searched'] = searched_index\n","                        continue\n","\n","                    # 9. min_distance = searcher_family_prop and min_distance = searched_family_prop\n","                        # Same family id for all, searcher family id could be used.\n","                    elif min_distance == searcher_family_prop and min_distance == searched_family_prop:\n","\n","                        fam_iden = df.loc[searcher_index, 'fam_id']\n","                        searched_fam = df.loc[searched_index, \"fam_id\"]\n","                        mask = df[df['fam_id']==searched_fam].index\n","                        df.loc[mask, \"fam_id\"] = fam_iden\n","                        #df.loc[mask, \"cond\"] = '9'\n","                        df.loc[searcher_index, 'cond'] = '9'\n","                        # df.loc[mask, 'initial_searched'] = searched_index\n","                        df.loc[searcher_index, 'initial_searched'] = searched_index\n","                        continue\n","\n","\n","\n","\n","        elif searched_families.size == 0: # Searched has no families\n","\n","            # N1. if min_distance > searcher_family_prop\n","            if min_distance > searcher_family_prop:\n","                print('2',searcher_index)\n","                df.loc[searcher_index, 'cond'] = '2N1'\n","                df.loc[searcher_index, 'initial_searched'] = closest_rows_indices[0]\n","                continue\n","\n","            # N2. if min_distance < searcher_family_prop\n","            # isolate, orelse it may be hard for it to find a new family giving how strong and existing bond in a family may be, no adultration\n","            elif min_distance < searcher_family_prop:\n","                fam_iden = \"1100fam_\" + df[unique_id_column][searcher_index] # what if it has a family with its name on it\n","                df.loc[searcher_index, 'fam_id'] = fam_iden\n","                df.loc[searcher_index, \"fam_prop\"] = min_distance\n","                df.loc[closest_rows_indices, 'fam_id'] = fam_iden # searched gets searcher familyid\n","                df.loc[closest_rows_indices, 'fam_prop'] = min_distance\n","                df.loc[searcher_index, 'cond'] = '2N2'\n","                df.loc[searcher_index, 'initial_searched'] = closest_rows_indices[0]\n","                continue\n","\n","            # N3. if min_distance == searcher_family_prop\n","            # Let searched join searcher family\n","            elif min_distance == searcher_family_prop:\n","                fam_iden = df.loc[searcher_index, 'fam_id']\n","                df.loc[closest_rows_indices, \"fam_id\"] = fam_iden # select all searched\n","                df.loc[closest_rows_indices, \"fam_prop\"] = searcher_family_prop\n","                df.loc[searcher_index, 'cond'] = '2N3'\n","                df.loc[searcher_index, 'initial_searched'] = closest_rows_indices[0]\n","                continue\n","\n","\n","\n","    if nuclearisation:\n","\n","        for (fam_id, clean_text_data), subgroup in df.groupby([\"fam_id\", \"clean_text_data\"]):\n","            first_index = subgroup.index[0]  # First index of the subgroup\n","            fam_iden = \"nuc_fam_\" + df.loc[first_index, unique_id_column]  # Assuming 'unique_id_column' is the column name\n","            df.loc[subgroup.index, \"fam_id\"] = fam_iden\n","\n","\n","    # THE GREAT MIGRATION\n","    df['initial_searched_prop'] = df['initial_searched'].apply(lambda x: df.loc[x]['fam_prop'])\n","    df['initial_searched_fam_id'] = df['initial_searched'].apply(lambda x: df.loc[x]['fam_id'])\n","    df['fam_distance'] = df['fam_prop'] - df['searcher_prop']\n","    df['initial_searcher_distance'] = df['searcher_prop'] - df['initial_searched_prop']\n","\n","\n","    df['fam_id'] = np.where(\n","        df['initial_searcher_distance'] < df['fam_distance'],  # Condition\n","        df['initial_searched_fam_id'],  # Value if condition is True\n","        df['fam_id']  # Value if condition is False\n","    )\n","\n","    # Additionally, updating the fam_prop column based on the same condition\n","    df['fam_prop'] = np.where(\n","        df['initial_searcher_distance'] < df['fam_distance'],  # Condition\n","        df['initial_searched_prop'],  # Value if condition is True (set fam_prop to initial_searcher_distance)\n","        df['fam_prop']  # Value if condition is False\n","    )\n","\n","    df.reset_index(inplace=True,drop=True)\n","\n","    df['fam_id'] = pd.factorize(df['fam_id'])[0] # when there is not family id it will come up as -1 as it is blank\n","    #df['initial_searched_fam_id'] = pd.factorize(df['initial_searched_fam_id'])[0] # when there is not family id it will come up as -1 as it is blank\n","\n","\n","    # df['fam_count'] = df.groupby('fam_id')['fam_id'].transform('count') # Count family id\n","\n","    df.sort_values(by='fam_id', inplace=True) # sort by family id\n","    #df.reset_index(inplace=True, drop=True)\n","    df.drop([unique_id_column,'cond',\n","       'initial_searched', 'initial_searched_prop', 'initial_searched_fam_id',\n","       'fam_distance', 'initial_searcher_distance','fam_prop', 'searcher_prop'], axis=1, inplace=True) # ,\"searcher_prop\",\"fam_prop\" ,\"searcher_prop\",\"fam_prop\"\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"plGi42RAJmhJ"},"outputs":[],"source":["# # NUC\n","# from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","# import pandas as pd\n","# import random\n","\n","# # GPT EMB with DSC thres = True .. more liberal less clusters\n","\n","\n","\n","# # Generate a random integer between 1 and 1000\n","# random_integer = random.randint(1, 1000)\n","\n","\n","# df_template = pd.DataFrame(columns=['iteration', 'silhouette_score', 'calinski_harabasz_score', 'unique_families'])\n","\n","# for i in range(20): # Nuc iteration. To determine best 100 unique fam after inital nuc may require 10 iterations total.\n","\n","#     iteration_list = []\n","#     sil_perf_list = []\n","#     calinski_perf_list = []\n","#     unique_fam_list = []\n","\n","#     random_integer = random.randint(1, 1000)\n","#     #random_integer = random.randint(1, 1000)\n","#     if i == 0:\n","#         df_original = dsc(df=df_iris) # 0.2 random_integer\n","\n","#     # Compare 'count_per_cluster_label_gpt_emb' with 'count_per_Manual_Grouping'\n","#     labels = df_original.fam_id\n","#     selected_columns = df_original.iloc[:,:4]\n","#     # Calculate metrics\n","#     calinski_harabasz = calinski_harabasz_score(selected_columns, labels)\n","#     davies_bouldin = davies_bouldin_score(selected_columns, labels)\n","#     sil_perf = silhouette_score(selected_columns, labels)\n","\n","\n","#     #first_row_each_group = df_original.groupby('fam_id').first().reset_index()\n","#     first_row_each_group = df_original.groupby('fam_id').mean(numeric_only=True).reset_index()\n","#     # Rename fam_id to track iteration number, and prepare dataframe for next iteration\n","#     first_row_each_group.rename(columns={'fam_id': 'fam_id_0'}, inplace=True)\n","#     #first_row_each_group.drop([\"fam_prop\", \"searcher_prop\", \"fam_count\"], axis=1, inplace=True)\n","#     first_row_each_group.drop([\"fam_count\"], axis=1, inplace=True)\n","\n","#     df2 = dsc(df = first_row_each_group) # 0.2\n","\n","#     sil_perf_list.append(sil_perf)\n","#     calinski_perf_list.append(calinski_harabasz)\n","#     unique_fam_list.append(df2.fam_id.nunique())\n","#     iteration_list.append(i)\n","\n","\n","#     df_merged = pd.merge(df_original, df2[['fam_id_0','fam_id']],\n","#                             left_on='fam_id', right_on='fam_id_0')\n","\n","\n","#     # Drop unnecessary columns\n","#     df_final = df_merged.drop(columns=['fam_id_x','fam_id_0'], axis=1)\n","\n","#     # Assuming df_merged is your DataFrame after merging\n","#     columns_to_rename = {'fam_id_y': 'fam_id'}\n","\n","#     df_final.rename(columns=columns_to_rename, inplace=True)\n","#     df_original = df_final.copy(deep=True)\n","#     df_original['fam_count'] = df_original.groupby('fam_id')['fam_id'].transform('count') # Count family id\n","\n","#     # Create a DataFrame from the zipped lists\n","#     df22 = pd.DataFrame(list(zip(iteration_list, sil_perf_list, calinski_perf_list,  unique_fam_list)),\n","#                 columns=['iteration', 'silhouette_score', 'calinski_harabasz_score', 'unique_families'])\n","\n","#     df_template = pd.concat([df_template, df22], ignore_index=True)\n","#     if df_original.fam_id.nunique() == 2: break\n","\n","\n","# # # # # Find the maximum score in the DataFrame\n","# # max_diff = df22['Performance'].max()\n","\n","\n","# # # # # Get rows where the score column is equal to the maximum score\n","# # max_diff_rows = df22[df22['Performance'] == max_diff]\n","\n","\n","\n","# df_template\n","\n","# # Change to after nuc instead"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PpNWiQg1JmhJ"},"outputs":[],"source":["dsc(df_iris)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"agOx56l_JmhJ"},"outputs":[],"source":["from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","import pandas as pd\n","import random\n","\n","# Assuming dsc() is a defined function elsewhere and df_iris is available\n","# Generate a random integer between 1 and 1000\n","random_integer = random.randint(1, 1000)\n","\n","# Initialize DataFrame outside the loop\n","df_template = pd.DataFrame(columns=['iteration', 'silhouette_score', 'calinski_harabasz_score', 'unique_families'])\n","\n","# Initialize lists outside the loop to accumulate values across iterations\n","iteration_list = []\n","sil_perf_list = []\n","calinski_perf_list = []\n","unique_fam_list = []\n","\n","for i in range(20): # Loop for Nuc iteration\n","    # Reset random_integer for each iteration (if needed)\n","    random_integer = random.randint(1, 1000)\n","\n","    if i == 0:\n","        df_original = dsc(df=df_iris)  # Initial clustering operation\n","        # Outlier removal\n","\n","        def iqr_outliers_indexer(df, column_name):\n","            Q1 = df[column_name].quantile(0.25)\n","            Q3 = df[column_name].quantile(0.75)\n","            IQR = Q3 - Q1\n","\n","            lower_bound = Q1 - 1.5 * IQR\n","            upper_bound = Q3 + 1.5 * IQR\n","\n","            outliers = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)]\n","            return outliers.index\n","\n","        out_index = iqr_outliers_indexer(df111, 'searcher_prop')\n","        df_original = df_original.drop(out_index, axis=0)\n","        df_original.reset_index(drop=True, inplace=True)\n","        # Calculate initial metrics\n","        labels = df_original.fam_id\n","        selected_columns = df_original.iloc[:, :4]\n","        sil_perf = silhouette_score(selected_columns, labels)\n","        calinski_harabasz = calinski_harabasz_score(selected_columns, labels)\n","\n","    labels = df_original.fam_id\n","    selected_columns = df_original.iloc[:, :4]\n","    sil_perf = silhouette_score(selected_columns, labels)\n","    calinski_harabasz = calinski_harabasz_score(selected_columns, labels)\n","\n","\n","    # Append initial or updated metrics to lists\n","    sil_perf_list.append(sil_perf)\n","    calinski_perf_list.append(calinski_harabasz)\n","    unique_fam_list.append(df_original.fam_id.nunique())\n","    iteration_list.append(i)\n","\n","    if df_original.fam_id.nunique() == 2:\n","     break\n","\n","\n","    #df_original = df_original.sort_values(by='searcher_prop')\n","    #first_row_each_group = df_original.groupby('fam_id').first().reset_index()\n","\n","    # RATIONAL: I would like to select the median row based on the median distance within the family or searcher_prop\n","    # Function to return the row with the median searcher_prop for each fam\n","    def row_with_median_searcher_prop(group):\n","        # Calculate the median searcher_prop for the current group\n","        median_searcher_prop = group['searcher_prop'].median()\n","        # Find the row where searcher_prop is closest to the median\n","        closest_row = group.iloc[(group['searcher_prop'] - median_searcher_prop).abs().argsort()[:1]]\n","        return closest_row\n","\n","    # Apply the function to each group and concatenate the results\n","    first_row_each_group = df_original.groupby('fam_id', group_keys=False).apply(row_with_median_searcher_prop).reset_index(drop=True)\n","\n","    #first_row_each_group = df_original.groupby('fam_id').median(numeric_only=True).reset_index()\n","    # Rename fam_id to track iteration number, and prepare dataframe for next iteration\n","    first_row_each_group.rename(columns={'fam_id': 'fam_id_0'}, inplace=True)\n","    first_row_each_group.drop([\"fam_prop\", \"searcher_prop\", \"fam_count\"], axis=1, inplace=True)\n","    #first_row_each_group.drop([\"fam_count\"], axis=1, inplace=True)\n","\n","    df2 = dsc(df = first_row_each_group) # 0.2\n","\n","\n","    df_merged = pd.merge(df_original, df2[['fam_id_0','fam_id']],\n","                            left_on='fam_id', right_on='fam_id_0')\n","\n","\n","    # Drop unnecessary columns\n","    df_final = df_merged.drop(columns=['fam_id_x','fam_id_0'], axis=1)\n","\n","    # Assuming df_merged is your DataFrame after merging\n","    columns_to_rename = {'fam_id_y': 'fam_id'}\n","\n","    df_final.rename(columns=columns_to_rename, inplace=True)\n","    df_original = df_final.copy(deep=True)\n","    df_original['fam_count'] = df_original.groupby('fam_id')['fam_id'].transform('count') # Count family id\n","\n","\n","    # Check and potentially break out of the loop if your exit condition is met\n","\n","\n","# After the loop, create a DataFrame from the accumulated lists\n","df_template = pd.DataFrame({\n","    'iteration': iteration_list,\n","    'silhouette_score': sil_perf_list,\n","    'calinski_harabasz_score': calinski_perf_list,\n","    'unique_families': unique_fam_list\n","})\n","\n","# Now df_template contains metrics from all iterations\n","df_template\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dVGHUOwxJmhJ"},"outputs":[],"source":["df_original.fam_id.nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZuBAbHFJmhJ"},"outputs":[],"source":["# KD trees suffers from the curse of dimensionality, that i why i can not use it as..perhaps pca first if need be\n","# my primary similarity searcher, however after the"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3biK3HeiJmhJ"},"outputs":[],"source":["# # THE GREAT MIGRATION\n","# df_original['initial_searched_prop'] = df_original['initial_searched'].apply(lambda x: df_original.loc[x]['fam_prop'])\n","# df_original['initial_searched_fam_id'] = df_original['initial_searched'].apply(lambda x: df_original.loc[x]['fam_id'])\n","# df_original['fam_distance'] = df_original['fam_prop'] - df_original['searcher_prop']\n","# df_original['initial_searcher_distance'] = df_original['searcher_prop'] - df_original['initial_searched_prop']\n","\n","# import numpy as np\n","\n","# df_original['fam_id'] = np.where(\n","#     df_original['initial_searcher_distance'] < df_original['fam_distance'],  # Condition\n","#     df_original['initial_searched_fam_id'],  # Value if condition is True\n","#     df_original['fam_id']  # Value if condition is False\n","# )\n","\n","# # Additionally, updating the fam_prop column based on the same condition\n","# df_original['fam_prop'] = np.where(\n","#     df_original['initial_searcher_distance'] < df_original['fam_distance'],  # Condition\n","#     df_original['initial_searched_prop'],  # Value if condition is True (set fam_prop to initial_searcher_distance)\n","#     df_original['fam_prop']  # Value if condition is False\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"caqFpYGVJmhJ"},"outputs":[],"source":["tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CdBGFVQ9JmhK"},"outputs":[],"source":["BallTree.valid_metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HMsQmwmsJmhK"},"outputs":[],"source":["# Cosine for ball tree\n","# Normalise first and them use the normal metric of euclidean\n","\n","# from sklearn.neighbors import NearestNeighbors\n","# from sklearn.preprocessing import normalize\n","\n","# # Assuming `word_embeddings` is your array of word embeddings\n","# normalized_embeddings = normalize(word_embeddings, norm='l2')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mwW1ZlrhJmhK"},"outputs":[],"source":["df_wine1.sample(frac=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4UfzfI4JmhK"},"outputs":[],"source":[" # kdtarees very useful for large datasets\n","# kdtrees very useful for large datasets\n","# Curse of dimensionality\n","# df_iris = df_iris.sample(frac=1)\n","df_original11 = df_original.drop('fam_id', axis=1).copy()\n","from sklearn.neighbors import KDTree, BallTree\n","import numpy as np\n","\n","# Assuming df_iris_sample is a DataFrame with numerical features\n","array1 = df_original11.to_numpy()\n","\n","# Create a k-d tree from the dataset\n","tree = KDTree(array1)\n","\n","# Loop through each row in the dataset to find its nearest neighbor\n","for searcher_index, point in enumerate(array1):\n","    # Reshape the point to a 2D array as required by KDTree.query\n","    point = point.reshape(1, -1)  # Required to avoid deprecation warning about 1D arrays\n","\n","    # Query the tree for the nearest neighbor excluding the point itself\n","    # k=2 returns the closest point and the next closest point\n","    distances, indices = tree.query(point, k=2)\n","\n","    # The first result might be the point itself (distance=0), so take the second result\n","    nearest_distance = distances[0][1]  # The distance to the nearest neighbor\n","    nearest_index = indices[0][1]  # The index of the nearest neighbor\n","\n","    print(searcher_index, nearest_index, nearest_distance)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaOttx6ZJmhK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZBFKDk-JmhK"},"outputs":[],"source":["df_original12 = dsc(df_iris)\n","df_original12"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RaBCW23RJmhK"},"outputs":[],"source":["df_original12.loc[59]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nFyFLYXkJmhK"},"outputs":[],"source":["# Simple dsc broader Dynamic Similarity Clustering algorithm\n","\n","import pandas as pd\n","from sklearn.metrics import pairwise_distances\n","import numpy as np\n","def dsc1(df, nuclearisation = False, random_df_state = 76): # ,\n","\n","\n","\n","    df = df.copy()\n","    #df = df.sample(frac=1, axis=0, random_state=random_df_state) #BP76\n","    # df.reset_index(drop=True, inplace=True)\n","\n","    df_numerical = df.copy()\n","\n","    # Initialize new columns in the DataFrame for future assignment\n","    df['unique_identification_col_##_76@_@'] = ['unq_id_' + str(i) for i in range(len(df))]\n","    unique_id_column = 'unique_identification_col_##_76@_@'\n","\n","    df[\"fam_id\"] = None\n","\n","    # Convert DataFrame to numpy array for cdist\n","    # Convert DataFrame to numpy array with float64 data type\n","    array1 = df_numerical.copy() #.astype(np.float64)\n","    # array1 = df_numerical.copy()\n","\n","    # Compute the distance matrix in float64 precision, then round\n","    # distances = np.round(cdist(array1, array1, metric='cosine').astype(np.float64), decimals=8)\n","    distances = pairwise_distances(array1, metric='euclidean') #.astype(np.float64)\n","    # distances = pairwise_distances(array1, metric='cosine')\n","    # distances = np.round(distances, decimals=5)\n","\n","\n","    # Loop through each row in the DataFrame\n","    for searcher_index in range(len(df)):\n","        # Skip the distance from the row to itself by setting it to np.inf\n","        distances[searcher_index, searcher_index] = np.inf\n","\n","        # Find the minimum distance for the current row, excluding itself\n","        min_distance = np.min(distances[searcher_index])\n","\n","        # Find all indices (rows) where the distance equals the minimum distance\n","        closest_rows_indices = np.where(distances[searcher_index] == min_distance)[0]\n","\n","\n","        # 1. If searched indices do not have a family create one and capture.\n","\n","\n","        # 1. Check if any of the closest_rows_indices have a null fam_id AND if the searcher_index's fam_id is null\n","        if pd.isnull(df.loc[searcher_index, 'fam_id']) and df.loc[closest_rows_indices, 'fam_id'].isnull().all():\n","\n","                # Construct the new fam_id value\n","                fam_iden = \"fam_\" + str(df.loc[searcher_index, unique_id_column])\n","                # Assign the new fam_id to all closest rows and the searcher row\n","                df.loc[closest_rows_indices, 'fam_id'] = fam_iden\n","                df.loc[searcher_index, 'fam_id'] = fam_iden\n","                continue\n","\n","\n","\n","        # 2\n","\n","    # Assuming df, unique_id_column, closest_rows_indices, and searcher_index are defined elsewhere\n","\n","        # Step 1 & 2: Check if the searcher_index's fam_id is null AND if any of the closest_rows_indices have a non-null fam_id\n","        elif pd.isnull(df.loc[searcher_index, 'fam_id']) and df.loc[closest_rows_indices, 'fam_id'].notnull().any():\n","            # Step 3: Select one non-null fam_id from the closest_rows_indices\n","\n","                non_null_fam_ids = df.loc[closest_rows_indices, 'fam_id'].dropna()\n","                if not non_null_fam_ids.empty:\n","                    fam_iden = non_null_fam_ids.iloc[0]  # Take the first non-null fam_id\n","\n","                    # Step 4: Update the fam_id for the searcher_index\n","                    df.loc[searcher_index, 'fam_id'] = fam_iden\n","\n","                    # Identify closest_rows_indices with null fam_id\n","                    null_fam_closest_indices = df.loc[closest_rows_indices, 'fam_id'][df.loc[closest_rows_indices, 'fam_id'].isnull()].index\n","                    # Update their fam_id to the selected one\n","                    df.loc[null_fam_closest_indices, 'fam_id'] = fam_iden\n","\n","                    # Step 5: Unify fam_id for all closest_rows_indices with a non-null fam_id\n","\n","                    for idx in closest_rows_indices:\n","                        if pd.notnull(df.loc[idx, 'fam_id']):\n","                            # Find all rows with the same fam_id as the current closest_row_index and update their fam_id\n","                            mask = df['fam_id'] == df.loc[idx, 'fam_id']\n","                            df.loc[mask, 'fam_id'] = fam_iden\n","                            continue\n","\n","      # 3\n","        # Check if the searcher_index's fam_id is not null\n","        elif pd.notnull(df.loc[searcher_index, 'fam_id']) and df.loc[closest_rows_indices, 'fam_id'].isnull().all():\n","                # Construct the new fam_id value\n","                fam_iden = df.loc[searcher_index, 'fam_id']\n","\n","                # Find all closest_rows_indices that have a null fam_id\n","                null_fam_closest_indices = df.loc[closest_rows_indices, 'fam_id'][df.loc[closest_rows_indices, 'fam_id'].isnull()].index\n","\n","                # Assign the new fam_id to all closest indices that have null fam_id\n","                df.loc[null_fam_closest_indices, 'fam_id'] = fam_iden\n","                continue\n","\n","\n","        # 4 Assuming df, closest_rows_indices, and searcher_index are defined elsewhere\n","\n","        # Check if the searcher_index's fam_id is not null\n","        elif pd.notnull(df.loc[searcher_index, 'fam_id']) and df.loc[closest_rows_indices, 'fam_id'].notnull().all():\n","                # Get the fam_id of the searcher_index\n","                fam_iden = df.loc[searcher_index, 'fam_id']\n","\n","                # Iterate over each of the closest_rows_indices\n","\n","                for idx in closest_rows_indices:\n","                    # Get the fam_id for the current index in closest_rows_indices\n","                    fam_id_closest_index = df.loc[idx, 'fam_id']\n","\n","                    # Find all rows with the same fam_id as the current closest_row_index\n","                    mask = df['fam_id'] == fam_id_closest_index\n","\n","                    # Update their fam_id to match that of the searcher_index\n","                    df.loc[mask, 'fam_id'] = fam_iden\n","                    continue\n","\n","\n","\n","\n","    df['fam_id'] = pd.factorize(df['fam_id'])[0]\n","\n","    df['fam_count'] = df.groupby('fam_id')['fam_id'].transform('count') # Count family id\n","\n","    df.sort_values(by='fam_id', inplace=True) # sort by family id\n","    #df.reset_index(inplace=True, drop=True)\n","    df.drop([unique_id_column], axis=1, inplace=True) # ,\"searcher_prop\",\"fam_prop\" ,\"searcher_prop\",\"fam_prop\"\n","    # 'cond',\n","    #    'initial_searched', 'initial_searched_prop', 'initial_searched_fam_id',\n","    #    'fam_distance', 'initial_searcher_distance'\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kIn8ULHIJmhK"},"outputs":[],"source":["# Simple dsc conservative Dynamic Similarity Clustering algorithm\n","\n","import pandas as pd\n","from sklearn.metrics import pairwise_distances\n","import numpy as np\n","def dsc1(df, nuclearisation = False, random_df_state = 76): # ,\n","\n","\n","\n","    df = df.copy()\n","    df = df.sample(frac=1, axis=0, random_state=random_df_state) #BP76\n","    # df.reset_index(drop=True, inplace=True)\n","\n","    df_numerical = df.copy()\n","\n","    # Initialize new columns in the DataFrame for future assignment\n","    df['unique_identification_col_##_76@_@'] = ['unq_id_' + str(i) for i in range(len(df))]\n","    unique_id_column = 'unique_identification_col_##_76@_@'\n","\n","    df[\"fam_id\"] = None\n","\n","    # Convert DataFrame to numpy array for cdist\n","    # Convert DataFrame to numpy array with float64 data type\n","    array1 = df_numerical.copy() #.astype(np.float64)\n","    # array1 = df_numerical.copy()\n","\n","    # Compute the distance matrix in float64 precision, then round\n","    # distances = np.round(cdist(array1, array1, metric='cosine').astype(np.float64), decimals=8)\n","    distances = pairwise_distances(array1, metric='euclidean') #.astype(np.float64)\n","    # distances = pairwise_distances(array1, metric='cosine')\n","    # distances = np.round(distances, decimals=5)\n","\n","\n","    # Loop through each row in the DataFrame\n","    for searcher_index in range(len(df)):\n","        # Skip the distance from the row to itself by setting it to np.inf\n","        distances[searcher_index, searcher_index] = np.inf\n","\n","        # Find the minimum distance for the current row, excluding itself\n","        min_distance = np.min(distances[searcher_index])\n","\n","        # Find all indices (rows) where the distance equals the minimum distance\n","        closest_rows_indices = np.where(distances[searcher_index] == min_distance)[0]\n","\n","\n","        # 1. If searched indices do not have a family create one and capture.\n","\n","\n","        # 1. Check if any of the closest_rows_indices have a null fam_id AND if the searcher_index's fam_id is null\n","        if pd.isnull(df.loc[searcher_index, 'fam_id']) and df.loc[closest_rows_indices, 'fam_id'].isnull().all():\n","\n","                # Construct the new fam_id value\n","                fam_iden = \"fam_\" + str(df.loc[searcher_index, unique_id_column])\n","                # Assign the new fam_id to all closest rows and the searcher row\n","                df.loc[closest_rows_indices, 'fam_id'] = fam_iden\n","                df.loc[searcher_index, 'fam_id'] = fam_iden\n","                continue\n","\n","\n","\n","        # 2\n","\n","    # Assuming df, unique_id_column, closest_rows_indices, and searcher_index are defined elsewhere\n","\n","        # Step 1 & 2: Check if the searcher_index's fam_id is null AND if any of the closest_rows_indices have a non-null fam_id\n","        elif pd.isnull(df.loc[searcher_index, 'fam_id']) and df.loc[closest_rows_indices, 'fam_id'].notnull().any():\n","            # Step 3: Select one non-null fam_id from the closest_rows_indices\n","                non_null_fam_ids = df.loc[closest_rows_indices, 'fam_id'].dropna()\n","                if not non_null_fam_ids.empty:\n","                    fam_iden = non_null_fam_ids.iloc[0]  # Take the first non-null fam_id\n","\n","                    # Step 4: Update the fam_id for the searcher_index\n","                    df.loc[searcher_index, 'fam_id'] = fam_iden\n","\n","                    # Identify closest_rows_indices with null fam_id\n","                    null_fam_closest_indices = df.loc[closest_rows_indices, 'fam_id'][df.loc[closest_rows_indices, 'fam_id'].isnull()].index\n","                    # Update their fam_id to the selected one\n","                    df.loc[null_fam_closest_indices, 'fam_id'] = fam_iden\n","\n","                    # Step 5: Unify fam_id for all closest_rows_indices with a non-null fam_id\n","\n","                    for idx in closest_rows_indices:\n","                        if pd.notnull(df.loc[idx, 'fam_id']):\n","                            # Find all rows with the same fam_id as the current closest_row_index and update their fam_id\n","                            mask = df['fam_id'] == df.loc[idx, 'fam_id']\n","                            df.loc[mask, 'fam_id'] = fam_iden\n","                    continue\n","\n","      # 3\n","        # Check if the searcher_index's fam_id is not null\n","        elif pd.notnull(df.loc[searcher_index, 'fam_id']) and df.loc[closest_rows_indices, 'fam_id'].isnull().all():\n","                # Construct the new fam_id value\n","                fam_iden = df.loc[searcher_index, 'fam_id']\n","\n","                # Find all closest_rows_indices that have a null fam_id\n","                null_fam_closest_indices = df.loc[closest_rows_indices, 'fam_id'][df.loc[closest_rows_indices, 'fam_id'].isnull()].index\n","\n","                # Assign the new fam_id to all closest indices that have null fam_id\n","                df.loc[null_fam_closest_indices, 'fam_id'] = fam_iden\n","                continue\n","\n","\n","        # 4 Assuming df, closest_rows_indices, and searcher_index are defined elsewhere\n","\n","        # Check if the searcher_index's fam_id is not null\n","        elif pd.notnull(df.loc[searcher_index, 'fam_id']) and df.loc[closest_rows_indices, 'fam_id'].notnull().all():\n","                #Get the fam_id of the searcher_index\n","                fam_iden = df.loc[searcher_index, 'fam_id']\n","\n","                # Iterate over each of the closest_rows_indices\n","\n","                for idx in closest_rows_indices:\n","                    # Get the fam_id for the current index in closest_rows_indices\n","                    fam_id_closest_index = df.loc[idx, 'fam_id']\n","\n","                    # Find all rows with the same fam_id as the current closest_row_index\n","                    mask = df['fam_id'] == fam_id_closest_index\n","\n","                    # Update their fam_id to match that of the searcher_index\n","                    df.loc[mask, 'fam_id'] = fam_iden\n","                    continue\n","\n","\n","\n","\n","    df['fam_id'] = pd.factorize(df['fam_id'])[0]\n","\n","    df['fam_count'] = df.groupby('fam_id')['fam_id'].transform('count') # Count family id\n","\n","    df.sort_values(by='fam_id', inplace=True) # sort by family id\n","    #df.reset_index(inplace=True, drop=True)\n","    df.drop([unique_id_column], axis=1, inplace=True) # ,\"searcher_prop\",\"fam_prop\" ,\"searcher_prop\",\"fam_prop\"\n","    # 'cond',\n","    #    'initial_searched', 'initial_searched_prop', 'initial_searched_fam_id',\n","    #    'fam_distance', 'initial_searcher_distance'\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bx7heMcWJmhK"},"outputs":[],"source":["df_iris_sample1 = df_iris_sample.copy(deep=True)\n","df_iris_sample1['fam_id'] = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g-HcwjAQJmhK"},"outputs":[],"source":["import random\n","random_integer = random.randint(1, 1000)\n","df_original_2 = dsc(df_iris_sample, random_df_state=random_integer)\n","df_original_2.fam_id.nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"szi04LIVJmhK"},"outputs":[],"source":["df_original_2.fam_count.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1AXFL2bJmhK"},"outputs":[],"source":["df_original_2.loc[[14]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4OCR2sg4JmhK"},"outputs":[],"source":["df_original_2[[\"sepal_length\"\t,\"sepal_width\"\t,\"petal_length\", \"petal_width\",\n","               \"fam_id\", \"fam_prop\", \"searcher_prop\", 'initial_searched' ,\"initial_searched_prop\",\n","               \"fam_count\", 'fam_distance']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oQFMSeGGJmhK"},"outputs":[],"source":["df_original_2[df_original_2.fam_id == 6]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YLqrIcuUJmhK"},"outputs":[],"source":["df_original_2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYzeJ74vJmhL"},"outputs":[],"source":[" # kdtarees very useful for large datasets\n","# kdtrees very useful for large datasets\n","# Curse of dimensionality\n","df_original11 =df_iris.iloc[:,:4]\n","from scipy.spatial import cKDTree\n","import numpy as np\n","\n","# Assuming df_iris_sample is a DataFrame with numerical features\n","array1 = df_original11.to_numpy()\n","\n","# Create a k-d tree from the dataset\n","tree = cKDTree(array1)\n","\n","# Loop through each row in the dataset to find its nearest neighbor\n","for searcher_index, point in enumerate(array1):\n","    # Query the tree for the nearest neighbor excluding the point itself\n","    # k=2 returns the closest point and the next closest point (itself being the closest)\n","    distances, indices = tree.query(point, k=2)\n","\n","    # The first result is the point itself (distance=0), so take the second result\n","    nearest_distance = distances[1]  # The distance to the nearest neighbor\n","    nearest_index = indices[1]  # The index of the nearest neighbor\n","\n","    print(searcher_index, nearest_index, nearest_distance)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmih1XCsJmhL"},"outputs":[],"source":["for searcher_index, point in enumerate(array1):\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hq7EGJ4_JmhL"},"outputs":[],"source":["# # More kdtrees\n","# import pandas as pd\n","# from scipy.spatial import cKDTree\n","# import numpy as np\n","\n","# # Creating a DataFrame with integer numbers\n","# data = {\n","#     'Column1': [1, 24, 3, 3, 5],\n","#     'Column2': [62, 74, 8, 8, 10],\n","#     'Column3': [111, 142, 13, 13, 15]\n","# }\n","\n","# df = pd.DataFrame(data)\n","\n","# # Convert the DataFrame to a NumPy array\n","# points = df.to_numpy()\n","\n","# # Create a k-d tree\n","# tree = cKDTree(points)\n","\n","# # Query point\n","# query_point = np.array([3, 8, 13])\n","\n","# # Find the nearest neighbor to get the distance\n","# distance, index = tree.query(query_point)\n","\n","# # Find all points within this distance (including the nearest one)\n","# indices = tree.query_ball_point(query_point, distance)\n","\n","# print(f\"Indices of points with the same lowest distance: {indices} and {distance}\")\n","# # Optionally, you can print the points and their distances\n","# for idx in indices:\n","#     point = points[idx]\n","#     dist = np.linalg.norm(point - query_point)\n","#     print(f\"Index: {idx}, Point: {point}, Distance: {dist}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zi3zXwfBJmhL"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","kmeans = KMeans(n_clusters=3).fit(df_original.iloc[:,:4])\n","df_original['class'] = kmeans.labels_\n","df_original['KmeansLabel'] = df_original['class']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pHb0pJtJJmhL"},"outputs":[],"source":["df_original['KmeansLabel'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKr-KNGQJmhL"},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","\n","# # Example data: Replace these with your actual values\n","# range_n_clusters = list(df_template.unique_families)\n","# silhouette_scores = list(df_template.silhouette_score)\n","# # Plotting\n","# plt.figure(figsize=(10, 6))\n","# plt.plot(range_n_clusters, silhouette_scores, marker='o', linestyle='-', color='b')\n","# plt.title('Silhouette Scores for Various Numbers of Clusters')\n","# plt.xlabel('Number of Clusters')\n","# plt.ylabel('Silhouette Score')\n","# plt.xticks(range_n_clusters)\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3KzvKfTJmhL"},"outputs":[],"source":["# df_original = dsc(df = df_wine, random_df_state = 76)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"weL9mguxJmhL"},"outputs":[],"source":["# HDBSCAN\n","\n","\n","# ! pip install hdbscan\n","# !pip install numpy\n","import numpy as np\n","import hdbscan\n","\n","# Assuming `embeddings` is an array of your text embeddings\n","# For example, embeddings could be generated from df1[\"clean_brandandtitle\"] as shown previously\n","\n","# 'metric' parameter of pairwise_distances must be a str among {'rogerstanimoto', 'sokalsneath', 'nan_euclidean', 'mahalanobis', 'haversine', 'correlation',\n","# 'wminkowski', 'russellrao', 'sqeuclidean', 'matching', 'hamming', 'yule', 'seuclidean', 'braycurtis', 'canberra', 'cosine',\n","# 'l1', 'chebyshev', 'manhattan', 'jaccard', 'dice', 'minkowski', 'precomputed', 'sokalmichener', 'cityblock', 'euclidean', 'l2'}\n","\n","# min_samples\n","# with less data it may be better to specify min cluster size\n","# Initialize and fit HDBSCAN\n","clusterer = hdbscan.HDBSCAN()\n","cluster_labels = clusterer.fit_predict(df_original.iloc[:,:4].values)\n","\n","# Add cluster labels back to your DataFrame\n","df_original['gpt_cluster_label'] = cluster_labels\n","\n","\n","counts = df_original.groupby('gpt_cluster_label')['gpt_cluster_label'].count()\n","df_original['count_per_cluster_label_gpt_emb'] = df_original['gpt_cluster_label'].map(counts)\n","df_original.gpt_cluster_label.nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QE0X5xIcJmhL"},"outputs":[],"source":["# # Speed improvement with dictionary\n","\n","# import numpy as np\n","# from scipy.spatial.distance import cdist\n","\n","# def dsc(data, random_state=76):\n","#     np.random.seed(random_state)\n","#     # Randomly shuffle data indices\n","#     indices = np.random.permutation(len(data))\n","#     data_shuffled = data[indices]\n","\n","#     # Assign unique IDs (could also be done with enumerate if data is already in a structured format)\n","#     unique_ids = np.array(['unq_id_' + str(i) for i in range(len(data_shuffled))])\n","\n","#     # Calculate distance matrix (assuming data is a 2D numpy array)\n","#     distance_matrix = cdist(data_shuffled, data_shuffled, 'euclidean')\n","\n","#     # Initialize properties\n","#     fam_ids = np.full(shape=len(data), fill_value=-1, dtype=np.int32)  # Placeholder for family IDs\n","#     fam_props = np.zeros(shape=len(data))  # Family properties\n","#     searcher_props = np.zeros(shape=len(data))  # Searcher properties\n","\n","#     # Example logic for assigning family based on distance (simplified)\n","#     for i in range(len(distance_matrix)):\n","#         distances = distance_matrix[i]\n","#         inverted_distances = 1 / (distances + 1e-9)\n","#         np.fill_diagonal(inverted_distances, 0)\n","#         max_dist = np.max(inverted_distances)\n","\n","#         if max_dist > 0:\n","#             closest_index = np.argmax(inverted_distances)\n","#             fam_ids[i] = closest_index  # Simplified example of assigning to a family\n","#             fam_props[i] = max_dist  # Example property update\n","#             searcher_props[i] = max_dist  # Example property update\n","\n","#     # Example of post-processing (simplified)\n","#     # Sort by family ID for demonstration purposes\n","#     sort_indices = np.argsort(fam_ids)\n","#     sorted_data = data_shuffled[sort_indices]\n","#     sorted_fam_ids = fam_ids[sort_indices]\n","\n","#     return sorted_data, sorted_fam_ids, fam_props, searcher_props\n","\n","# # Assuming data is a numpy array of your features\n","# # data = np.array([...])\n","\n","# # Example call to the function (assuming 'data' is your dataset)\n","# # sorted_data, sorted_fam_ids, fam_props, searcher_props = dsc(data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L9FPoZsdJmhL"},"outputs":[],"source":["# GPT EMB with HDBSCAN\n","df_wine_selected_columns = df_original.iloc[:,:4]\n","\n","\n","import pandas as pd\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","\n","\n","# Assume 'labels' is the array of cluster labels obtained from a clustering algorithm\n","# For example: labels = [0, 1, 0, 1] - replace this with your actual labels\n","labels = df_original.gpt_cluster_label\n","# Calculate metrics\n","silhouette_avg = silhouette_score(df_wine_selected_columns, labels)\n","calinski_harabasz = calinski_harabasz_score(df_wine_selected_columns, labels)\n","davies_bouldin = davies_bouldin_score(df_wine_selected_columns, labels)\n","\n","print(f\"Silhouette Score: {silhouette_avg}\")\n","print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n","print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_FxLKFRyJmhL"},"outputs":[],"source":["# Kmeans\n","df_wine_selected_columns = df_original.iloc[:,:4]\n","\n","\n","import pandas as pd\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","\n","\n","# Assume 'labels' is the array of cluster labels obtained from a clustering algorithm\n","# For example: labels = [0, 1, 0, 1] - replace this with your actual labels\n","labels = df_original.KmeansLabel\n","# Calculate metrics\n","silhouette_avg = silhouette_score(df_wine_selected_columns, labels)\n","calinski_harabasz = calinski_harabasz_score(df_wine_selected_columns, labels)\n","davies_bouldin = davies_bouldin_score(df_wine_selected_columns, labels)\n","\n","print(f\"Silhouette Score: {silhouette_avg}\")\n","print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n","print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdE8Z3PiJmhL"},"outputs":[],"source":["# DSC- PURE\n","df_wine_selected_columns = df_original.iloc[:,:4]\n","\n","import pandas as pd\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","# Assume 'labels' is the array of cluster labels obtained from a clustering algorithm\n","# For example: labels = [0, 1, 0, 1] - replace this with your actual labels\n","labels = df_original.fam_id\n","# Calculate metrics\n","silhouette_avg = silhouette_score(df_wine_selected_columns, labels)\n","calinski_harabasz = calinski_harabasz_score(df_wine_selected_columns, labels)\n","davies_bouldin = davies_bouldin_score(df_wine_selected_columns, labels)\n","\n","print(f\"Silhouette Score: {silhouette_avg}\")\n","print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n","print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C430mFeqJmhL"},"outputs":[],"source":["# Visualisation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcbpGphvJmhL"},"outputs":[],"source":["# Kmeans\n","\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","# Applying t-SNE\n","tsne = TSNE(n_components=2, random_state=76)\n","X_tsne = tsne.fit_transform(df_original.iloc[:,:4])\n","\n","plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=df_original.KmeansLabel, cmap='viridis')\n","plt.title('t-SNE Clustering')\n","plt.xlabel('t-SNE feature 1')\n","plt.ylabel('t-SNE feature 2')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QTcyaPBXJmhL"},"outputs":[],"source":["# HDBscan\n","\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","# Applying t-SNE\n","tsne = TSNE(n_components=2, random_state=76)\n","X_tsne = tsne.fit_transform(df_original.iloc[:,:4])\n","\n","plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=df_original.gpt_cluster_label, cmap='viridis')\n","plt.title('t-SNE Clustering')\n","plt.xlabel('t-SNE feature 1')\n","plt.ylabel('t-SNE feature 2')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMUmjYDeJmhL"},"outputs":[],"source":["# DSC\n","\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","# Applying t-SNE\n","tsne = TSNE(n_components=2, random_state=76)\n","X_tsne = tsne.fit_transform(df_original.iloc[:,:4])\n","\n","plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=df_original.fam_id, cmap='viridis')\n","plt.title('t-SNE Clustering')\n","plt.xlabel('t-SNE feature 1')\n","plt.ylabel('t-SNE feature 2')\n","plt.show()\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
