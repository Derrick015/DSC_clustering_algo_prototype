{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6236,"status":"ok","timestamp":1714575700829,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"wJfscrIYJmg3"},"outputs":[],"source":["import pandas as pd\n","\n","# try diagnosing from say 6 to 4 or something my indexing seems to work as it should\n","# so i think the issue is when the centroid brings back a group not every row may truly belong to that group\n","# Could there be some kind of test to see if ever row agrees to the new cluster ? or they will switch teams.\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","\n","\n","df_ir = pd.read_csv('/content/drive/My Drive/dsc_dir/IRIS.csv')\n","df_ir1 = df_ir.drop('species', axis=1)\n","# # Initialize the scaler\n","scaler = MinMaxScaler()\n","\n","# Fit the scaler to the data and transform it\n","scaled_data = scaler.fit_transform(df_ir1)\n","\n","# Convert the scaled data back into a pandas DataFrame\n","df_iris = pd.DataFrame(scaled_data, columns=df_ir1.columns)\n","# df_iris = df_iris.sample(frac=1).reset_index(drop=True)\n","# df_iris_sample = df_iris.sample(n=40).reset_index(drop=True)\n"]},{"cell_type":"markdown","metadata":{"id":"jYdJT6IhJmg5"},"source":["# MAIN MODEL"]},{"cell_type":"markdown","metadata":{"id":"PTZY5Dr3Jmg6"},"source":["## Requirements"]},{"cell_type":"markdown","metadata":{"id":"VsS_ClIGJmg6"},"source":["1. USE ROBUST SCALING LIKE MIN MAX AND ROBOST SCALER FOR DATA\n","\n","If the scaling method you used is sensitive to outliers (like Z-score normalization), you might benefit from rescaling after outlier removal. However, if you used a more robust scaling method (like Min-Max scaling or Robust Scaler in sklearn that scales data according to percentiles and is thus less sensitive to outliers), rescaling might not be as necessary.\n"]},{"cell_type":"markdown","metadata":{"id":"VqZRt9tSJmg6"},"source":["## Outlier removal"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6324,"status":"ok","timestamp":1714575805334,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"LZ_cXdeSJmg6"},"outputs":[],"source":["# import pandas as pd\n","# import numpy as np\n","# df11 = pd.read_hdf('/content/drive/My Drive/dsc_dir/Godiva_df_emb.h5', key='df11')\n","# embeddings_array = np.stack(df11[\"gpt_3_large_emb\"].values)"]},{"cell_type":"markdown","metadata":{"id":"Lmdmx-nUJmg7"},"source":["# Array with loop and outlier removal\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Upwl7ZxBJmg7"},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import pairwise_distances\n","\n","# Assuming embeddings is a numpy array already defined\n","df_original = df_14.values.copy()\n","\n","# Compute the pairwise distances\n","dist_matrix = pairwise_distances(df_original, metric='cosine')\n","\n","# Replace diagonal with np.inf to ignore self-distance\n","np.fill_diagonal(dist_matrix, np.inf)\n","\n","# Find the index of the closest row and the distance for each row\n","closest_indices = np.argmin(dist_matrix, axis=1)\n","closest_distances = np.min(dist_matrix, axis=1)\n","\n","def remove_upper_outliers_and_save(data, distances):\n","    quartile_1, quartile_3 = np.percentile(distances, [25, 75])\n","    iqr = quartile_3 - quartile_1\n","    upper_bound = quartile_3 + (iqr * 1.5)\n","\n","    not_outliers_mask = distances <= upper_bound\n","    outliers_mask = distances > upper_bound\n","\n","    filtered_data = data[not_outliers_mask]\n","    outliers_data = data[outliers_mask]\n","\n","    return filtered_data, outliers_data\n","\n","filtered_data, outliers_data = remove_upper_outliers_and_save(df_original, closest_distances)\n","\n","# If you simply want to track the indices, you could just keep them as separate arrays:\n","filtered_ids = np.array(['unq_id_' + str(i) for i in range(filtered_data.shape[0])])\n","outlier_ids = np.array(['unq_id_' + str(i) for i in range(filtered_data.shape[0], filtered_data.shape[0] + outliers_data.shape[0])])\n","\n","def remove_upper_outliers_and_save(distances):\n","    quartile_1, quartile_3 = np.percentile(distances, [25, 75])\n","    iqr = quartile_3 - quartile_1\n","    upper_bound = quartile_3 + 1.5 * iqr\n","    not_outliers_mask = distances <= upper_bound\n","    return not_outliers_mask\n","\n","not_outliers_mask = remove_upper_outliers_and_save(closest_distances)\n","filtered_df11 = df11.iloc[not_outliers_mask].copy(deep=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3k6vxvQ2Jmg7","outputId":"75b27fe4-4f18-473b-ebb5-2db1ecf3764a"},"outputs":[],"source":["%%time\n","\n","import networkx as nx\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import pairwise_distances\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","import numpy as np\n","\n","# Assuming df_original is already defined\n","\n","df_original = filtered_data.copy()\n","# df_original['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_original))]\n","\n","fam_ids = np.array(['unq_id_' + str(i) for i in range(len(df_original))])\n","fam_ids_array = np.array(fam_ids).reshape(-1, 1)\n","new_array = np.concatenate((df_original, fam_ids_array), axis=1)\n","\n","# 2. MAIN NETWORK CLUSTERING\n","\n","## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","# Assuming df is your DataFrame and it includes 'fam_id' along with the features.\n","\n","# Compute the pairwise distances using only the feature columns.\n","dist_matrix0 = pairwise_distances(df_original, metric='cosine')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"erEbAAzoJmg7"},"outputs":[],"source":["breaker = 32"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yoS-QXvJmg7","outputId":"78fae222-153c-496b-bee4-a498cf5725fe"},"outputs":[],"source":["%%time\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","# ee\n","iteration_list = []\n","sil_perf_list = []\n","calinski_perf_list = []\n","unique_fam_list = []\n","\n","\n","# NUCER\n","\n","for ir in range(50):\n","    dist_matrix = np.copy(dist_matrix0)\n","    fam_ids = new_array[:, -1]\n","    same_fam_id = new_array[:, -1][:, None] == new_array[:, -1]\n","    dist_matrix[same_fam_id] = np.inf\n","\n","    closest_indices = np.argmin(dist_matrix, axis=1)\n","    closest_distances = np.min(dist_matrix, axis=1)\n","\n","    # Handling migrations\n","    closest_families = fam_ids[closest_indices]\n","    dtype = [('fam_id', 'U10'), ('closest_distance', 'f8'), ('closest_family', 'U10')]\n","    structured_array = np.array(list(zip(fam_ids, closest_distances, closest_families)), dtype=dtype)\n","    sorted_structured_array = np.sort(structured_array, order=['fam_id', 'closest_distance'])\n","    _, unique_indices = np.unique(sorted_structured_array['fam_id'], return_index=True)\n","    result_structured_array = sorted_structured_array[unique_indices]\n","    migration_map = {fam_id: closest_family for fam_id, closest_family in result_structured_array[['fam_id', 'closest_family']]}\n","    migrate_to_family = np.array([migration_map[fam] for fam in fam_ids])\n","\n","\n","    # Assuming fam_ids, migrate_to_families, and closest_distances are defined as NumPy arrays\n","\n","    # Create a structured array for sorting and unique operation\n","    dtype = [('migrate_to_family', 'U10'), ('closest_distance', float), ('fam_id', 'U10')]\n","    structured_array = np.array(list(zip(migrate_to_family , closest_distances, fam_ids)), dtype=dtype)\n","\n","    # Sort by 'migrate_to_family' and 'closest_distance'\n","    sorted_array = np.sort(structured_array, order=['migrate_to_family', 'closest_distance'])\n","\n","    # Remove duplicates\n","    _, unique_indices = np.unique(sorted_array['migrate_to_family'], return_index=True)\n","    unique_families = sorted_array[unique_indices]\n","\n","    # Mapping for migration decisions\n","    migration_decision_map = {entry['migrate_to_family']: entry['fam_id'] for entry in unique_families}\n","\n","    # Calculate the minimum distance per migration family\n","    unique_families_distances = {entry['migrate_to_family']: entry['closest_distance'] for entry in unique_families}\n","\n","    # Assign migration permissions\n","    can_migrate = np.array([closest_distances[i] == unique_families_distances[migrate_to_family[i]] for i in range(len(fam_ids))])\n","\n","    import networkx as nx\n","\n","    uf = nx.utils.UnionFind()\n","    for i, entry in enumerate(structured_array):\n","        uf.union(entry['fam_id'], entry['migrate_to_family'])\n","\n","    # Map the results back\n","    fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","    mapped_fam_ids = np.array([fam_id_mapping[fam_id] for fam_id in fam_ids])\n","\n","    from sklearn.metrics import pairwise_distances\n","\n","    # Calculate centroids\n","    unique_fam_ids = np.unique(mapped_fam_ids)\n","    centroids = np.array([df_original[mapped_fam_ids == fam_id].mean(axis=0) for fam_id in unique_fam_ids])\n","\n","    # Calculate distances from each point to each centroid and find the closest\n","    distances = pairwise_distances(df_original, centroids, metric='cosine')\n","    closest_centroids = np.argmin(distances, axis=1)\n","\n","    # Assign new fam_ids based on the closest centroid\n","    new_fam_ids = np.array([unique_fam_ids[idx] for idx in closest_centroids])\n","    fam_ids_array1 = np.array(new_fam_ids).reshape(-1, 1)\n","    new_array = np.concatenate((df_original, fam_ids_array1), axis=1)\n","\n","\n","    features = new_array[:, :-1].astype(float)\n","    # features = new_array[:, :-1]  # All rows, all columns except the last\n","\n","    # Extract 'fam_id' for labels\n","    labels = new_array[:, -1]  # Last column for labels\n","\n","\n","\n","\n","    sil_perf = silhouette_score(features, labels)\n","    calinski_harabasz = calinski_harabasz_score(features, labels)\n","    sil_perf_list.append(sil_perf)\n","    calinski_perf_list.append(calinski_harabasz)\n","    unique_fam_list.append(len(set(new_fam_ids)))\n","    iteration_list.append(ir)\n","\n","\n","    if len(set(new_fam_ids)) == breaker:\n","        break\n","\n","data = {\n","    'iteration_list': iteration_list,\n","    'sil_perf_list': sil_perf_list,\n","    'calinski_perf_list': calinski_perf_list,\n","    'unique_fam_list': unique_fam_list\n","}\n","\n","# Create DataFrame\n","df_eval = pd.DataFrame(data)\n","\n","df_eval\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DNjQkQMuJmg7","outputId":"8900fcd6-f417-4e1c-9b47-87a2f5a2b94d"},"outputs":[],"source":["len(set(new_fam_ids))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kge5yEbPJmg7"},"outputs":[],"source":["\n","filtered_df11['label'] = new_fam_ids\n","outliers_mask = ~not_outliers_mask  # Get the outliers mask\n","outliers_df11 = df11.iloc[outliers_mask].copy(deep=True)  # Extract outliers\n","outliers_df11['label'] = -1\n","df_comb = pd.concat([filtered_df11, outliers_df11], axis=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"soHabVojJmg7"},"outputs":[],"source":["df21 = df_comb.drop(['gpt_3_large_emb'],axis=1)\n","df21.to_excel('Godiva_done_32_pca.xlsx')"]},{"cell_type":"markdown","metadata":{"id":"bTGnLOLSJmg7"},"source":["\n","# Array no loop no outlier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ab3RYY3qJmg7"},"outputs":[],"source":["%%time\n","\n","import networkx as nx\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import pairwise_distances\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","import numpy as np\n","\n","# Assuming df_original is already defined\n","\n","df_original = df_iris.values.copy()\n","# df_original['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_original))]\n","\n","fam_ids = np.array(['unq_id_' + str(i) for i in range(len(df_original))])\n","fam_ids_array = np.array(fam_ids).reshape(-1, 1)\n","new_array = np.concatenate((df_original, fam_ids_array), axis=1)\n","\n","# 2. MAIN NETWORK CLUSTERING\n","\n","## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","# Assuming df is your DataFrame and it includes 'fam_id' along with the features.\n","\n","# Compute the pairwise distances using only the feature columns.\n","dist_matrix0 = pairwise_distances(df_original, metric='euclidean')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSqCP6zsJmg7"},"outputs":[],"source":["# Finding the closest valid entries\n","\n","dist_matrix = np.copy(dist_matrix0)\n","fam_ids = new_array[:, -1]\n","same_fam_id = new_array[:, -1][:, None] == new_array[:, -1]\n","dist_matrix[same_fam_id] = np.inf\n","\n","closest_indices = np.argmin(dist_matrix, axis=1)\n","closest_distances = np.min(dist_matrix, axis=1)\n","\n","# Handling migrations\n","closest_families = fam_ids[closest_indices]\n","dtype = [('fam_id', 'U10'), ('closest_distance', 'f8'), ('closest_family', 'U10')]\n","structured_array = np.array(list(zip(fam_ids, closest_distances, closest_families)), dtype=dtype)\n","sorted_structured_array = np.sort(structured_array, order=['fam_id', 'closest_distance'])\n","_, unique_indices = np.unique(sorted_structured_array['fam_id'], return_index=True)\n","result_structured_array = sorted_structured_array[unique_indices]\n","migration_map = {fam_id: closest_family for fam_id, closest_family in result_structured_array[['fam_id', 'closest_family']]}\n","migrate_to_family = np.array([migration_map[fam] for fam in fam_ids])\n","\n","\n","# Assuming fam_ids, migrate_to_families, and closest_distances are defined as NumPy arrays\n","\n","# Create a structured array for sorting and unique operation\n","dtype = [('migrate_to_family', 'U10'), ('closest_distance', float), ('fam_id', 'U10')]\n","structured_array = np.array(list(zip(migrate_to_family , closest_distances, fam_ids)), dtype=dtype)\n","\n","# Sort by 'migrate_to_family' and 'closest_distance'\n","sorted_array = np.sort(structured_array, order=['migrate_to_family', 'closest_distance'])\n","\n","# Remove duplicates\n","_, unique_indices = np.unique(sorted_array['migrate_to_family'], return_index=True)\n","unique_families = sorted_array[unique_indices]\n","\n","# Mapping for migration decisions\n","migration_decision_map = {entry['migrate_to_family']: entry['fam_id'] for entry in unique_families}\n","\n","# Calculate the minimum distance per migration family\n","unique_families_distances = {entry['migrate_to_family']: entry['closest_distance'] for entry in unique_families}\n","\n","# Assign migration permissions\n","can_migrate = np.array([closest_distances[i] == unique_families_distances[migrate_to_family[i]] for i in range(len(fam_ids))])\n","\n","import networkx as nx\n","\n","uf = nx.utils.UnionFind()\n","for i, entry in enumerate(structured_array):\n","    uf.union(entry['fam_id'], entry['migrate_to_family'])\n","\n","# Map the results back\n","fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","mapped_fam_ids = np.array([fam_id_mapping[fam_id] for fam_id in fam_ids])\n","\n","from sklearn.metrics import pairwise_distances\n","\n","# Calculate centroids\n","unique_fam_ids = np.unique(mapped_fam_ids)\n","centroids = np.array([df_original[mapped_fam_ids == fam_id].mean(axis=0) for fam_id in unique_fam_ids])\n","\n","# Calculate distances from each point to each centroid and find the closest\n","distances = pairwise_distances(df_original, centroids, metric='euclidean')\n","closest_centroids = np.argmin(distances, axis=1)\n","\n","# Assign new fam_ids based on the closest centroid\n","new_fam_ids = np.array([unique_fam_ids[idx] for idx in closest_centroids])\n","fam_ids_array1 = np.array(new_fam_ids).reshape(-1, 1)\n","new_array = np.concatenate((df_original, fam_ids_array1), axis=1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwHM0hPIJmg8"},"outputs":[],"source":["len(set(new_fam_ids))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3TxPcWMJmg8"},"outputs":[],"source":["%%time\n","\n","    # NUCER\n","    # 2. MAIN NETWORK CLUSTERING\n","\n","    ## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","    # dist_matrix = dist_matrix0.copy()\n","    dist_matrix = np.copy(dist_matrix0)\n","    # Reset the index of the DataFrame to keep track of original indices\n","\n","    # OPTIMIZATION 1\n","    #############\n","    # WAS\n","    # # Get a boolean matrix where True represents rows with the same 'fam_id'.\n","    # same_fam_id = df_original['fam_id'].values[:, None] == df_original['fam_id'].values\n","\n","    # # Set the distances for rows with the same 'fam_id' to infinity.\n","    # # np.inf is used here to ensure these distances are never considered as the minimum.\n","    # dist_matrix[same_fam_id] = np.inf\n","\n","    # NOW..SIGNIFICANTLY FASTER\n","    # df_with_index = df_original.reset_index()\n","\n","    # Perform the merge to find all pairs with the same 'fam_id'\n","    # same_fam_id = df_with_index.merge(\n","    #     df_with_index[['index', 'fam_id']], on='fam_id', suffixes=('_left', '_right'))\n","\n","    # # Minimal column merge to reduce memory footprint\n","    # same_fam_id = df_with_index[['index', 'fam_id']].merge(\n","    #     df_with_index[['index', 'fam_id']], on='fam_id', suffixes=('_left', '_right'))\n","    # Get a boolean matrix where True represents rows with the same 'fam_id'.\n","    same_fam_id = df_original['fam_id'].values[:, None] == df_original['fam_id'].values\n","\n","    # Set the distances for rows with the same 'fam_id' to infinity.\n","    # np.inf is used here to ensure these distances are never considered as the minimum.\n","    dist_matrix[same_fam_id] = np.inf\n","\n","\n","    ##########\n","\n","    # num_rows = len(df_original)\n","    # # Initialize a boolean matrix of size (num_rows, num_rows)\n","    # same_fam_id_matrix = np.zeros((num_rows, num_rows), dtype=bool)\n","\n","    # # Fill the boolean matrix with True where indices have the same 'fam_id'\n","    # same_fam_id_matrix[same_fam_id['index_left'].values, same_fam_id['index_right'].values] = True\n","\n","    # dist_matrix[same_fam_id_matrix]= np.inf\n","    ##########\n","\n","    # Find the index of the closest row and the distance for each row, ignoring those with the same 'fam_id'.\n","    closest_indices = dist_matrix.argmin(axis=1)\n","    closest_distances = dist_matrix.min(axis=1)\n","\n","    # Add the results back to the original DataFrame.\n","    df_original['closest_row'] = closest_indices\n","    df_original['closest_distance'] = closest_distances\n","\n","\n","    # OPTIMIZATION 2\n","    #############\n","    # WAS\n","\n","    # Assuming df_original is your original DataFrame and it includes the 'fam_id' column.\n","\n","    # 2B set up families to migrate to and solve conflicts if mulitple row in the same family want to migrate to the different families\n","    # Add a column for the family that the closest row belongs to for easier reference\n","    # df_original['closest_family'] = df_original['closest_row'].apply(lambda x: df_original.at[x, 'fam_id'] if pd.notnull(x) else None)\n","\n","    # # Determine the closest family for each original family based on the smallest distance\n","    # closest_family_for_group = df_original.groupby('fam_id').apply(lambda x: x.loc[x['closest_distance'].idxmin(), 'closest_family'])\n","\n","    # # Map the target family for migration back to the original DataFrame\n","    # df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","    # NOW\n","\n","    df_original['closest_family'] = df_original.loc[df_original['closest_row'], 'fam_id'].values\n","\n","    # Sort the DataFrame first by 'fam_id', then by 'closest_distance'\n","    sorted_df = df_original.sort_values(by=['fam_id', 'closest_distance'])\n","\n","    # Drop duplicates, keeping the first entry for each 'fam_id'\n","    # which will be the one with the smallest 'closest_distance' due to the sort order\n","    result_df = sorted_df.drop_duplicates(subset='fam_id', keep='first')\n","\n","    # Select only the 'fam_id' and 'closest_family' columns if necessary\n","    closest_family_for_group = result_df.set_index('fam_id')['closest_family']\n","\n","    # Map the target family for migration back to the original DataFrame\n","    df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","    ##################\n","\n","    # OPTIMIZATION 3\n","    #############\n","\n","    # WAS\n","    # Choosing only the closest family on the migrate_to_family side\n","    # Step 1: Determine the eligible family for each target family\n","\n","    # Group by 'migrate_to_family' and find the family with the closest distance for each target\n","    # eligible_families = df_original.groupby('migrate_to_family').apply(\n","    #     lambda x: x.loc[x['closest_distance'].idxmin(), 'fam_id']\n","    # ).reset_index().rename(columns={0: 'eligible_fam_id'})\n","\n","    # NOW\n","    # Sort the DataFrame by 'migrate_to_family' and 'closest_distance'\n","    sorted_df = df_original.sort_values(by=['migrate_to_family', 'closest_distance'])\n","\n","    # Drop duplicates, keeping the first entry for each 'migrate_to_family'\n","    # which will be the one with the smallest 'closest_distance' due to the sort order\n","    unique_families = sorted_df.drop_duplicates(subset='migrate_to_family', keep='first')\n","\n","    # Rename and clean up the result if necessary\n","    eligible_families = unique_families.rename(columns={'fam_id': 'eligible_fam_id'})[['migrate_to_family', 'eligible_fam_id']]\n","\n","    ##############\n","\n","    # Step 2: Assign migration permission based on eligibility\n","    # Merge the eligible families DataFrame back to the original DataFrame to assign migration permission\n","    df_original = df_original.merge(\n","        eligible_families,\n","        how='left',\n","        left_on=['migrate_to_family', 'fam_id'],\n","        right_on=['migrate_to_family', 'eligible_fam_id']\n","    )\n","\n","\n","    # Step 1 & 2: Group by 'migrate_to_family' and find all families with the smallest distance for each target\n","    # This involves creating a temporary DataFrame to calculate minimum distances for each migration target group\n","    min_distances = df_original.groupby('migrate_to_family')['closest_distance'].transform('min')\n","\n","    # Step 3: Assign migration permission based on matching the minimum distance\n","    # Families that match the minimum distance to their target are allowed to migrate\n","    df_original['can_migrate'] = df_original['closest_distance'] == min_distances\n","\n","    # Step 1: Find all fam_id values where at least one row has can_migrate = True\n","    fam_ids_to_update = df_original[df_original['can_migrate'] == True]['fam_id'].unique()\n","\n","    # Step 2: Update can_migrate for rows where fam_id matches the identified fam_ids\n","    df_original.loc[df_original['fam_id'].isin(fam_ids_to_update), 'can_migrate'] = True\n","\n","    # Logic to update 'migrate_to_family' where 'can_migrate' is False\n","    df_original.loc[df_original['can_migrate'] == False, 'migrate_to_family'] = df_original['fam_id']\n","\n","    # Drop the 'eligible_fam_id' column as it's no longer needed\n","    df_original.drop('eligible_fam_id', axis=1, inplace=True)\n","\n","    # Initialize NetworkX UnionFind\n","\n","\n","\n","    ## OPTIMIZATION 4\n","    ##################################\n","\n","    # WAS\n","    # # Using DataFrame.apply() to perform Union operations on all rows at once\n","    # uf = nx.utils.UnionFind()\n","    # df_original.apply(lambda row: uf.union(row['fam_id'], row['migrate_to_family']), axis=1)\n","\n","    # # Mapping the results back to the DataFrame can be optimized by\n","    # # creating a mapping dictionary from the UnionFind structure and then using map()\n","    # fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","\n","    # df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping)\n","\n","    # Drop unnecessary columns in a more concise way\n","    # columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","    # df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","\n","    # NOW\n","    # Iterate over DataFrame rows using itertuples() for better performance\n","    uf = nx.utils.UnionFind()\n","    for row in df_original.itertuples():\n","        uf.union(row.fam_id, row.migrate_to_family)\n","\n","    # Creating a mapping dictionary from the UnionFind structure\n","    fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","\n","    # Ensure that all original IDs are covered in the mapping\n","    df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping).fillna(df_original['fam_id'])\n","\n","    # Drop unnecessary columns in a more concise way\n","    columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","    df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","\n","        # # Solution to all my issues.. recenter\n","    def calculate_centroids(df):\n","            \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","            centroids = df.groupby('fam_id').mean()\n","            return centroids\n","\n","    def reassign_families(df, centroids):\n","        \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","        # Calculate distances between each row and each centroid\n","        distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","        # Find the closest centroid for each row\n","        closest_centroids = np.argmin(distances, axis=1)\n","\n","        # Map centroid indices back to family labels\n","        index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","        df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","        return df\n","\n","    # # Calculate initial centroids\n","    centroids = calculate_centroids(df_original)\n","\n","    # Reassign families based on closest centroid\n","    df_original = reassign_families(df_original, centroids)\n","\n","    ####################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GYztWagKJmg9"},"outputs":[],"source":["df_original.fam_id.nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-3kUX1RJmg9"},"outputs":[],"source":["# # Medoid\n","\n","# # Reset the index to ensure alignment\n","# df_with_index = df_original.reset_index()\n","# # Perform the merge on 'fam_id' to identify the same family pairings\n","# same_fam_id = df_with_index.merge(df_with_index[['index', 'fam_id']],\n","#                     on='fam_id', suffixes=('_left', '_right'))\n","\n","# # Initialize a  full infinity matrix\n","# intra_fam_dist_matrix = np.full((len(df_original), len(df_original)), np.inf)\n","\n","# # Fill only the distances for the same family members\n","# intra_fam_dist_matrix[same_fam_id['index_left'].values, same_fam_id['index_right'].values] = dist_matrix0[same_fam_id['index_left'].values, same_fam_id['index_right'].values]\n","\n","# # Sum distances in the intra-family distance matrix\n","# sum_distances = np.sum(intra_fam_dist_matrix, axis=1)\n","\n","# # Add a column to df_original to identify the medoid's index within each family\n","# df_with_index['sum_distances'] = sum_distances\n","\n","# # Identify the index with the minimum sum of distances within each family group\n","# medoids = df_with_index.loc[df_with_index.groupby('fam_id')['sum_distances'].idxmin()]\n","\n","# medoid_family_mapping = df_original.loc[medoids.index, ['fam_id']]\n","# medoid_family_mapping['medoid_index'] = medoids.index\n","# # Select only the columns of the distance matrix corresponding to the medoids\n","# medoid_distances = dist_matrix0[:, medoid_family_mapping['medoid_index']]\n","# closest_medoid_indices = np.argmin(medoid_distances, axis=1)\n","# # Create a mapping from index in the medoid_distances array to family ID\n","# index_to_family_id = medoid_family_mapping['fam_id'].iloc[closest_medoid_indices].values\n","# df_original['fam_id'] = index_to_family_id"]},{"cell_type":"markdown","metadata":{"id":"HlDe3CLdJmg9"},"source":["# Dataframe version with loop and outlier"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 188 ms\n","Wall time: 382 ms\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>iteration_list</th>\n","      <th>sil_perf_list</th>\n","      <th>calinski_perf_list</th>\n","      <th>unique_fam_list</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.307999</td>\n","      <td>394.834372</td>\n","      <td>303</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.361714</td>\n","      <td>287.235809</td>\n","      <td>175</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.405565</td>\n","      <td>256.649844</td>\n","      <td>103</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.428583</td>\n","      <td>297.048743</td>\n","      <td>62</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.422243</td>\n","      <td>281.018386</td>\n","      <td>35</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>0.435867</td>\n","      <td>323.781674</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>0.387908</td>\n","      <td>460.719629</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>0.281347</td>\n","      <td>326.963439</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>0.355716</td>\n","      <td>275.212680</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>0.277275</td>\n","      <td>336.132545</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>0.273687</td>\n","      <td>474.058596</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>0.794613</td>\n","      <td>850.404654</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    iteration_list  sil_perf_list  calinski_perf_list  unique_fam_list\n","0                0       0.307999          394.834372              303\n","1                1       0.361714          287.235809              175\n","2                2       0.405565          256.649844              103\n","3                3       0.428583          297.048743               62\n","4                4       0.422243          281.018386               35\n","5                5       0.435867          323.781674               22\n","6                6       0.387908          460.719629               13\n","7                7       0.281347          326.963439                8\n","8                8       0.355716          275.212680                5\n","9                9       0.277275          336.132545                4\n","10              10       0.273687          474.058596                3\n","11              11       0.794613          850.404654                2"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","\n","breaker = 2\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import pairwise_distances\n","# df_original = df_iris1.copy(deep=True)\n","#df_original = df_credit_card_3.copy(deep=True)\n","all_outliers = pd.DataFrame()\n","\n","all_df_original = pd.DataFrame()\n","df_original = df_14.copy(deep=True)\n","\n","## Outlier removal hashed out\n","# # Compute the pairwise distances\n","# dist_matrix = pairwise_distances(df, metric='euclidean')\n","\n","# # Replace diagonal with np.inf to ignore self-distance\n","# np.fill_diagonal(dist_matrix, np.inf)\n","\n","# # Find the index of the closest row and the distance for each row\n","# closest_indices = dist_matrix.argmin(axis=1)\n","# closest_distances = dist_matrix.min(axis=1)\n","\n","# # Add the results to the original DataFrame\n","# df['Closest_Row'] = closest_indices\n","# df['Distance'] = closest_distances\n","\n","# # 1. Remove outliers after\n","# # We will modify the function to also return the outliers\n","\n","# def remove_upper_outliers_and_save(df, column_name):\n","#     quartile_1, quartile_3 = np.percentile(df[column_name], [25, 75])\n","#     iqr = quartile_3 - quartile_1\n","#     upper_bound = quartile_3 + (iqr * 1.5) # 1.5\n","\n","#     # Filter the DataFrame to remove data points above the upper bound\n","#     filtered_df = df[df[column_name] <= upper_bound]\n","#     # Save the outliers in a separate DataFrame\n","#     outliers_df = df[df[column_name] > upper_bound]\n","\n","#     return filtered_df, outliers_df\n","\n","# # Apply the modified function to remove upper outliers from 'Distance' column and save the removed outliers\n","# filtered_df, outliers_df = remove_upper_outliers_and_save(df, 'Distance')\n","\n","# # Now we have two DataFrames: `filtered_df` without outliers and `outliers_df` with only the removed outliers\n","# # (filtered_df, outliers_df)\n","\n","# outliers_df0 = outliers_df.copy(deep=True)\n","# del(outliers_df)\n","# #filtered_df.reset_index(inplace=True, drop=True)\n","\n","# df_original = filtered_df.copy(deep=True)\n","# df_original.drop(['Closest_Row','Distance'], axis = 1, inplace=True)\n","# outliers_df0.drop(['Closest_Row','Distance'], axis = 1, inplace=True)\n","# outliers_df0['fam_id'] = -1\n","# all_outliers = pd.concat([all_outliers, outliers_df0], ignore_index=True)\n","# # del(outliers_df0)\n","\n","# df_clean1 = df_cleaned.loc[df_original.index].copy(deep=True)\n","# df_clean1.reset_index(inplace=True, drop=True)\n","\n","# # df_original['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_original))]\n","# df_original.reset_index(drop=True, inplace=True)\n","#\n","#\n","#\n","# Using factorize to encode 'fam_id'\n","# df_original['fam_id'] = pd.factorize(df_original['fam_id'])[0]\n","\n","# # Convert the encoded numbers to strings\n","# df_original['fam_id'] = df_original['fam_id'].astype(str)\n","\n","\n","\n","\n","#\n","#\n","#\n","#\n","#44\n","# df_original = df_iris.copy(deep=True)\n","df_original['fam_id'] = ['unq_id_' + str(i) for i in range(len(df_original))]\n","\n","# 2. MAIN NETWORK CLUSTERING\n","\n","## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","import networkx as nx\n","import numpy as np\n","import pandas as pd\n","from scipy.spatial.distance import cdist\n","\n","from sklearn.metrics import pairwise_distances\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","# Assuming df is your DataFrame and it includes 'fam_id' along with the features.\n","\n","# Compute the pairwise distances using only the feature columns.\n","features = df_original.drop('fam_id', axis=1)\n","dist_matrix0 = pairwise_distances(features, metric='euclidean')\n","\n","\n","# # Solution to all my issues.. recenter\n","def calculate_centroids(df):\n","        \"\"\"Calculate centroids of families in the dataframe.\"\"\"\n","        centroids = df.groupby('fam_id').mean()\n","        return centroids\n","\n","def reassign_families(df, centroids):\n","    \"\"\"Reassign rows to the closest family based on Euclidean distance to centroids.\"\"\"\n","    # Calculate distances between each row and each centroid\n","    distances = pairwise_distances(df.drop('fam_id',axis=1), centroids, metric='euclidean')\n","\n","    # Find the closest centroid for each row\n","    closest_centroids = np.argmin(distances, axis=1)\n","\n","    # Map centroid indices back to family labels\n","    index_to_family = {i: family for i, family in enumerate(centroids.index)}\n","    df['fam_id'] = [index_to_family[index] for index in closest_centroids]\n","\n","    return df\n","\n","iteration_list = []\n","sil_perf_list = []\n","calinski_perf_list = []\n","unique_fam_list = []\n","\n","\n","# NUCER\n","\n","for ir in range(50):\n","\n","    # 2. MAIN NETWORK CLUSTERING\n","\n","    ## 2A Find closet distance and index of rows with closes distance in a different fam_id\n","    # dist_matrix = dist_matrix0.copy()\n","    features = df_original.drop('fam_id', axis=1)\n","    dist_matrix0 = pairwise_distances(features, metric='euclidean')\n","\n","    dist_matrix = np.copy(dist_matrix0)\n","    # Reset the index of the DataFrame to keep track of original indices\n","\n","    # OPTIMIZATION 1\n","    #############\n","    # WAS\n","    # # Get a boolean matrix where True represents rows with the same 'fam_id'.\n","    # same_fam_id = df_original['fam_id'].values[:, None] == df_original['fam_id'].values\n","\n","    # # Set the distances for rows with the same 'fam_id' to infinity.\n","    # # np.inf is used here to ensure these distances are never considered as the minimum.\n","    # dist_matrix[same_fam_id] = np.inf\n","\n","    # NOW..SIGNIFICANTLY FASTER\n","    # df_with_index = df_original.reset_index()\n","\n","    # # Perform the merge to find all pairs with the same 'fam_id'\n","    # same_fam_id = df_with_index.merge(\n","    #     df_with_index[['index', 'fam_id']], on='fam_id', suffixes=('_left', '_right'))\n","\n","    # num_rows = len(df_original)\n","    # # Initialize a boolean matrix of size (num_rows, num_rows)\n","    # same_fam_id_matrix = np.zeros((num_rows, num_rows), dtype=bool)\n","\n","    # # Fill the boolean matrix with True where indices have the same 'fam_id'\n","    # same_fam_id_matrix[same_fam_id['index_left'].values, same_fam_id['index_right'].values] = True\n","\n","    # dist_matrix[same_fam_id_matrix]= np.inf\n","    ##########\n","\n","    # Get a boolean matrix where True represents rows with the same 'fam_id'.\n","    same_fam_id = df_original['fam_id'].values[:, None] == df_original['fam_id'].values\n","\n","    # Set the distances for rows with the same 'fam_id' to infinity.\n","    # np.inf is used here to ensure these distances are never considered as the minimum.\n","    dist_matrix[same_fam_id] = np.inf\n","\n","    # Find the index of the closest row and the distance for each row, ignoring those with the same 'fam_id'.\n","    closest_indices = dist_matrix.argmin(axis=1)\n","    closest_distances = dist_matrix.min(axis=1)\n","\n","    # Add the results back to the original DataFrame.\n","    df_original['closest_row'] = closest_indices\n","    df_original['closest_distance'] = closest_distances\n","\n","    # OPTIMIZATION 2\n","    #############\n","    # WAS\n","\n","    # Assuming df_original is your original DataFrame and it includes the 'fam_id' column.\n","\n","    # 2B set up families to migrate to and solve conflicts if mulitple row in the same family want to migrate to the different families\n","    # Add a column for the family that the closest row belongs to for easier reference\n","    # df_original['closest_family'] = df_original['closest_row'].apply(lambda x: df_original.at[x, 'fam_id'] if pd.notnull(x) else None)\n","\n","    # # Determine the closest family for each original family based on the smallest distance\n","    # closest_family_for_group = df_original.groupby('fam_id').apply(lambda x: x.loc[x['closest_distance'].idxmin(), 'closest_family'])\n","\n","    # # Map the target family for migration back to the original DataFrame\n","    # df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","    # NOW\n","\n","    df_original['closest_family'] = df_original.loc[df_original['closest_row'], 'fam_id'].values\n","\n","\n","    # Sort the DataFrame first by 'fam_id', then by 'closest_distance'\n","    sorted_df = df_original.sort_values(by=['fam_id', 'closest_distance'])\n","\n","    # Drop duplicates, keeping the first entry for each 'fam_id'\n","    # which will be the one with the smallest 'closest_distance' due to the sort order\n","    result_df = sorted_df.drop_duplicates(subset='fam_id', keep='first')\n","\n","    # Select only the 'fam_id' and 'closest_family' columns if necessary\n","    closest_family_for_group = result_df.set_index('fam_id')['closest_family']\n","\n","    # Map the target family for migration back to the original DataFrame\n","    df_original['migrate_to_family'] = df_original['fam_id'].map(closest_family_for_group)\n","\n","    ##################\n","\n","    # OPTIMIZATION 3\n","    #############\n","\n","    # WAS\n","    # Choosing only the closest family on the migrate_to_family side\n","    # Step 1: Determine the eligible family for each target family\n","\n","    # Group by 'migrate_to_family' and find the family with the closest distance for each target\n","    # eligible_families = df_original.groupby('migrate_to_family').apply(\n","    #     lambda x: x.loc[x['closest_distance'].idxmin(), 'fam_id']\n","    # ).reset_index().rename(columns={0: 'eligible_fam_id'})\n","\n","    # NOW\n","    # Sort the DataFrame by 'migrate_to_family' and 'closest_distance'\n","    sorted_df = df_original.sort_values(by=['migrate_to_family', 'closest_distance'])\n","\n","    # Drop duplicates, keeping the first entry for each 'migrate_to_family'\n","    # which will be the one with the smallest 'closest_distance' due to the sort order\n","    unique_families = sorted_df.drop_duplicates(subset='migrate_to_family', keep='first')\n","\n","    # Rename and clean up the result if necessary\n","    eligible_families = unique_families.rename(columns={'fam_id': 'eligible_fam_id'})[['migrate_to_family', 'eligible_fam_id']]\n","\n","    ##############\n","\n","    # Step 2: Assign migration permission based on eligibility\n","    # Merge the eligible families DataFrame back to the original DataFrame to assign migration permission\n","    df_original = df_original.merge(\n","        eligible_families,\n","        how='left',\n","        left_on=['migrate_to_family', 'fam_id'],\n","        right_on=['migrate_to_family', 'eligible_fam_id']\n","    )\n","\n","\n","    # Step 1 & 2: Group by 'migrate_to_family' and find all families with the smallest distance for each target\n","    # This involves creating a temporary DataFrame to calculate minimum distances for each migration target group\n","    min_distances = df_original.groupby('migrate_to_family')['closest_distance'].transform('min')\n","\n","    # Step 3: Assign migration permission based on matching the minimum distance\n","    # Families that match the minimum distance to their target are allowed to migrate\n","    df_original['can_migrate'] = df_original['closest_distance'] == min_distances\n","\n","    # Step 1: Find all fam_id values where at least one row has can_migrate = True\n","    fam_ids_to_update = df_original[df_original['can_migrate'] == True]['fam_id'].unique()\n","\n","    # Step 2: Update can_migrate for rows where fam_id matches the identified fam_ids\n","    df_original.loc[df_original['fam_id'].isin(fam_ids_to_update), 'can_migrate'] = True\n","\n","    # Logic to update 'migrate_to_family' where 'can_migrate' is False\n","    df_original.loc[df_original['can_migrate'] == False, 'migrate_to_family'] = df_original['fam_id']\n","\n","    # Drop the 'eligible_fam_id' column as it's no longer needed\n","    df_original.drop('eligible_fam_id', axis=1, inplace=True)\n","\n","    # Initialize NetworkX UnionFind\n","\n","\n","\n","    ## OPTIMIZATION 4\n","    ##################################\n","\n","    # WAS\n","    # # Using DataFrame.apply() to perform Union operations on all rows at once\n","    # uf = nx.utils.UnionFind()\n","    # df_original.apply(lambda row: uf.union(row['fam_id'], row['migrate_to_family']), axis=1)\n","\n","    # # Mapping the results back to the DataFrame can be optimized by\n","    # # creating a mapping dictionary from the UnionFind structure and then using map()\n","    # fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","\n","    # df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping)\n","\n","    # Drop unnecessary columns in a more concise way\n","    # columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","    # df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","\n","    # NOW\n","    # Iterate over DataFrame rows using itertuples() for better performance\n","    uf = nx.utils.UnionFind()\n","    for row in df_original.itertuples():\n","        uf.union(row.fam_id, row.migrate_to_family)\n","\n","    # Creating a mapping dictionary from the UnionFind structure\n","    fam_id_mapping = {item: uf[item] for item in uf.parents.keys()}\n","\n","    # Ensure that all original IDs are covered in the mapping\n","    df_original['fam_id'] = df_original['fam_id'].map(fam_id_mapping).fillna(df_original['fam_id'])\n","\n","    # Drop unnecessary columns in a more concise way\n","    columns_to_drop = ['closest_row', 'closest_distance', 'closest_family', 'migrate_to_family', 'can_migrate']\n","    df_original.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","    ####################################\n","\n","    # # Calculate initial centroids\n","    centroids = calculate_centroids(df_original)\n","\n","    # Reassign families based on closest centroid\n","    df_original = reassign_families(df_original, centroids)\n","    # df_original.reset_index(drop=True,inplace=True)\n","\n","    # # Outlier removal\n","\n","\n","    # #Assuming df_original is your DataFrame and it already contains 'fam_id'\n","    # #Select numeric columns for centroid calculation, excluding 'fam_id' or any other non-relevant columns\n","    # features = [col for col in df_original.select_dtypes(include=np.number).columns if col != 'fam_id']\n","\n","    # # Calculate the centroid for each family using a groupby operation, this remains efficient\n","    # centroids = df_original.groupby('fam_id')[features].transform('mean')\n","\n","    # # Calculate pairwise distances to centroid for each member without using apply()\n","    # # Using scipy's cdist function for efficient distance calculation\n","    # distances_to_centroid = cdist(df_original[features], centroids, metric='euclidean').diagonal()\n","    # df_original['distance_to_centroid'] = distances_to_centroid\n","\n","    # # Identify outliers using the IQR method for each family (only above upper bound), vectorized for efficiency\n","    # Q1 = df_original.groupby('fam_id')['distance_to_centroid'].transform(lambda x: x.quantile(0.25))\n","    # Q3 = df_original.groupby('fam_id')['distance_to_centroid'].transform(lambda x: x.quantile(0.75))\n","    # IQR = Q3 - Q1\n","    # upper_bound = Q3 + 1.5 * IQR\n","\n","    # # Assigning -1 to fam_id for outliers, more directly\n","    # df_original.loc[df_original['distance_to_centroid'] > upper_bound, 'fam_id'] = -1\n","\n","    # # Splitting outliers and non-outliers into separate DataFrames as needed\n","    # outliers_df = df_original[df_original['fam_id'] == -1].copy(deep=True)\n","    # df_original = df_original[df_original['fam_id'] != -1].copy(deep=True)\n","\n","    # del(all_df_original)\n","    # all_df_original = pd.DataFrame()\n","    # df_original.drop('distance_to_centroid', axis=1, inplace=True)\n","    # outliers_df.drop('distance_to_centroid', axis=1, inplace=True)\n","\n","    # # filtered_df1 = df_clean1.loc[df_original.index].copy()\n","    # # filtered_df1 = filtered_df1.join(df_original['fam_id'], how='left')\n","    # if ir == 0:\n","    #   filtered_df1 = df_clean1.join(df_original['fam_id'], how='right')\n","    #   filtered_outlier_df = df_clean1.loc[outliers_df.index]\n","    #   filtered_df1.reset_index(inplace=True, drop=True)\n","    # else:\n","    #   filtered_df2 = filtered_df1.join(df_original['fam_id'], how='right')\n","    #   filtered_outlier_df = filtered_df1.loc[outliers_df.index]\n","    #   filtered_outlier_df['fam_id'] = -1\n","    #   filtered_df2.reset_index(inplace=True, drop=True)\n","    #   filtered_df1 = filtered_df2.copy(deep=True)\n","\n","\n","    # all_df_original = pd.concat([all_df_original, filtered_df1], ignore_index=True)\n","    # all_outliers = pd.concat([all_outliers, filtered_outlier_df], ignore_index=True)\n","    # filtered_df1.drop('fam_id', inplace=True, axis=1)\n","    # # Optionally, drop the 'distance_to_centroid' column if no longer needed\n","\n","    df_original.reset_index(inplace=True, drop=True)\n","\n","    labels = df_original.fam_id\n","    selected_columns = df_original.drop(['fam_id'], axis = 1)\n","    sil_perf = silhouette_score(selected_columns, labels)\n","    calinski_harabasz = calinski_harabasz_score(selected_columns, labels)\n","    sil_perf_list.append(sil_perf)\n","    calinski_perf_list.append(calinski_harabasz)\n","    unique_fam_list.append(df_original.fam_id.nunique())\n","    iteration_list.append(ir)\n","\n","    if df_original.fam_id.nunique() == breaker:\n","        break\n","\n","data = {\n","    'iteration_list': iteration_list,\n","    'sil_perf_list': sil_perf_list,\n","    'calinski_perf_list': calinski_perf_list,\n","    'unique_fam_list': unique_fam_list\n","}\n","\n","# Create DataFrame\n","df_eval = pd.DataFrame(data)\n","\n","df_eval\n","#555"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":221,"status":"ok","timestamp":1714577367074,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"k1M2PCjGMoT-"},"outputs":[],"source":["# Possible issue with all_outliers\n","df_comb1 = pd.concat([all_df_original, all_outliers], ignore_index=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":11489,"status":"ok","timestamp":1714577396376,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"93nuAR6WO__U"},"outputs":[],"source":["df_comb1.to_excel('Godiva_true_umap_148.xlsx')"]},{"cell_type":"markdown","metadata":{},"source":["# Performance"]},{"cell_type":"markdown","metadata":{"id":"Oe5otvBuJmhF"},"source":["## Test cluster performance\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":337,"status":"ok","timestamp":1714575780829,"user":{"displayName":"Derrick Ofori","userId":"03859556030576694070"},"user_tz":-60},"id":"Yvt-Il3xJmhF","outputId":"2955b94a-5fc6-491c-ee2a-d309e5c6ba62"},"outputs":[],"source":["# DSC- PURE\n","# df_original = df_original_2.copy()\n","\n","### DOUBLE CHECK iloc\n","df_wine_selected_columns = df_original.drop('fam_id',axis=1).copy(deep=True)\n","\n","import pandas as pd\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","# Assume 'labels' is the array of cluster labels obtained from a clustering algorithm\n","# For example: labels = [0, 1, 0, 1] - replace this with your actual labels\n","labels = df_original.fam_id\n","# Calculate metrics\n","silhouette_avg = silhouette_score(df_wine_selected_columns, labels)\n","calinski_harabasz = calinski_harabasz_score(df_wine_selected_columns, labels)\n","davies_bouldin = davies_bouldin_score(df_wine_selected_columns, labels)\n","\n","print(f\"Silhouette Score: {silhouette_avg}\")\n","print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n","print(f\"Davies-Bouldin Index: {davies_bouldin}\")"]},{"cell_type":"markdown","metadata":{},"source":["## For array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CB5EcD9_JmhF"},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","# Assuming 'new_array' is your NumPy array with the last column as 'fam_id'\n","# Drop 'fam_id' column for feature data\n","features = new_array[:, :-1].astype(float)\n","# features = new_array[:, :-1]  # All rows, all columns except the last\n","\n","# Extract 'fam_id' for labels\n","labels = new_array[:, -1]  # Last column for labels\n","\n","# Calculate metrics\n","silhouette_avg = silhouette_score(features, labels)\n","calinski_harabasz = calinski_harabasz_score(features, labels)\n","davies_bouldin = davies_bouldin_score(features, labels)\n","\n","print(f\"Silhouette Score: {silhouette_avg}\")\n","print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n","print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n"]},{"cell_type":"markdown","metadata":{"id":"o92tTmawJmhH"},"source":["## DSC Vis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8IhZAyo9JmhH","outputId":"08313f2a-673a-4f56-e4d9-c60746038005"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import umap.umap_ as umap\n","\n","# Assuming 'new_array' is a NumPy array and it's already loaded\n","# Prepare data: Assuming the first columns except the last are features since data is already standardized\n","features = new_array[:, :-1]  # All rows, all columns except the last\n","colors = new_array[:, -1]     # 'fam_id' is the last column\n","\n","# Run UMAP\n","reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n","embedding = reducer.fit_transform(features)\n","\n","# Plotting\n","plt.figure(figsize=(12, 10))\n","scatter = sns.scatterplot(\n","    x=embedding[:, 0],\n","    y=embedding[:, 1],\n","    hue=colors,\n","    palette=\"viridis\",\n","    legend='full'\n",")\n","plt.title('UMAP projection of the dataset clusters, colored by fam_id')\n","plt.xlabel('UMAP 1')\n","plt.ylabel('UMAP 2')\n","plt.legend(title='fam_id')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJW86U_lJmhH"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import umap.umap_ as umap\n","#import umap\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Assuming 'first_row_each_group' is your DataFrame and it's already loaded\n","# new_array = new_array_2.copy()\n","# Prepare data: Assuming the first four columns are features since data is already standardized\n","features = new_array.drop('fam_id',axis=1).copy(deep=True)\n","#features = new_array.copy(deep=True)\n","# Assuming 'fam_id' is a column you want to use for coloring the points\n","colors = new_array['fam_id']\n","\n","# Run UMAP\n","reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n","embedding = reducer.fit_transform(features)\n","\n","# Plotting\n","plt.figure(figsize=(12, 10))\n","# Plot with hue based on 'fam_id'\n","scatter = sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], hue=colors, palette=\"viridis\", legend='full')\n","#scatter = sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], palette=\"viridis\", legend='full')\n","#Label points with the DataFrame index\n","# for i, point in enumerate(embedding):\n","#     plt.text(point[0]+0.1,  # Adding a small offset to x position for clarity\n","#              point[1],\n","#              new_array.index[i],  # The text is the index of the row\n","#              horizontalalignment='left',\n","#              size='small',\n","#              color='black')\n","\n","plt.title('UMAP projection of the dataset clusters, colored by fam_id')\n","plt.xlabel('UMAP 1')\n","plt.ylabel('UMAP 2')\n","plt.legend(title='fam_id')\n","plt.show()\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
